{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto web (Semana 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este proyecto nos fue solicitado que realizaramos dos tareas:\n",
    "1. Hacer uso de un API para generar un dataset.\n",
    "2. Aplicar web scraping para generar un dataset.\n",
    "\n",
    "Estas dos tareas deben resultar en los siguientes archivos:\n",
    "1. Un archivo \".csv\" en el cual tengamos el dataset generado via API.\n",
    "2. Un archivo \".csv\" en el cual tengamos el dataset generado via API, aplicando labores de limpieza y manipulación.\n",
    "3. Un archivo \".csv\" en el cual tengamos el dataset generado via web scraping.\n",
    "4. Un archivo \".csv\" en el cual tengamos el dataset generado via web scraping, aplicando labores de limpieza y manipulación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas para el proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando que los datos son la materia prima para proyectos de analitica, decidí utilizar el API de un gran sitio (Kaggle) que contiene datasets sobre diferentes temas, la gran mayoria de manera pública.\n",
    "\n",
    "En el caso del web scraping decidí tomar una página que contiene un gran número de modelos en 3D, y obtener información acerca de cada uno de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que realice para el uso del API, fue instalar un wrapper que ofrece el API de Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install mdutils kaggle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue con una celda para realizar todos los imports que se vayan requiriendo a lo largo del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import operator\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El wrapper del API de Kaggle realiza la autentificación con los siguientes comandos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api = KaggleApi({\"username\":\"\",\"key\":\"\"})\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este momento estamos autorizados para utilizar el API a partir de todos los métodos que provee el wrapper. En teoría el API de kaggle es más fácil de usar desde un shell, y su documentación (https://github.com/Kaggle/kaggle-api) esta redactada para su uso en shell. Pero es completamente factible traducir todos sus comandos al metodo incluido en el wrapper. Algunos de los comandos disponibles se enlistan a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Comando                  | Parametros                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Descripción                                                                                                                                                                      |\n",
    "|--------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| dataset_list()           | sort_by: how to sort the result, see valid_dataset_sort_bys for options size: the size of the dataset, see valid_dataset_sizes for string options file_type: the format, see valid_dataset_file_types for string options license_name: string descriptor for license, see valid_dataset_license_names tag_ids: tag identifiers to filter the search search: a search term to use (default is empty string) user: username to filter the search to mine: boolean if True, group is changed to \"my\" to return personal page: the page to return (default is 1) | Comando para realizar búsqueda de datasets, los parámetros extra permiten ordenarlos, filtrar por tags, página que obtener, buscar datasets por usuario y otras caracteristicas. |\n",
    "| dataset_view()           | :param str owner_slug: Dataset owner (required) :param str dataset_slug: Dataset name (required)                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Ver metadatos de un dataset.                                                                                                                                                     |\n",
    "| dataset_metadata()       | dataset: name dataset path: its obtain with the name of the dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Ver metadatos de un dataset.                                                                                                                                                     |\n",
    "| dataset_list_files()     | dataset: the string identified of the dataset should be in format [owner]/[dataset-name]                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Lista los archivos presentes en el dataset.                                                                                                                                      |\n",
    "| dataset_download_file()  | dataset: the string identified of the dataset should be in format [owner]/[dataset-name] file_name: the dataset configuration file path: if defined, download to this location force: force the download if the file already exists (default False) quiet: suppress verbose output (default is True)                                                                                                                                                                                                                                                         | Descarga un archivo presente  en un dataset.                                                                                                                                     |\n",
    "| dataset_download_files() | dataset: the string identified of the dataset should be in format [owner]/[dataset-name] path: the path to download the dataset to force: force the download if the file already exists (default False) quiet: suppress verbose output (default is True) unzip: if True, unzip files upon download (default is False)                                                                                                                                                                                                                                        | Descacarga todos los archivos  de un dataset.                                                                                                                                    |\n",
    "| download_file()          | response: the response to download outfile: the output file to download to quiet: suppress verbose output (default is True) chunk_size: the size of the chunk to stream                                                                                                                                                                                                                                                                                                                                                                                      | También descarga un archivo.                                                                                                                                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mi proyecto me interesa obtener datasets que tengan relación palabras clave que yo seleccione, para esto construyo una lista con dichas palabras, en ella preferentemente hay que agregar palabras en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intereses = ['currencies','currency','forex','finance','exchanges','tweets','news','fake news']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue realizando una búsqueda en la API con cada interes, decidí agregar una pausa entre cada solicitud a la API de un 1.5s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets_category = pd.DataFrame()\n",
    "result_busqueda_list = []\n",
    "categoria_list = []\n",
    "\n",
    "for interes in intereses:\n",
    "    time.sleep(1.5)\n",
    "    response = api.dataset_list(search=interes)\n",
    "    if len(response) != 0:\n",
    "        result_busqueda_list.extend(response)\n",
    "        categoria_list.extend(((interes+',')*len(response)).split(',')[:-1])\n",
    "    \n",
    "datasets_category['Dataset'] = result_busqueda_list\n",
    "datasets_category['Category'] = categoria_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Se obtuvieron %i datasets en la busqueda sobre los interes seleccionados.' % len(datasets_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos si existen repeticiones en los resultados de busqueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(set(datasets_category['Dataset'])) != len(datasets_category):\n",
    "    print('Existen datasets repetidos')\n",
    "else:\n",
    "    print('No hay datasets repetidos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada uno de los datasets encontrados en la búsqueda descargaremos sus metadatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_datasets_list = []\n",
    "for dataset in datasets_category['Dataset']:\n",
    "    time.sleep(1)\n",
    "    owner_name = str(dataset).split('/')[0]\n",
    "    name = str(dataset).split('/')[1]\n",
    "    metadata_datasets_list.append(api.datasets_view(owner_name,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_keys = ['id', 'ref', 'subtitle', 'tags', 'creatorName', 'creatorUrl',\n",
    "                 'totalBytes', 'url', 'lastUpdated', 'downloadCount', 'isPrivate',\n",
    "                 'isReviewed', 'isFeatured', 'licenseName', 'description', 'ownerName',\n",
    "                 'ownerRef', 'kernelCount', 'title', 'topicCount', 'viewCount', 'voteCount',\n",
    "                 'currentVersionNumber', 'files', 'versions', 'usabilityRating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.DataFrame(metadata_datasets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el dataset sin limpiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.to_csv('dataset_api.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccioné las columnas que considero útiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_key_metadata = ['title','subtitle','description','lastUpdated','ref','totalBytes','url','tags','downloadCount','licenseName',\n",
    "                       'kernelCount','versions','usabilityRating'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = metadata_df[useful_key_metadata]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificando valores nulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos datos que no sean un valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = dataset.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se encontraron valores nulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulación del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregamos la columna de categoría de búsqueda con la que iniciamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['category'] = categoria_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna total bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificamos el valor base de la columna de \"Bytes\" a \"Mega bytes\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_to_gb = lambda x: x/1000000\n",
    "dataset[\"totalBytes\"] = dataset[\"totalBytes\"].apply(byte_to_gb)\n",
    "dataset = dataset.rename(columns = {\"totalBytes\":\"totalGigaBytes\"})\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columna Tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la columna \"tags\" encontramos referencias a los grupos en los cuales se encuentra clasificado el dataset.\n",
    "Esta columna varia de dataset a dataset. Sin embargo nos permite tener aún más grupos sobre los cuales realizar\n",
    "busquedas con resultados que puedan ser de interes al usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta columna realizaremos una extracción de todas las etiquetas y obtenemos la frecuencia de un set para evitar repeticiones,además de que presentaremos las tres más frecuentes al usuario de manera que este pueda usarlas en una búsqueda de intereses aún mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_interest = [element['ref'] for tag in dataset['tags'] for element in tag]\n",
    "set_suggest = set(suggest_interest)\n",
    "dict_freq_suggest = {k:suggest_interest.count(k) for k in set_suggest}\n",
    "sorted_tups = sorted(dict_freq_suggest.items(), key=operator.itemgetter(1))\n",
    "print(sorted_tups[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas sugerencias se pueden interpretar como los hashtag que contiene el dataset, por tanto hay que ser cuidadosos al seleccionar nuevos intereses de la lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así también tenemos valores vacios para la columna etiquetas, así que sustituiremos la columna tags\n",
    "por \"number of tags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = lambda x: len(x)\n",
    "dataset[\"tags\"] = dataset[\"tags\"].apply(num_tags)\n",
    "dataset = dataset.rename(columns = {\"tags\":\"numberOfTags\"})\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columna Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna versions contiene al menos una versión para el dataset, sin embargo en caso de que tenga más solo sería\n",
    "de nuestro interes la última versión y su fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"versions\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_version = lambda x: str(x[0]['status']) + ' version: ' +  str(x[0]['versionNumber']) + ', ' + str(x[0]['creationDate'])\n",
    "dataset[\"versions\"] = dataset[\"versions\"].apply(last_version)\n",
    "dataset = dataset.rename(columns = {\"versions\":\"lastVersion\"})\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente por ahora, podemos utilizar cada una de las columnas disponibles para realizar algunos filtros con los cuales obtener información interesante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejor score de utilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort_values(['usabilityRating'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mayor tamaño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort_values(['totalGigaBytes'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El más utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort_values(['kernelCount'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el dataset limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('dataset_api_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo que siempre me ha gustado son los modelos 3D, estos pueden ser útiles para empresas de videojuegos, para la industria de la animación y algunos otros sectores.\n",
    "\n",
    "La primera página que encontre con cientos de modelos de pago y gratuitos para descargar fue: https://www.turbosquid.com/\n",
    "\n",
    "Mi objetivo será hacer web scraping a su sitio y obtener información útil para cada uno de los modelos 3D diponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generé la clase que utilizaré para realizar el scraping tomando como base el trabajo en el lab de web scraping avanzado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSpider:\n",
    "    \"\"\"\n",
    "    This is the constructor class to which you can pass a bunch of parameters. \n",
    "    These parameters are stored to the class instance variables so that the\n",
    "    class functions can access them later.\n",
    "    \n",
    "    url_pattern: the regex pattern of the web urls to scape\n",
    "    pages_to_scrape: how many pages to scrape\n",
    "    sleep_interval: the time interval in seconds to delay between requests. If <0, requests will not be delayed.\n",
    "    content_parser: a function reference that will extract the intended info from the scraped content.\n",
    "    \"\"\"\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "        self.output = []\n",
    "    \"\"\"\n",
    "    Scrape the content of a single url.\n",
    "    \"\"\"\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        if str(response) != '<Response [200]>':\n",
    "            print('Error en la respuesta del servidor',response)\n",
    "        elif str(response) == '<Response [408]>':\n",
    "            print('Error en el limite de tiempo de respuesta del servidor')\n",
    "        elif str(response) == '<Response [429]>':\n",
    "            print('Error demasiadas peticiones')\n",
    "        # I didn't find the SSL error but I add a 404 error catching.\n",
    "        elif str(response) == '<Response [404]>':\n",
    "            print('No se encontro el contenido')\n",
    "        else:\n",
    "            result = self.content_parser(response.content)\n",
    "            self.output_results(result)\n",
    "    \n",
    "    \"\"\"\n",
    "    Export the scraped content. Right now it simply print out the results.\n",
    "    But in the future you can export the results into a text file or database.\n",
    "    \"\"\"\n",
    "    def output_results(self, r):\n",
    "        self.output.extend(r)\n",
    "        print('Se agregaron %i url a la lista' % (len(r)))\n",
    "    \"\"\"\n",
    "    After the class is instantiated, call this function to start the scraping jobs.\n",
    "    This function uses a FOR loop to call `scrape_url()` for each url to scrape.\n",
    "    \"\"\"\n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            if self.sleep_interval > 0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "                self.scrape_url(self.url_pattern % i)\n",
    "            else:\n",
    "                self.scrape_url(self.url_pattern % i)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estructura del sítio resulto ser no tan mala, sin duda una más que utiliza los div de manera impulsiva pero encontramos el url del modelo en los \"div\" de clase \"thumbnail thumbnail-md\".\n",
    "\n",
    "La estructura para el paginado es la siguiente:\n",
    "\n",
    "\"https://www.turbosquid.com/Search/3D-Models?page_num=2&sort_column=a5&sort_order=asc\"\n",
    "\n",
    "Ya que he utilizado sus herramientas de filtrado para ordenar de menor a mayor costo, y tener los modelos gratuitos al inicio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una prueba para la página 0\n",
    "response = requests.get('https://www.turbosquid.com/Search/3D-Models?sort_column=a5&sort_order=asc')\n",
    "print(response)\n",
    "content = response.content\n",
    "soup_prueba = BeautifulSoup(content,'html')\n",
    "divs_modelos = soup_prueba.find_all('div',{'class':'thumbnail thumbnail-md'})\n",
    "divs_modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para obtener el url del modelo necesitamos llegar a:\n",
    "divs_modelos[0].select('a')[0]['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando esto a toda la página de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_modelos = [div.select('a')[0]['href'] for div in divs_modelos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tenemos %i modelos por página' % len(urls_modelos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dentro de la página hay %i modelos' % (100*7668) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto la función para el scraping de las url de los modelos queda de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_parser(content):\n",
    "    soup_prueba = BeautifulSoup(content,'html')\n",
    "    divs_modelos = soup_prueba.find_all('div',{'class':'thumbnail thumbnail-md'})\n",
    "    urls_modelos = [div.select('a')[0]['href'] for div in divs_modelos]\n",
    "    return urls_modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D models\n",
    "# https://www.turbosquid.com/Search/3D-Models?sort_column=a5&sort_order=asc\n",
    "URL_PATTERN = 'https://www.turbosquid.com/Search/3D-Models?page_num=%i&sort_column=a5&sort_order=asc' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 10 # how many webpages to scrapge\n",
    "SLEEP_INTERVAL = 1\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "project_spider = WebSpider(URL_PATTERN,PAGES_TO_SCRAPE,SLEEP_INTERVAL, content_parser = web_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "urls_modelos_pages = project_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un siguiente paso es obtener información sobre cada uno de los modelos entrando a la url obtenida y realizar scraping de nuevo, en este caso nos interesa obtener la siguiente información:\n",
    "\n",
    "1. Nombre del modelo, div class productTitle\n",
    "2. Dueño del modelo, div class productArtist\n",
    "3. Precio del modelo, div class priceSection price\n",
    "4. Licencia de uso, div class LicenseUses\n",
    "5. Fecha de publicación\n",
    "6. Formatos incluidos, tabla clase exchange\n",
    "7. Categorias agregadas al modelo, accediendo al div class categorySection\n",
    "8. Tags agragados al modelo, accediendo al div class tagSection\n",
    "9. Descripción del modelo, accediendo al div class descriptionSection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una nueva clase de Spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelsSpider:\n",
    "    \"\"\"\n",
    "    This is the constructor class to which you can pass a bunch of parameters. \n",
    "    These parameters are stored to the class instance variables so that the\n",
    "    class functions can access them later.\n",
    "    \n",
    "    url_pattern: the regex pattern of the web urls to scape\n",
    "    pages_to_scrape: how many pages to scrape\n",
    "    sleep_interval: the time interval in seconds to delay between requests. If <0, requests will not be delayed.\n",
    "    content_parser: a function reference that will extract the intended info from the scraped content.\n",
    "    \"\"\"\n",
    "    def __init__(self, urls_pages, sleep_interval=-1, content_parser=None):\n",
    "        self.urls_pages = urls_pages\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "        self.output = []\n",
    "    \"\"\"\n",
    "    Scrape the content of a single url.\n",
    "    \"\"\"\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        if str(response) != '<Response [200]>':\n",
    "            print('Error en la respuesta del servidor',response)\n",
    "        elif str(response) == '<Response [408]>':\n",
    "            print('Error en el limite de tiempo de respuesta del servidor')\n",
    "        elif str(response) == '<Response [429]>':\n",
    "            print('Error demasiadas peticiones')\n",
    "        # I didn't find the SSL error but I add a 404 error catching.\n",
    "        elif str(response) == '<Response [404]>':\n",
    "            print('No se encontro el contenido')\n",
    "        else:\n",
    "            result = self.content_parser(response.content)\n",
    "            self.output_results(result)\n",
    "    \n",
    "    \"\"\"\n",
    "    Export the scraped content. Right now it simply print out the results.\n",
    "    But in the future you can export the results into a text file or database.\n",
    "    \"\"\"\n",
    "    def output_results(self, r):\n",
    "        self.output.append(r)\n",
    "    \"\"\"\n",
    "    After the class is instantiated, call this function to start the scraping jobs.\n",
    "    This function uses a FOR loop to call `scrape_url()` for each url to scrape.\n",
    "    \"\"\"\n",
    "    def kickstart(self):\n",
    "        for i in self.urls_pages:\n",
    "            if self.sleep_interval > 0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "                self.scrape_url(i)\n",
    "            else:\n",
    "                self.scrape_url(i)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un nuevo parser para extraer los datos de cada página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_parser(content):\n",
    "    check_content = lambda x: True if x != [] else False\n",
    "    soup_prueba = BeautifulSoup(content,'html')\n",
    "    try: \n",
    "        check_content(soup_prueba.find_all('div',{'class':'productTitle'})[0].select('h1')[0]['content'])\n",
    "        nombre = soup_prueba.find_all('div',{'class':'productTitle'})[0].select('h1')[0]['content']\n",
    "    except:\n",
    "        nombre = ''\n",
    "    try: \n",
    "        check_content(soup_prueba.find_all('div',{'class':'productArtist'})[0].text[3:])\n",
    "        dueño = soup_prueba.find_all('div',{'class':'productArtist'})[0].text[3:]\n",
    "    except:\n",
    "        dueño = ''\n",
    "    try: \n",
    "        check_content(soup_prueba.find_all('div',{'class':'priceSection price'})[0].text)\n",
    "        precio = soup_prueba.find_all('div',{'class':'priceSection price'})[0].text\n",
    "    except:\n",
    "        precio = ''\n",
    "    try: \n",
    "        check_content(soup_prueba.find_all('div',{'class':'LicenseUses'})[0].text)\n",
    "        licencia = soup_prueba.find_all('div',{'class':'LicenseUses'})[0].text\n",
    "    except:\n",
    "        licencia = ''\n",
    "    try: \n",
    "        check_content(soup_prueba.find_all('table',{'class':'SpecificationTable'})[0].select('time')[0]['datetime'])\n",
    "        fecha_pub = soup_prueba.find_all('table',{'class':'SpecificationTable'})[0].select('time')[0]['datetime']\n",
    "    except:\n",
    "        fecha_pub = ''\n",
    "    try: \n",
    "        check_content(soup_prueba.find_all('table',{'class':'exchange'})[0].text)\n",
    "        formatos = soup_prueba.find_all('table',{'class':'exchange'})[0].text\n",
    "    except:\n",
    "        formatos = ''\n",
    "    try: \n",
    "        check_content(soup_prueba.find_all('div',{'class':'FeatureGraphCategories'})[0].select('a'))\n",
    "        categorias = soup_prueba.find_all('div',{'class':'FeatureGraphCategories'})[0].select('a')\n",
    "        links_categorias = [categoria['href'] for categoria in categorias]\n",
    "        list_categorias = [categoria.text for categoria in categorias]\n",
    "    except:\n",
    "        links_categorias = []\n",
    "        list_categorias = []\n",
    "    try: \n",
    "        check_content(soup_prueba.find_all('div',{'class':'tagSection'})[0].select('a'))\n",
    "        tags = soup_prueba.find_all('div',{'class':'tagSection'})[0].select('a')\n",
    "        links_tags = [tag['href'] for tag in tags]\n",
    "        list_tags = [tag.text for tag in tags]\n",
    "    except:\n",
    "        links_tags = []\n",
    "        list_tags = []\n",
    "    try: \n",
    "        check_content(soup_prueba.find_all('div',{'class':'descriptionSection'})[0].select('.descriptionContentParagraph')[0].text)\n",
    "        descripcion = soup_prueba.find_all('div',{'class':'descriptionSection'})[0].select('.descriptionContentParagraph')[0].text\n",
    "    except:\n",
    "        descripcion = ''\n",
    "    row_dataset = [nombre,dueño,precio,licencia,fecha_pub,formatos,list_categorias,links_categorias,list_tags,links_tags,descripcion]\n",
    "    return row_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_INTERVAL = 0.5\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "project_spider_models = ModelsSpider(urls_modelos_pages,SLEEP_INTERVAL, content_parser = model_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "rows_dataset = project_spider_models.kickstart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scraping = pd.DataFrame(rows_dataset)\n",
    "dataset_scraping.columns = [['name_model','owner','price','license','published_date','formats_available','list_categories','links_categories','list_tags','links_tags','description']]\n",
    "dataset_scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por el momento he realizado el scraping a 10 páginas del sitio, con cien modelos, lo que representa 1000 requests. He mantenido este número por la posibilidad de que bloquen la dirección IP, sin embargo puede modificarse el parametro del primer Spider y tratar de obtener los url de todos los modelos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos en archivo csv los resultados obtenidos.\n",
    "dataset_scraping.to_csv('dataset_scraping.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario aplicar una limpieza a los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos la cantida de valores vacios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_scraping = dataset_scraping.isna()\n",
    "missing_values_scraping = missing_values_scraping.sum()\n",
    "missing_values_scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columna name_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso podemos darle un formato más limpio a los nombres (capitalize) y eliminar palabras que son innecesarias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the name we now that all are 3D models, so we remove this word for all the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scraping.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I had a problem because I got a multiindex dataframe, so I had to apply some extra steps each time I wanted to modify a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = dataset_scraping['name_model']\n",
    "nombres = [nombres.loc[[i]].values[0][0] for i in range(len(nombres))]\n",
    "nombres = [nombre.replace('3D ','').replace('model ','').replace('3D','').replace('model','').capitalize() for nombre in nombres]\n",
    "dataset_scraping['name_model'] = nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columna price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero nos damos una idea de la variedad de precios en esta columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = dataset_scraping['price']\n",
    "prices = [prices.loc[[i]].values[0][0] for i in range(len(prices))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que dentro de los primeros 1000 modelos todos son gratuitos, por lo que solo limpiamos un poco el string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scraping['price'] = [re.sub(r'\\n','',price) for price in prices]\n",
    "dataset_scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columna formats_available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta columna necesitamos un formao similar a la columna price, solo que generamos una lista de formatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatos = dataset_scraping['formats_available']\n",
    "formatos = [formatos.loc[[i]].values[0][0] for i in range(len(formatos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scraping['formats_available'] = [re.findall(r'\\b\\w+\\s(?:\\w+)?',str(formato)) for formato in formatos]\n",
    "dataset_scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente podemos hacer un reordenamiento de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orden_columnas = ['name_model', 'description','owner','price','license','published_date',\n",
    "                  'formats_available','list_categories','links_categories','list_tags','links_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scraping = dataset_scraping[orden_columnas]\n",
    "dataset_scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sustituimos los valores nulos y listas vacias por la cadena \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scraping.fillna('No disponible')\n",
    "change_to_unknown = lambda x: 'Unknown' if x == [] else x\n",
    "for i in dataset_scraping.columns:\n",
    "    dataset_scraping[i] = dataset_scraping[i].apply(change_to_unknown)\n",
    "dataset_scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el dataset limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scraping.to_csv('datase_scraping_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos obtener las categorias o etiquetas más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_categorias = dataset_scraping['list_categories']\n",
    "lista_categorias = [lista_categorias.loc[[i]].values[0][0] for i in range(len(lista_categorias))]\n",
    "lista_categorias_total = [categoria for lista in lista_categorias for categoria in lista]\n",
    "set_categorias = set(lista_categorias_total)\n",
    "dict_freq_categories = {k:lista_categorias_total.count(k) for k in set_categorias}\n",
    "freq_categories_order = sorted(dict_freq_categories.items(), key=operator.itemgetter(1))\n",
    "print('Categorias más frecuentes en orden ascendente:')\n",
    "print(freq_categories_order[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_etiquetas = dataset_scraping['list_tags']\n",
    "lista_etiquetas = [lista_etiquetas.loc[[i]].values[0][0] for i in range(len(lista_etiquetas))]\n",
    "lista_etiquetas_total = [etiqueta for lista in lista_etiquetas for etiqueta in lista]\n",
    "set_etiquetas = set(lista_etiquetas_total)\n",
    "dict_freq_tags = {k:lista_etiquetas_total.count(k) for k in set_etiquetas}\n",
    "freq_tags_order = sorted(dict_freq_tags.items(), key=operator.itemgetter(1))\n",
    "print('Categorias más frecuentes en orden ascendente:')\n",
    "print(freq_tags_order[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscar los \"dueños\" más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dueños = dataset_scraping['owner']\n",
    "dueños = [dueños.loc[[i]].values[0][0] for i in range(len(dueños))]\n",
    "len(set(dueños))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dueños_dict = {k:dueños.count(k) for k in set(dueños)}\n",
    "freq_dueños_order = sorted(dueños_dict.items(), key=operator.itemgetter(1))\n",
    "print('Los dueños con más modelos en orden ascendente:')\n",
    "print(freq_dueños_order[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La posibilidad de filtrar para encontrar información relevante sobre que dueño es el más activo con base en la fecha de publicación de sus modelos y el número de modelos es posible calcularla con base en el dataset limpio.\n",
    "\n",
    "Ahora bien como podemos ver tambien sería interesante analizar información sobre los modelos de pago que desgrciadamente no alcanzamos a obtener debido a la inmensa cantidad de modelos que hay en la página. Por ahora se han prácticado las habilidades de uso de APIs y web scraping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
