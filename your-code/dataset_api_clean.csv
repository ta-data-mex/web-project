,title,subtitle,description,lastUpdated,ref,totalGigaBytes,url,numberOfTags,downloadCount,licenseName,kernelCount,lastVersion,usabilityRating,category
0,Every Cryptocurrency Daily Market Price,"Daily crypto markets open, close, low, high data for every token ever","# Cryptocurrency Market Data
## Historical Cryptocurrency Prices For ALL Tokens!

### **Summary**
	&gt; Observations: 758,534
	&gt; Variables: 13  
	&gt; Crypto Tokens: 1,584
	&gt; Start Date: 28/04/2017  
	&gt; End Date: 21/05/2018  
  
### **Description**
   All historic open, high, low, close, trading volume and market cap info for all cryptocurrencies.  

   I've had to go over the code with a fine tooth comb to get it compatible with CRAN so there have been significant enhancements to how some of the field conversions have been undertaken and the data being cleaned. This should eliminate a few issues around number formatting or unexpected handling of scientific notations.  
  
### **Data Structure**
    Observations: 649,051    
    Variables: 13    
    $ slug        ",2018-12-01T13:56:58.277Z,jessevent/all-crypto-currencies,23.636187,https://www.kaggle.com/jessevent/all-crypto-currencies,3,9042,Other (specified in description),63,"Ready version: 17, 2018-12-01T13:56:58.277Z",0.852941155,currencies
1,Crypto Currencies,Cryptocurrency Market Capitalizations,"Â«Datasets per la comparaciÃ³ de moviments i patrons entre els principals Ã­ndexs borsatils espanyols i les crypto-monedesÂ»

### Context

En aquest cas el context Ã©s detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polÃ­tics, econÃ²mics, etc...), en els principals Ã­ndexs borsatils espanyols i de les crypto-monedes.

Hem seleccionat diferents fonts de dades per generar fitxers Â«csvÂ», guardar diferents valors en el mateix perÃ­ode de temps. Ã‰s important destacar que ens interessa mÃ©s les tendÃ¨ncies alcistes o baixes, que podem calcular o recuperar en aquests perÃ­odes de temps.

### Content

En aquest cas el contingut estÃ  format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals sâ€™ha generat un fitxer per dia del perÃ­ode de temps estudiat.

Pel que fa als moviments del principals Ã­ndexs borsatils sâ€™ha generat una carpeta per dia del perÃ­ode, en cada directori un fitxer amb cadascun del noms dels Ã­ndexs. Degut aixÃ² sâ€™han comprimit aquests Ãºltims abans de publicar-los en el directori de Â«open dataÂ» kaggle.com.

Pel que fa als camps, ens interessÃ  detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patrÃ³ similar en les cryptomonedes i els Ã­ndexs. Els camps especialment destacats sÃ³n:

    â€¢ Nom: Nom empresa o cryptomoneda;
    â€¢ Preu: Valor en euros dâ€™una acciÃ³ o una cryptomoneda;
    â€¢ Volum: En euros/volum 24 hores,acumulat de les transaccions diÃ ries en milions dâ€™euros
    â€¢ Simbol: SÃ­mbol o acrÃ²nim de la moneda
    â€¢ Cap de mercat: Valor total de totes les monedes en el moment actual
    â€¢ Oferta circulant: Valor en oportunitat de negoci
    â€¢ % 1h, % 2h i %7d, tant per cent del valor la moneda en 1h, 2h o 7d sobre la resta de cyprtomonedes.

### Acknowledgements

En aquest cas les fonts de dades que sâ€™han utilitzat per a la realitzaciÃ³ dels datasets corresponent a:

 - http://www.eleconomista.es
 - https://coinmarketcap.com

Per aquest fet, les dades de borsa i crypto-moneda estan en Ãºltima instÃ ncia sota llicÃ¨ncia de les webs respectivament.
Pel que fa a la terminologia financera podem veure vocabulari en renta4banco.  
[https://www.r4.com/que-necesitas/formacion/diccionario]

### Inspiration

Hi ha un estudi anterior on poder tenir primÃ­cies de com han enfocat els algoritmes:     

 - https://arxiv.org/pdf/1410.1231v1.pdf

En aquest cas el Â«tradingÂ» en cryptomoneda Ã©s relativament nou, forÃ§a popular per la seva formulaciÃ³ com a mitja digital dâ€™intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjÃ  dâ€™un entramat dâ€™agents.

La comunitat podrÃ  respondre, entre altres preguntes, a:

 - EstÃ  afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del paÃ­s d'Espanya?
 - Els efectes o agents externs afecten per igual a les accions o cryptomonedes? 
 - Hi ha relacions cause efecte entre les acciones i cryptomonedes?

### Project repository
https://github.com/acostasg/scraping

### Datasets
Els fitxers csv generats que componen el dataset sâ€™han publicat en el repositori kaggle.com:

* https://www.kaggle.com/acostasg/stock-index/ 
* https://www.kaggle.com/acostasg/crypto-currencies

Per una banda, els fitxers els Â«stock-indexÂ» estan comprimits per carpetes amb la data dâ€™extracciÃ³ i cada fitxer amb el nom dels Ã­ndexs borsatil.  De forma diferent, les cryptomonedes aquestes estan dividides per fitxer on sÃ³n totes les monedes amb la data dâ€™extracciÃ³.",2017-12-03T18:55:04.34Z,acostasg/crypto-currencies,1.321667,https://www.kaggle.com/acostasg/crypto-currencies,2,565,"Database: Open Database, Contents: Database Contents",2,"Ready version: 8, 2017-12-03T18:55:04.34Z",0.7058824,currencies
2,Analysis about crypto currencies and Stock Index,Relation and patterns between movements of stock exchange indexes and cryptocurrency,"Â«Datasets per la comparaciÃ³ de moviments i patrons entre els principals Ã­ndexs borsatils espanyols i les crypto-monedesÂ»

### Context

En aquest cas el context Ã©s detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polÃ­tics, econÃ²mics, etc...), en els principals Ã­ndexs borsatils espanyols i de les crypto-monedes.

Hem seleccionat diferents fonts de dades per generar fitxers Â«csvÂ», guardar diferents valors en el mateix perÃ­ode de temps. Ã‰s important destacar que ens interessa mÃ©s les tendÃ¨ncies alcistes o baixes, que podem calcular o recuperar en aquests perÃ­odes de temps.

### Content

En aquest cas el contingut estÃ  format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals sâ€™ha generat un fitxer per dia del perÃ­ode de temps estudiat.

Pel que fa als moviments del principals Ã­ndexs borsatils sâ€™ha generat una carpeta per dia del perÃ­ode, en cada directori un fitxer amb cadascun del noms dels Ã­ndexs. Degut aixÃ² sâ€™han comprimit aquests Ãºltims abans de publicar-los en el directori de Â«open dataÂ» kaggle.com.

Pel que fa als camps, ens interessÃ  detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patrÃ³ similar en les cryptomonedes i els Ã­ndexs. Els camps especialment destacats sÃ³n:

    â€¢ Data: Data de la observaciÃ³
    â€¢ Nom: Nom empresa o cryptomoneda, per identificar de quina moneda o index estem representant.
    â€¢ SÃ­mbol: SÃ­mbol de la moneda o del index borsatil, per realitzar grÃ fic posteriorment dâ€™una forma mes senzilla que el nom.
    â€¢ Preu: Valor en euros dâ€™una acciÃ³ o una cryptomoneda (transformarem la moneda a euros en el cas de estigui en dÃ²lars amb l'Ãºltima cotitzaciÃ³ (un dollar a 0,8501 euro)
    â€¢ Tipus_cotitzacio: Valor nou que agregarem per discretitzar entre la cotitzaciÃ³: baix (0 i 1), normal (1 i 100), alt (100 i 1000), molt_alt (&gt;1000)

# Script R

* AnÃ lisis de les observacions i el domini de les dades
* AnÃ lisis en especial de Bitcoin i la IOTA.
* Test de Levene per veure la homogeneitat
* Kmeans per creaciÃ³ de cluster per veure la homegeneitat
* FreqÃ¼Ã¨ncies de les distribucions
* Test de contrast d'hipÃ²tesis de variables dependents (Wilcoxon)
* Test de Shapiro-Wilk per veure la normalitat de les dades, per normalitzar-les o no
* CorrelaciÃ³ d'Ã­ndexs borsatils, per eliminar complexitat dels Ã­ndexs amb grau mÃ©s alt de correlaciÃ³
* IteraciÃ³ de Regressions lineals per obtenir el model amb mÃ©s qualitat, observa'n el p-valor i l'Ã­ndex de correlaciÃ³
* ValidaciÃ³ de la qualitat del model
* RepresentaciÃ³ grafica

### Acknowledgements

En aquest cas les fonts de dades que sâ€™han utilitzat per a la realitzaciÃ³ dels datasets corresponent a:

 - http://www.eleconomista.es
 - https://coinmarketcap.com

Per aquest fet, les dades de borsa i crypto-moneda estan en Ãºltima instÃ ncia sota llicÃ¨ncia de les webs respectivament.
Pel que fa a la terminologia financera podem veure vocabulari en renta4banco.  
[https://www.r4.com/que-necesitas/formacion/diccionario]

### Inspiration

Hi ha un estudi anterior on poder tenir primÃ­cies de com han enfocat els algoritmes:     

 - https://arxiv.org/pdf/1410.1231v1.pdf

En aquest cas el Â«tradingÂ» en cryptomoneda Ã©s relativament nou, forÃ§a popular per la seva formulaciÃ³ com a mitja digital dâ€™intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjÃ  dâ€™un entramat dâ€™agents.

La comunitat podrÃ  respondre, entre altres preguntes, a:

 - EstÃ  afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del paÃ­s d'Espanya?
 - Els efectes o agents externs afecten per igual a les accions o cryptomonedes? 
 - Hi ha relacions cause efecte entre les acciones i cryptomonedes?

### Project repository
https://github.com/acostasg/scraping

### Datasets
Els fitxers csv generats que componen el dataset sâ€™han publicat en el repositori kaggle.com:

* https://www.kaggle.com/acostasg/stock-index/ 
* https://www.kaggle.com/acostasg/crypto-currencies

Per una banda, els fitxers els Â«stock-indexÂ» estan comprimits per carpetes amb la data dâ€™extracciÃ³ i cada fitxer amb el nom dels Ã­ndexs borsatil.  De forma diferent, les cryptomonedes aquestes estan dividides per fitxer on sÃ³n totes les monedes amb la data dâ€™extracciÃ³.",2017-12-13T22:38:33.32Z,acostasg/cryptocurrenciesvsstockindex,0.681413,https://www.kaggle.com/acostasg/cryptocurrenciesvsstockindex,3,515,"Database: Open Database, Contents: Â© Original Authors",1,"Ready version: 2, 2017-12-13T22:38:33.32Z",0.7058824,currencies
3,Currencies,,"### Context

This is a different timeframe currencies csv from a trading program


### Content
There are 42 . csv of different dataframes and currencies.


### Acknowledgements

I acknowlege every person in the world who spend their time sharing  there knowledge from youtube to blogs. That people make  world a better place to live.  


### Inspiration

I want to create code to select the ones that have a tf of 1 day amonng others. My intention is create a code to help us to select the documents we want inside a messy folder.",2017-09-24T19:50:59.687Z,mitillo/currencies,1.151577,https://www.kaggle.com/mitillo/currencies,0,134,Unknown,1,"Ready version: 3, 2017-09-24T19:50:59.687Z",0.4117647,currencies
4,Currency Exchange Rates,Daily exchange rates for 51 currencies from 1995 to 2018,"This dataset contains the daily currency exchange rates as reported to the *International Monetary Fund* by the issuing central bank. Included are 51 currencies over the period from 01-01-1995 to 11-04-2018.

The format is known as *currency units per U.S. Dollar*. Explained by example, each rate in the *Euro* column says how much *U.S. Dollar* you had to pay at a certain date to buy 1 Euro. Hence, the rates in the column *U.S. Dollar* are always `1`.",2018-05-02T17:48:28.943Z,thebasss/currency-exchange-rates,0.596854,https://www.kaggle.com/thebasss/currency-exchange-rates,1,521,CC0: Public Domain,0,"Ready version: 2, 2018-05-02T17:48:28.943Z",0.647058845,currencies
5,Crypto Currencies,Cryptocurrency Market Capitalizations,"Â«Datasets per la comparaciÃ³ de moviments i patrons entre els principals Ã­ndexs borsatils espanyols i les crypto-monedesÂ»

### Context

En aquest cas el context Ã©s detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polÃ­tics, econÃ²mics, etc...), en els principals Ã­ndexs borsatils espanyols i de les crypto-monedes.

Hem seleccionat diferents fonts de dades per generar fitxers Â«csvÂ», guardar diferents valors en el mateix perÃ­ode de temps. Ã‰s important destacar que ens interessa mÃ©s les tendÃ¨ncies alcistes o baixes, que podem calcular o recuperar en aquests perÃ­odes de temps.

### Content

En aquest cas el contingut estÃ  format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals sâ€™ha generat un fitxer per dia del perÃ­ode de temps estudiat.

Pel que fa als moviments del principals Ã­ndexs borsatils sâ€™ha generat una carpeta per dia del perÃ­ode, en cada directori un fitxer amb cadascun del noms dels Ã­ndexs. Degut aixÃ² sâ€™han comprimit aquests Ãºltims abans de publicar-los en el directori de Â«open dataÂ» kaggle.com.

Pel que fa als camps, ens interessÃ  detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patrÃ³ similar en les cryptomonedes i els Ã­ndexs. Els camps especialment destacats sÃ³n:

    â€¢ Nom: Nom empresa o cryptomoneda;
    â€¢ Preu: Valor en euros dâ€™una acciÃ³ o una cryptomoneda;
    â€¢ Volum: En euros/volum 24 hores,acumulat de les transaccions diÃ ries en milions dâ€™euros
    â€¢ Simbol: SÃ­mbol o acrÃ²nim de la moneda
    â€¢ Cap de mercat: Valor total de totes les monedes en el moment actual
    â€¢ Oferta circulant: Valor en oportunitat de negoci
    â€¢ % 1h, % 2h i %7d, tant per cent del valor la moneda en 1h, 2h o 7d sobre la resta de cyprtomonedes.

### Acknowledgements

En aquest cas les fonts de dades que sâ€™han utilitzat per a la realitzaciÃ³ dels datasets corresponent a:

 - http://www.eleconomista.es
 - https://coinmarketcap.com

Per aquest fet, les dades de borsa i crypto-moneda estan en Ãºltima instÃ ncia sota llicÃ¨ncia de les webs respectivament.
Pel que fa a la terminologia financera podem veure vocabulari en renta4banco.  
[https://www.r4.com/que-necesitas/formacion/diccionario]

### Inspiration

Hi ha un estudi anterior on poder tenir primÃ­cies de com han enfocat els algoritmes:     

 - https://arxiv.org/pdf/1410.1231v1.pdf

En aquest cas el Â«tradingÂ» en cryptomoneda Ã©s relativament nou, forÃ§a popular per la seva formulaciÃ³ com a mitja digital dâ€™intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjÃ  dâ€™un entramat dâ€™agents.

La comunitat podrÃ  respondre, entre altres preguntes, a:

 - EstÃ  afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del paÃ­s d'Espanya?
 - Els efectes o agents externs afecten per igual a les accions o cryptomonedes? 
 - Hi ha relacions cause efecte entre les acciones i cryptomonedes?

### Project repository
https://github.com/acostasg/scraping

### Datasets
Els fitxers csv generats que componen el dataset sâ€™han publicat en el repositori kaggle.com:

* https://www.kaggle.com/acostasg/stock-index/ 
* https://www.kaggle.com/acostasg/crypto-currencies

Per una banda, els fitxers els Â«stock-indexÂ» estan comprimits per carpetes amb la data dâ€™extracciÃ³ i cada fitxer amb el nom dels Ã­ndexs borsatil.  De forma diferent, les cryptomonedes aquestes estan dividides per fitxer on sÃ³n totes les monedes amb la data dâ€™extracciÃ³.",2017-11-07T20:19:07.32Z,acostasg/crypto-currencies-data,1.082628,https://www.kaggle.com/acostasg/crypto-currencies-data,2,160,"Database: Open Database, Contents: Database Contents",0,"Ready version: 1, 2017-11-07T20:19:07.32Z",0.647058845,currencies
6,World Coins,A collection of coin images from 32 different currencies.,"### Context

I put together this dataset when I decided to build a Deep Learning model to detect a coin from an image.


### Content

A collection of 211 different coins from 32 currencies. 

* ``cat_to_name.json`` maps the folder id with a specific coin.

* ``data`` contains all the coin images. The dataset has already been splitted in train, validation and test.


### Acknowledgements

**I am not the owner of this data. Most of these images have been downloaded from  [ucoin.net][1]  and other online sources.  These images may be subject of copyright.**


  [1]: http://ucoin.net",2019-03-27T09:26:10.133Z,wanderdust/coin-images,480.602984,https://www.kaggle.com/wanderdust/coin-images,4,69,Other (specified in description),1,"Ready version: 1, 2019-03-27T09:26:10.133Z",0.9375,currencies
7,Cryptocurrency Historical Prices,"Prices of top cryptocurrencies including Bitcoin, Ethereum, Ripple, Bitcoin cash","### Context

Things like Block chain, Bitcoin, Bitcoin cash, Ethereum, Ripple etc are constantly coming in the news articles I read. So I wanted to understand more about it and [this post][1] helped me get started. Once the basics are done, the data scientist inside me started raising questions like:

1. How many cryptocurrencies are there and what are their prices and valuations?
2. Why is there a sudden surge in the interest in recent days? 

For getting answers to all these questions (and if possible to predict the future prices ;)), I started collecting data from [coinmarketcap][2] about the cryptocurrencies. 



So what next? 
Now that we have the price data, I wanted to dig a little more about the factors affecting the price of coins. I started of with Bitcoin and there are quite a few parameters which affect the price of Bitcoin. Thanks to [Blockchain Info][3], I was able to get quite a few parameters on once in two day basis.

This will help understand the other factors related to Bitcoin price and also help one make future predictions in a better way than just using the historical price.


### Content

The dataset has one csv file for each currency. Price history is available on a daily basis from April 28, 2013.  This dataset has the historical price information of some of the top crypto currencies by market capitalization. The currencies included are:

 - Bitcoin
 - Ethereum
 - Ripple
 - Bitcoin cash
 - Bitconnect
 - Dash
 - Ethereum Classic
 - Iota
 - Litecoin
 - Monero
 - Nem
 - Neo
 - Numeraire
 - Stratis
 - Waves



 - Date : date of observation 
 - Open : Opening price on the given day
 - High : Highest price on the given day
 - Low : Lowest price on the given day
 - Close : Closing price on the given day
 - Volume : Volume of transactions on the given day
 - Market Cap : Market capitalization in USD

**Bitcoin Dataset (bitcoin_dataset.csv) :**

This dataset has the following features.

 - Date : Date of observation
 - btc_market_price : Average USD market price across major bitcoin exchanges.
 - btc_total_bitcoins : The total number of bitcoins that have already been mined.
 - btc_market_cap : The total USD value of bitcoin supply in circulation.
 - btc_trade_volume : The total USD value of trading volume on major bitcoin exchanges.
 - btc_blocks_size : The total size of all block headers and transactions.
 - btc_avg_block_size : The average block size in MB.
 - btc_n_orphaned_blocks : The total number of blocks mined but ultimately not attached to the main Bitcoin blockchain.
 - btc_n_transactions_per_block : The average number of transactions per block.
 - btc_median_confirmation_time : The median time for a transaction to be accepted into a mined block.
 - btc_hash_rate : The estimated number of tera hashes per second the Bitcoin network is performing.
 - btc_difficulty : A relative measure of how difficult it is to find a new block.
 - btc_miners_revenue : Total value of coinbase block rewards and transaction fees paid to miners.
 - btc_transaction_fees : The total value of all transaction fees paid to miners.
 - btc_cost_per_transaction_percent : miners revenue as percentage of the transaction volume.
 - btc_cost_per_transaction : miners revenue divided by the number of transactions.
 - btc_n_unique_addresses : The total number of unique addresses used on the Bitcoin blockchain.
 - btc_n_transactions : The number of daily confirmed Bitcoin transactions.
 - btc_n_transactions_total : Total number of transactions.
 - btc_n_transactions_excluding_popular : The total number of Bitcoin transactions, excluding the 100 most popular addresses.
 - btc_n_transactions_excluding_chains_longer_than_100 : The total number of Bitcoin transactions per day excluding long transaction chains.
 - btc_output_volume : The total value of all transaction outputs per day.
 - btc_estimated_transaction_volume : The total estimated value of transactions on the Bitcoin blockchain.
 - btc_estimated_transaction_volume_usd : The estimated transaction value in USD value.

**Ethereum Dataset (ethereum_dataset.csv):**

This dataset has the following features

 - Date(UTC) : Date of transaction
 - UnixTimeStamp : unix timestamp
 - eth_etherprice : price of ethereum
 - eth_tx : number of transactions per day
 - eth_address : Cumulative address growth
 - eth_supply : Number of ethers in supply
 - eth_marketcap : Market cap in USD
 - eth_hashrate : hash rate in GH/s
 - eth_difficulty : Difficulty level in TH
 - eth_blocks : number of blocks per day
 - eth_uncles : number of uncles per day
 - eth_blocksize : average block size in bytes
 - eth_blocktime : average block time in seconds
 - eth_gasprice : Average gas price in Wei
 - eth_gaslimit : Gas limit per day
 - eth_gasused : total gas used per day
 - eth_ethersupply : new ether supply per day
 - eth_chaindatasize : chain data size in bytes
 - eth_ens_register : Ethereal Name Service (ENS) registrations per day



### Acknowledgements

This data is taken from [coinmarketcap][5] and it is [free][6] to use the data.

Bitcoin dataset is obtained from [Blockchain Info][7].

Ethereum dataset is obtained from [Etherscan][8].

Cover Image : Photo by Thomas Malama on Unsplash

### Inspiration

Some of the questions which could be inferred from this dataset are:

 1. How did the historical prices / market capitalizations of various currencies change over time?
 2. Predicting the future price of the currencies
 3. Which currencies are more volatile and which ones are more stable?
 4. How does the price fluctuations of currencies correlate with each other?
 5. Seasonal trend in the price fluctuations

Bitcoin / Ethereum dataset could be used to look at the following:

 1. Factors affecting the bitcoin / ether price.
 2. Directional prediction of bitcoin / ether price. (refer [this paper][9] for more inspiration)
 3. Actual bitcoin price prediction.
 


  [1]: https://www.linkedin.com/pulse/blockchain-absolute-beginners-mohit-mamoria
  [2]: https://coinmarketcap.com/
  [3]: https://blockchain.info/
  [4]: https://etherscan.io/charts
  [5]: https://coinmarketcap.com/
  [6]: https://coinmarketcap.com/faq/
  [7]: https://blockchain.info/
  [8]: https://etherscan.io/charts
  [9]: http://cs229.stanford.edu/proj2014/Isaac%20Madan,%20Shaurya%20Saluja,%20Aojia%20Zhao,Automated%20Bitcoin%20Trading%20via%20Machine%20Learning%20Algorithms.pdf",2018-02-21T12:36:47.22Z,sudalairajkumar/cryptocurrencypricehistory,0.715347,https://www.kaggle.com/sudalairajkumar/cryptocurrencypricehistory,2,19255,CC0: Public Domain,39,"Ready version: 13, 2018-02-21T12:36:47.22Z",0.7058824,currencies
8,Exchange rate BRIC currencies/US dollar,historical data monthly frequencies 01/07/1997 - 1/12/2015,"### Content

over 10 years of historical exchange rate data of BRIC countries currencies/ U.S. dollar",2017-06-15T14:52:31.757Z,luigimersico/exchange-rate-bric-currenciesus-dollar,0.003657,https://www.kaggle.com/luigimersico/exchange-rate-bric-currenciesus-dollar,3,102,Unknown,2,"Ready version: 1, 2017-06-15T14:52:31.757Z",0.5294118,currencies
9,Price History of 1654 Crypto-Currencies,Historical Coin Prices to Understand the Big Picture,"### Context

Here's one of the largest Crypto-Currency datasets. 


### Content

**1654 Coins** with *Open*, *Close*, *High*, *Low*, *Market Cap* and *Volume* values day by day since 2013.


### Acknowledgements

The data is from [coinmarketcap][1] as they allow everyone to use it for academic or journalistic purposes. I definitely encourage you to check out their [terms][2] before you use the data.


### Inspiration

You may use the data to understand the coin market and be creative about it.


### Contact

You can find me on [Twitter][3] if you want to talk about the data and crypto-currencies in general.


  [1]: https://coinmarketcap.com
  [2]: https://coinmarketcap.com/faq
  [3]: https://twitter.com/ulsc",2018-06-09T02:44:13.39Z,ulascengiz/price-history-of-1654-cryptocurrencies,19.131516,https://www.kaggle.com/ulascengiz/price-history-of-1654-cryptocurrencies,5,102,Other (specified in description),0,"Ready version: 1, 2018-06-09T02:44:13.39Z",0.6875,currencies
10,Complete Cryptocurrency Market History,Daily historical prices for all cryptocurrencies listed on CoinMarketCap,"### Cryptocurrencies

Cryptocurrencies are fast becoming rivals to traditional currency across the world. The digital currencies are available to purchase in many different places, making it accessible to everyone, and with retailers accepting various cryptocurrencies it could be a sign that money as we know it is about to go through a major change.

In addition, the blockchain technology on which many cryptocurrencies are based, with its revolutionary distributed digital backbone, has many other promising applications. Implementations of secure, decentralized systems can aid us in conquering organizational issues of trust and security that have plagued our society throughout the ages. In effect, we can fundamentally disrupt industries core to economies, businesses and social structures, eliminating inefficiency and human error.

### Content

The dataset contains all historical daily prices (open, high, low, close) for all cryptocurrencies listed on [CoinMarketCap].

### Acknowledgements

- [Every Cryptocurrency Daily Market Price] - I initially developed kernels for this dataset before making my own scraper and dataset so that I could keep it regularly updated.
- [CoinMarketCap]  - For the data

  [Every Cryptocurrency Daily Market Price]: https://www.kaggle.com/jessevent/all-crypto-currencies ""Every Cryptocurrency Daily Market Price""
  [CoinMarketCap]: https://coinmarketcap.com/ ""CoinMarketCap""",2018-09-29T15:17:17.567Z,taniaj/cryptocurrency-market-history-coinmarketcap,13.939281,https://www.kaggle.com/taniaj/cryptocurrency-market-history-coinmarketcap,5,3104,CC0: Public Domain,8,"Ready version: 9, 2018-09-29T15:17:17.567Z",0.7647059,currencies
11,Complete Historical Cryptocurrency Financial Data,Top 200 Cryptocurrencies by Marketcap,"**Context**

Recent growing interest in cryptocurrencies, specifically as a speculative investment vehicle, has sparked global conversation over the past 12 months. Although this data is available across various sites, there is a lack of understanding as to what is driving the exponential rise of many individual currencies. This data set is intended to be a starting point for a detailed analysis into what is driving price action, and what can be done to predict future movement.

**Content**

Consolidated financial information for the top 200 cryptocurrencies by marketcap. Pulled from CoinMarketCap.com. Attributes include:

 - Currency name (e.g. bitcoin)
 - Date  
 - Open
 - High
 - Low
 - Close
 - Volume
 - Marketcap

**Inspiration**

For the past few months I have been searching for a reliable source for historical price information related to cryptocurrencies. I wasn't able to find anything that I could use to my liking, so I built my own data set.

I've written a small script that scrapes historical price information for the top 200 coins by market cap as listed on CoinMarketCap.com.

I plan to run some basic analysis on it to answer questions that I have a ""gut"" feeling about, but no quantitative evidence (yet!).

Questions such as: 

 - What is the correlation between bitcoin and alt coin prices?
 - What is the average age of the top 10 coins by market cap?
 - What day of the week is best to buy/sell?
 - Which coins in the top two hundred are less than 6 months old?
 - Which currencies are the most volatile? 
 - What the hell happens when we go to bed and Asia starts trading?

Feel free to use this for your own purposes! I just ask that you share your results with the group when complete. Happy hunting!",2019-04-25T00:37:10.423Z,philmohun/cryptocurrency-financial-data,0.348672,https://www.kaggle.com/philmohun/cryptocurrency-financial-data,4,2851,CC0: Public Domain,4,"Ready version: 3, 2019-04-25T00:37:10.423Z",0.852941155,currencies
12,Exchange Rates,Exchange rates as far back as 1971 between the USA and 23 countries,"The Federal Reserve's H.10 statistical release provides data on exchange rates between the US dollar, 23 other currencies, and three benchmark indexes. The data extend back to 1971 for several of these.

Please note that the csv has six header rows. The first contains the 

## Acknowledgements
This dataset was provided by the [US Federal Reserve][1]. If you need the current version, you can find their weekly updates [here][2].

## Inspiration

 - Venezuela is both unusually dependent on oil revenues and experiencing unusual degrees of political upheaval. Can you determine which movements in their currency were driven by changes in the oil price and which were driven by political events?

  [1]: https://www.federalreserve.gov/aboutthefed.htm
  [2]: https://www.federalreserve.gov/releases/h10/Hist/",2017-09-05T20:29:37.953Z,federalreserve/exchange-rates,0.659356,https://www.kaggle.com/federalreserve/exchange-rates,2,1240,CC0: Public Domain,13,"Ready version: 1, 2017-09-05T20:29:37.953Z",0.8235294,currencies
13,Zomato Restaurants Data,Analyzing the best restaurants of the major cities,"### Context

I really get fascinated by good quality food being served in the restaurants and would like to help community find the best cuisines around their area

### Content

Zomato API Analysis is one of the most useful analysis for foodies who want to taste the best cuisines of every part of the world which lies in their budget. This analysis is also for those who want to find the value for money restaurants in various parts of the country for the cuisines. Additionally, this analysis caters the needs of people who are striving to get the best cuisine of the country and which locality of that country serves that cuisines with maximum number of restaurants.â™¨ï¸

For more information on Zomato API and Zomato API key
â€¢	Visit : https://developers.zomato.com/api#headline1
â€¢	Data Collection: https://developers.zomato.com/documentation

Data
Fetching the data:
â€¢	Data has been collected from the Zomato API in the form of .json files(raw data) using the url=https://developers.zomato.com/api/v2.1/search?entity_id=1&entity_type=city&start=1&count=20
â€¢	Raw data can be seen here

Data Collection:
Data collected can be seen as a raw .json file here

Data Storage:
The collected data has been stored in the Comma Separated Value file Zomato.csv. Each restaurant in the dataset is uniquely identified by its Restaurant Id. Every Restaurant contains the following variables:

â€¢	Restaurant Id: Unique id of every restaurant across various cities of the world
â€¢	Restaurant Name: Name of the restaurant
â€¢	Country Code: Country in which restaurant is located
â€¢	City: City in which restaurant is located
â€¢	Address: Address of the restaurant
â€¢	Locality: Location in the city
â€¢	Locality Verbose: Detailed description of the locality
â€¢	Longitude: Longitude coordinate of the restaurant's location
â€¢	Latitude: Latitude coordinate of the restaurant's location
â€¢	Cuisines: Cuisines offered by the restaurant
â€¢	Average Cost for two: Cost for two people in different currencies ğŸ‘« 
â€¢	Currency: Currency of the country
â€¢	Has Table booking: yes/no
â€¢	Has Online delivery: yes/ no
â€¢	Is delivering: yes/ no
â€¢	Switch to order menu: yes/no
â€¢	Price range: range of price of food
â€¢	Aggregate Rating: Average rating out of 5
â€¢	Rating color: depending upon the average rating color
â€¢	Rating text: text on the basis of rating of rating
â€¢	Votes: Number of ratings casted by people




### Acknowledgements

I would like to thank Zomato API for helping me collecting data


### Inspiration
Data Processing has been done on the following categories: 
Currency
City
Location
Rating Text",2018-03-13T04:56:25.81Z,shrutimehta/zomato-restaurants-data,5.732263,https://www.kaggle.com/shrutimehta/zomato-restaurants-data,1,19144,CC0: Public Domain,54,"Ready version: 2, 2018-03-13T04:56:25.81Z",0.7941176,currencies
14,Meta Numerai,Numerai Tournament Results,"### Context

Numer.ai tournament results


### Acknowledgements

1. The dataset was collected with [NumerApi](https://github.com/uuazed/numerapi)
2. The daily currency info is from [CoinMarketCap](https://coinmarketcap.com/currencies/numeraire/)

Photo by Alex Knight on Unsplash",2019-07-01T08:29:53.457Z,gaborfodor/meta-numerai,41.912215,https://www.kaggle.com/gaborfodor/meta-numerai,2,98,CC0: Public Domain,11,"Ready version: 36, 2019-07-01T08:29:53.457Z",0.7058824,currencies
15,Crypto Market Data,CoinMarketCap data from 1/May/13 to 8/5/18 of popular crypto currencies,"### Context

Historical data for crypto currencies. 


### Content

The dataset contains historical data of the last 5 years (1-May-13 to 8-May-18) from CoinMarketCap of the following crypto currencies.

 - Bitcoin
 - Ethereum
 - Lite coin
 - Ripple
 - Verge
 - Bitcoin Cash
 - Ethereum Classic
 - Neo
 - Nano
 - Dash
 - EOS
 - IOTA
 - Tron
 - Stellar

**Note: ** For the coins not older than 5 years, the dataset contains the data from their listing on CoinMarketCap

### Acknowledgements

The data has been scraped from **CoinMarketCap**


### Inspiration

Trend Analysis for currencies.
Symptoms of price change.
Reason of price crash.
Growth rate and possible market leader.",2018-05-08T14:38:02.187Z,anasshahid88/crypto-market-data,0.313499,https://www.kaggle.com/anasshahid88/crypto-market-data,3,163,CC0: Public Domain,0,"Ready version: 1, 2018-05-08T14:38:02.187Z",0.647058845,currencies
16,Ethereum Blockchain,Complete live historical Ethereum blockchain data (BigQuery),"## Context

Bitcoin and other cryptocurrencies have captured the imagination of technologists, financiers, and economists. Digital currencies are only one application of the underlying blockchain technology. Like its predecessor, Bitcoin, the [Ethereum][1] blockchain can be described as an immutable distributed ledger. However, creator Vitalik Buterin also extended the set of capabilities by including a virtual machine that can execute arbitrary code stored on the blockchain as smart contracts.

Both Bitcoin and Ethereum are essentially [OLTP][2] databases, and provide little in the way of [OLAP][3] (analytics) functionality. However the Ethereum dataset is notably distinct from the Bitcoin dataset:

* The Ethereum blockchain has as its primary unit of value Ether, while the Bitcoin blockchain has Bitcoin. However, the majority of value transfer on the Ethereum blockchain is composed of so-called tokens. Tokens are created and managed by smart contracts.

* Ether value transfers are precise and direct, resembling accounting ledger debits and credits. This is in contrast to the Bitcoin value transfer mechanism, for which it can be difficult to determine the balance of a given wallet address.

* Addresses can be not only wallets that hold balances, but can also contain smart contract bytecode that allows the programmatic creation of agreements and automatic triggering of their execution.  An aggregate of coordinated smart contracts could be used to build a [decentralized autonomous organization][4].

## Content

The Ethereum blockchain data are now available for exploration with BigQuery. All historical data are in the [`ethereum_blockchain dataset`][5], which updates daily.

Our hope is that by making the data on public blockchain systems more readily available it promotes technological innovation and increases societal benefits.

## Querying BigQuery tables

You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at `bigquery-public-data.crypto_ethereum.[TABLENAME]`. **[Fork this kernel to get started][6]**.

## Acknowledgements

[Cover photo by Thought Catalog][7] on Unsplash

## Inspiration

* What are the most popularly exchanged digital tokens, represented by ERC-721 and ERC-20 smart contracts? 
* Compare transaction volume and transaction networks over time
* Compare transaction volume to historical prices by joining with other available data sources like [Bitcoin Historical Data][8]


  [1]: https://ethereum.org/
  [2]: https://en.wikipedia.org/wiki/Online_transaction_processing
  [3]: https://en.wikipedia.org/wiki/Online_analytical_processing
  [4]: https://en.wikipedia.org/wiki/Decentralized_autonomous_organization
  [5]: https://bigquery.cloud.google.com/dataset/bigquery-public-data:ethereum_blockchain
  [6]: https://www.kaggle.com/mrisdal/visualizing-average-ether-costs-over-time
  [7]: https://unsplash.com/photos/bj8U389A9N8
  [8]: https://www.kaggle.com/bigquery/bitcoin-blockchain",2019-03-04T14:57:55.953Z,bigquery/ethereum-blockchain,910127.001043,https://www.kaggle.com/bigquery/ethereum-blockchain,5,0,CC0: Public Domain,20,"Ready version: 4, 2019-03-04T14:57:55.953Z",0.7058824,currencies
17,Bitcoin & Altcoins in 2017,Price transition of bitcoin and altcoins in 2017,"### Context

I made this dataset for Coursera assignment ([Applied Plotting, Charting & Data Representation in Python](https://www.coursera.org/learn/python-plotting)).

### Content

Price transition of crypto-currencies in 2017.
These data were downloaded via Poloniex API.",2017-12-31T15:01:20.877Z,minaba/bitcoin-altcoins-in-2017,0.803789,https://www.kaggle.com/minaba/bitcoin-altcoins-in-2017,2,149,CC BY-SA 4.0,3,"Ready version: 1, 2017-12-31T15:01:20.877Z",0.7058824,currencies
18,Kickstarter videogames released on Steam,A dataset collected from Kickstarter and SteamSpy,"### Context

I have generated this set of auxilary tables to complement the [dataset of Kickstarter projects][1] with the focus on videogames.

### Content

Currently the set contains three tables:

**SteamSpy** table contains aggregate information on released games tracked by SteamSpy

**KSreleased** table links the Steam appid's with Kickstarter project IDs for those KS games, that after a successful campaign were finished and released on Steam 

**Currencies** table shows historical currency exchange rates to USD($) for each week since the earliest campaign deadline among those in KSreleased

### Acknowledgements

SteamSpy table was created using the site's [API][2] and I would like to take this opportunity to praise the site's creator **Sergey Galyonkin**

KSreleased table was generated by crawling [Kickstarter ""Play now"" pages][3]

Currencies table was generated using Fixer.io [API][4]

If you would like to know the details/see the code that I wrote to generate the data, I uploaded it as the ""DEMO: generate data"" kernel. It won't work online (otherwise I wouldn't have the need to create the dataset in the first place), but you can download the notebook and run it locally or just check my poor coding style :)

### Inspiration

I intend to finalize my analysis on KS games that were released on Steam and publish it here, but of course I would like you to find more uses for this data beyond what I would have thought of. And again, I don't think this dataset is useful on its own, so please don't forget to connect to the [KS projects dataset][1] by Kemical


  [1]: https://www.kaggle.com/kemical/kickstarter-projects
  [2]: http://steamspy.com/api.php
  [3]: https://www.kickstarter.com/play
  [4]: http://fixer.io/",2018-01-21T23:54:08.17Z,tonyplaysguitar/steam-spy-data-from-api-request,1.080727,https://www.kaggle.com/tonyplaysguitar/steam-spy-data-from-api-request,2,181,CC0: Public Domain,2,"Ready version: 4, 2018-01-21T23:54:08.17Z",0.875,currencies
19,Predict Future Sales Supplementary,Dataset provides some supplementary data for Predict Future Sales challenge.,"# Kaggle Challenge: Predict Future Sales.
This dataset aims to publish the files that I will use on the Kaggle challenge called [Predict Future Sales](https://www.kaggle.com/c/competitive-data-science-predict-future-sales)

# Data
- I have downloaded **test** and **train** data from the [competition webpage](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data).
- I have downloaded **shop** and **item** information data from the English translations of [@deargle](https://www.kaggle.com/deargle) from [this post](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/54949). Then I have made some changes in the data described in this [R file](https://github.com/kazimanil/predict-future-sales/blob/master/data-manipulation-once-used.R).
- I have collected historical **USD/RUB** rates from [Investing.com](https://www.investing.com/currencies/usd-rub-historical-data). I have used the most recent data for the days which does not include a rate info (i.e. Saturdays and Sundays which markets are closed).
- I have prepared a calendar depicting public holidays and weekends. Public Holiday info for Russia is collected from [this site](https://www.officeholidays.com/countries/russia/).",2018-05-10T13:07:40.25Z,kazimanil/predict-future-sales-supplementary,0.354256,https://www.kaggle.com/kazimanil/predict-future-sales-supplementary,1,243,"Database: Open Database, Contents: Database Contents",4,"Ready version: 3, 2018-05-10T13:07:40.25Z",0.7647059,currencies
20,Demonetization in India Twitter Data,Data extracted from Twitter regarding the recent currency demonetization,"# Context

The **demonetization of â‚¹500 and â‚¹1000** banknotes was a step taken by the **Government of India** on 8 November 2016, ceasing the usage of all â‚¹500 and â‚¹1000 banknotes of the Mahatma Gandhi Series as a form of legal tender in India from 9 November 2016.

The announcement was made by the Prime Minister of India **Narendra Modi** in an unscheduled live televised address to the nation at 20:15 Indian Standard Time (IST) the same day. In the announcement, Modi declared circulation of all â‚¹500 and â‚¹1000 banknotes of the Mahatma Gandhi Series as invalid and announced the issuance of new â‚¹500 and â‚¹2000 banknotes of the Mahatma Gandhi New Series in exchange for the old banknotes.

# Content

The data contains 6000 most recent tweets on #demonetization. There are 6000 rows(one for each tweet) and 14 columns.

## Metadata:

* Text (Tweets)
* favorited
* favoriteCount
* replyToSN
* created
* truncated
* replyToSID
* id
* replyToUID
* statusSource
* screenName
* retweetCount
* isRetweet
* retweeted

# Acknowledgement

The data was collected using the **""twitteR""** package in R using the twitter API.

# Past Research

I have performed my own analysis on the data. I only did a sentiment analysis and formed a word cloud.

[Click here to see the analysis on GitHub](https://github.com/arathee2/demonetization-india/blob/master/demonetization-sentiment-analysis.md)

# Inspiration

* What percentage of tweets are negative, positive or neutral ?
* What are the most famous/re-tweeted tweets ?",2017-04-21T17:35:02.253Z,arathee2/demonetization-in-india-twitter-data,0.990156,https://www.kaggle.com/arathee2/demonetization-in-india-twitter-data,4,4783,Unknown,171,"Ready version: 3, 2017-04-21T17:35:02.253Z",0.7352941,currency
21,EURUSD - 15m - 2010-2016,"FOREX currency rates data for EURUSD, 15 minute candles, BID, years 2010-2016","# Context 

I've always wanted to have a proper sample Forex currency rates dataset for testing purposes, so I've created one.


# Content

The data contains Forex EURUSD currency rates in 15-minute slices (OHLC - Open High Low Close, and Volume). BID price only. Spread is *not provided*, so be careful. 

(Quick reminder: Bid price + Spread = Ask price)

The dates are in the yyyy-mm-dd hh:mm format, GMT. Volume is in Units.

# Acknowledgements

Dukascopy Bank SA
https://www.dukascopy.com/swiss/english/marketwatch/historical/

# Inspiration

Just would like to see if there is still an way to beat the current Forex market conditions, with the prop traders' advanced automatic algorithms running in the wild.",2017-02-22T14:42:13.003Z,meehau/EURUSD,3.494511,https://www.kaggle.com/meehau/EURUSD,2,1460,CC BY-NC-SA 4.0,12,"Ready version: 2, 2017-02-22T14:42:13.003Z",0.8235294,currency
22,Currency Exchange Rates,Daily exchange rates for 51 currencies from 1995 to 2018,"This dataset contains the daily currency exchange rates as reported to the *International Monetary Fund* by the issuing central bank. Included are 51 currencies over the period from 01-01-1995 to 11-04-2018.

The format is known as *currency units per U.S. Dollar*. Explained by example, each rate in the *Euro* column says how much *U.S. Dollar* you had to pay at a certain date to buy 1 Euro. Hence, the rates in the column *U.S. Dollar* are always `1`.",2018-05-02T17:48:28.943Z,thebasss/currency-exchange-rates,0.596854,https://www.kaggle.com/thebasss/currency-exchange-rates,1,521,CC0: Public Domain,0,"Ready version: 2, 2018-05-02T17:48:28.943Z",0.647058845,currency
23,Currency Exchange Rate,Currency Exchange Rate from 1950-2017,"### Context

The data set consist currency exchange rate of different  countries since 1950.

### Content

Exchange rates are defined as the price of one country's' currency in relation to another. Exchange rates may be expressed as the average rate for a period of time or as the rate at the end of the period. Exchange rates are classified by the International Monetary Fund in three broad categories, reflecting the role of the authorities in the determination of the exchange rates and/or the multiplicity of exchange rates in a country: the market rate, in which the rate ""floats"" and is largely set by market forces; the official rate, in which the rate is ""fixed"" by a country's authorities; and arrangements falling between the two, in which the rate holds a stable value against another currency or a composite of currencies. This indicator is measured in terms of national currency per US dollar.

### Acknowledgements
* [ source :](https://data.oecd.org/conversion/exchange-rates.htm)
* [ Purchasing power of currency : wiki](https://en.wikipedia.org/wiki/Exchange_rate)
### Inspiration:
What is exchange rate variation by year?

",2018-02-27T16:33:39.507Z,sudhirnl7/currency-excahnge-rate,0.026714,https://www.kaggle.com/sudhirnl7/currency-excahnge-rate,2,182,CC0: Public Domain,1,"Ready version: 4, 2018-02-27T16:33:39.507Z",0.7352941,currency
24,Binance Crypto Klines,"Minutely crypto currency open/close prices, high/low, trades and others","### Context

Each file contains klines for 1 month period with 1 minute intervals. File name formating looks like mm-yyyy-SMB1SMB2 (e.g. 11-2017-XRPBTC).

This data set contains now only XRP/BTC and ETH/USDT symbol pair now, but it will be expand soon.


### Features
 - Open time -&gt; timestamp (milliseconds)
 - Open price -&gt; float
 - High price -&gt; float
 - Low price -&gt; float
 - Close price -&gt; float
 - Volume -&gt; float
 - Quote asset volume -&gt; float
 - Close time -&gt; timestamp (milliseconds)
 - Number of trades -&gt; int
 - Taker buy base asset volume -&gt; float
 - Taker buy quote asset volume -&gt; float


### Acknowledgements

This dataset was collected from [Binance Exchange | Worlds Largest Crypto Exchange][1]


### Inspiration

This data set could inspire you on most efficient trading algorithms. 


  [1]: https://www.binance.com",2018-04-08T09:58:41.477Z,binance/binance-crypto-klines,1004.510014,https://www.kaggle.com/binance/binance-crypto-klines,5,488,CC0: Public Domain,1,"Ready version: 5, 2018-04-08T09:58:41.477Z",0.75,currency
25,Iraqi Money Ø§Ù„Ø¹Ù…Ù„Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©,Object detection dataset for Iraqi currency,"### Object detection dataset for Iraqi currency

About 4000 images for both sides of each Iraqi currency with object position inside image.


**IMPORTANT**

objects.json contains object frame as (midX, midY, Width, Height) inside each image with file name alphabetic sorting.

This dataset used in training CoreML model for MoneyReader app for iOS:

Download and try here: http://itunes.apple.com/app/id1421092136 


Dataset created by HusamAamer from AppChief.net



### Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡ÙŠ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ù„Ù„Ø¹Ù…Ù„Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø¯Ø§Ø®Ù„ ØµÙˆØ±Ø©

Ø­ÙˆØ§Ù„ÙŠ Ù¤Ù Ù Ù  ØµÙˆØ±Ø© Ù„ÙˆØ¬Ù‡ÙŠ ÙƒÙ„ ÙØ¦Ø© Ù…Ù† ÙØ¦Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ù…Ù‚Ø³Ù…Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©

**Ù‡Ù€Ù€Ø§Ù…**

ÙŠØ­ØªÙˆÙŠ Ù…Ù„Ù Ø§Ù„Ø¬ÙŠØ³ÙˆÙ† Ø¹Ù„Ù‰ Ù…ØµÙÙˆÙØ© ØŒ ÙƒÙ„ Ø¹Ù†ØµØ± ØªØ§Ø¨Ø¹ Ù„ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø© Ø¹Ù†Ø¯ ØªØ±ØªÙŠØ¨ Ø§Ù„ØµÙˆØ± Ø­Ø³Ø¨ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¨Ø¬Ø¯ÙŠØŒ ÙˆÙƒÙ„ Ø¹Ù†ØµØ± ÙŠØ­ØªÙˆÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ù…Ù„Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙˆØ±Ø©  (Ø§Ù„Ù…Ø±ÙƒØ² Ø§ÙƒØ³ ØŒ Ø§Ù„Ù…Ø±ÙƒØ² ÙˆØ§ÙŠ ØŒ Ø§Ù„Ø¹Ø±Ø¶ ØŒ Ø§Ù„Ø¥Ø±ØªÙØ§Ø¹)

ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØºØ±Ø¶ ØªØ¯Ø±ÙŠØ¨ Ù…ÙˆØ¯ÙŠÙ„ CoreML Ù„Ù‚Ø§Ø±Ø¦ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø¹Ù„Ù‰ Ù…ØªØ¬Ø± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª

Ø­Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ¬Ø±Ø¨Ù‡ Ù…Ø¬Ø§Ù†Ø§Ù‹ : http://itunes.apple.com/app/id1421092136",2018-08-23T09:28:29.143Z,husamaamer/iraqi-currency-,1435.021165,https://www.kaggle.com/husamaamer/iraqi-currency-,4,40,Unknown,2,"Ready version: 2, 2018-08-23T09:28:29.143Z",0.6875,currency
26,401 crypto currency pairs at 1-minute resolution,Historical crypto currency data from the Bitfinex exchange including Bitcoin,"## About this dataset

With the rise of crypto currency markets the interest in creating automated trading strategies, or trading bots, has grown. Developing algorithmic trading strategies however requires intensive backtesting to ensure profitable performance. It follows that access to high resolution historical trading data is the foundation of every successful  algorithmic trading strategy. This dataset therefore provides open, high, low, close (OHLC) data at 1 minute resolution of various crypto currency pairs for the development of automated trading systems.

### Content

This dataset contains the historical trading data (OHLC) of 401 trading pairs at 1 minute resolution reaching back until the year 2013. It was collected from the Bitfinex exchange as described in [this article](https://medium.com/coinmonks/how-to-get-historical-crypto-currency-data-954062d40d2d).
The data in the CSV files is the raw output of the Bitfinex API. This means that there are no timestamps for time periods in which the exchange was down. Also if there were time periods without any activity or trades there will be no timestamp as well. 

### Inspiration

This dataset is intended to facilitate the development of automatic trading strategies. Machine learning algorithms, as they are available through various open source libraries these days, typically require large amounts of training data to unveil their full power. Also the process of backtesting new strategies before deploying them rests on high quality data. Most crypto trading datasets that are currently available either have low temporal resolution, are not free of charge or focus only on a limited number of currency pairs. This dataset on the other hand provides high temporal resolution data of almost 400 currency pairs for the development of new trading algorithms.",2019-07-09T21:25:22.227Z,tencars/392-crypto-currency-pairs-at-minute-resolution,393.638972,https://www.kaggle.com/tencars/392-crypto-currency-pairs-at-minute-resolution,3,119,CC BY-SA 4.0,2,"Ready version: 2, 2019-07-09T21:25:22.227Z",1.0,currency
27,Crypto currency data,High resolution data of all BTC based pairs from bittrex. ,"### Context
One week of highly resolved crypto currency data from bittrex.


### Content

The data set comprises four .csv files, containing market price, ask price, bid price and trading volume of all 196 crypto currency coin pairs, listed on bittrex. Bitcoin is the base currency, both for prices and trading volume. 
The temporal resolution is one minute, the time stamp is in UNIX time. 

### Acknowledgements

Thanks to the bittrex API which made the data available!

### Inspiration
Gain more insights on the intra-day correlations among different currency pairs and price development related to trading volume. E.g., predict price pumps based on the changes in trading volume.

",2018-04-15T16:42:18.337Z,mhansinger/bittrex-bitcoin-pairs,44.064008,https://www.kaggle.com/mhansinger/bittrex-bitcoin-pairs,3,126,CC0: Public Domain,0,"Ready version: 2, 2018-04-15T16:42:18.337Z",0.647058845,currency
28,Bitcoin Historical Data,"Bitcoin data at 1-min intervals from select exchanges, Jan 2012 to March 2019","### Context 
Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus ""chained"" together, serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow. Included here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. Happy (data) mining! 

### Content

coinbaseUSD_1-min_data_2014-12-01_to_2019-01-09.csv

bitstampUSD_1-min_data_2012-01-01_to_2019-03-13.csv 

CSV files for select bitcoin exchanges for the time period of Jan 2012 to March 2019, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price.  **Timestamps are in Unix time.  Timestamps without any trades or activity have their data fields filled with NaNs.** If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforseen technical error in data reporting or gathering. All effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk. 


### Acknowledgements and Inspiration

Bitcoin charts for the data. The various exchange APIs, for making it difficult or unintuitive enough to get OHLC and volume data at 1-min intervals that I set out on this data scraping project. Satoshi Nakamoto and the novel core concept of the blockchain, as well as its first execution via the bitcoin protocol. I'd also like to thank viewers like you! Can't wait to see what code or insights you all have to share. 
",2019-03-15T16:22:58.397Z,mczielinski/bitcoin-historical-data,123.326534,https://www.kaggle.com/mczielinski/bitcoin-historical-data,2,43353,CC BY-SA 4.0,128,"Ready version: 16, 2019-03-15T16:22:58.397Z",1.0,currency
29,Zomato Restaurants Data,Analyzing the best restaurants of the major cities,"### Context

I really get fascinated by good quality food being served in the restaurants and would like to help community find the best cuisines around their area

### Content

Zomato API Analysis is one of the most useful analysis for foodies who want to taste the best cuisines of every part of the world which lies in their budget. This analysis is also for those who want to find the value for money restaurants in various parts of the country for the cuisines. Additionally, this analysis caters the needs of people who are striving to get the best cuisine of the country and which locality of that country serves that cuisines with maximum number of restaurants.â™¨ï¸

For more information on Zomato API and Zomato API key
â€¢	Visit : https://developers.zomato.com/api#headline1
â€¢	Data Collection: https://developers.zomato.com/documentation

Data
Fetching the data:
â€¢	Data has been collected from the Zomato API in the form of .json files(raw data) using the url=https://developers.zomato.com/api/v2.1/search?entity_id=1&entity_type=city&start=1&count=20
â€¢	Raw data can be seen here

Data Collection:
Data collected can be seen as a raw .json file here

Data Storage:
The collected data has been stored in the Comma Separated Value file Zomato.csv. Each restaurant in the dataset is uniquely identified by its Restaurant Id. Every Restaurant contains the following variables:

â€¢	Restaurant Id: Unique id of every restaurant across various cities of the world
â€¢	Restaurant Name: Name of the restaurant
â€¢	Country Code: Country in which restaurant is located
â€¢	City: City in which restaurant is located
â€¢	Address: Address of the restaurant
â€¢	Locality: Location in the city
â€¢	Locality Verbose: Detailed description of the locality
â€¢	Longitude: Longitude coordinate of the restaurant's location
â€¢	Latitude: Latitude coordinate of the restaurant's location
â€¢	Cuisines: Cuisines offered by the restaurant
â€¢	Average Cost for two: Cost for two people in different currencies ğŸ‘« 
â€¢	Currency: Currency of the country
â€¢	Has Table booking: yes/no
â€¢	Has Online delivery: yes/ no
â€¢	Is delivering: yes/ no
â€¢	Switch to order menu: yes/no
â€¢	Price range: range of price of food
â€¢	Aggregate Rating: Average rating out of 5
â€¢	Rating color: depending upon the average rating color
â€¢	Rating text: text on the basis of rating of rating
â€¢	Votes: Number of ratings casted by people




### Acknowledgements

I would like to thank Zomato API for helping me collecting data


### Inspiration
Data Processing has been done on the following categories: 
Currency
City
Location
Rating Text",2018-03-13T04:56:25.81Z,shrutimehta/zomato-restaurants-data,5.732263,https://www.kaggle.com/shrutimehta/zomato-restaurants-data,1,19144,CC0: Public Domain,54,"Ready version: 2, 2018-03-13T04:56:25.81Z",0.7941176,currency
30,Daily Crypto Currency and Lunar Geocentric Data ,"Daily crypto markets open, close, low, high data and Lunar Phases (2013-2018)","### Context

This data includes daily open, high, low, close values for all crypto currencies (since April 2013) as well as Daily Lunar Geocentric data (distance, declination, brightness, illumination %, and constellation). Please note that I have consolidated this data from the below two sources (originally submitted by MCrescenzo and jvent) after my mother asked me if there's a correlation between the lunar status and the financial markets.

### Content

This data includes daily open, high, low, close values for all crypto currencies (since April 2013 until January 2018) as well as Daily Lunar Geocentric data (distance, declination, brightness, illumination %, and constellation) for same timeframe.

### Acknowledgements

Lunar Daily Distance and Declination : 1800-2020. [Click for original data submitted by MCrescenzo][1]
Every Cryptocurrency Daily Market Price. [Click for original data submitted by jvent][2]

### Inspiration
 - Is there any correlation between  cryptocurrencies and Lunar Phases?
 - Can we predict cryptocurrency movements by the Lunar Phases?


  [1]: https://www.kaggle.com/crescenzo/lunardistance
  [2]: https://www.kaggle.com/jessevent/all-crypto-currencies",2018-01-25T01:33:42.077Z,rudymizrahi/daily-crypto-currency-and-lunar-geocentric-data,30.710297,https://www.kaggle.com/rudymizrahi/daily-crypto-currency-and-lunar-geocentric-data,5,88,Unknown,0,"Ready version: 1, 2018-01-25T01:33:42.077Z",0.647058845,currency
31,Mobile App Store ( 7200 apps),Analytics for Mobile Apps,"**Mobile App Statistics (Apple iOS app store)**
======================================
The ever-changing mobile landscape is a challenging space to navigate.  . The percentage of mobile over desktop is only increasing.  Android holds about 53.2% of the smartphone market, while iOS is 43%.  To get more people to download your app, you need to make sure they can easily find your app.  Mobile app analytics is a great way to understand the existing strategy to drive growth and retention of future user.

With million of apps around nowadays,  the following data set  has become very key to getting top trending apps in iOS app store.  This data set contains more than 7000 Apple iOS mobile application details. The data was extracted from the [iTunes Search API](http://www.transtats.bhttps://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/iTuneSearchAPI/SearchExamples.html#//apple_ref/doc/uid/TP40017632-CH6-SW1ts.gov/DatabaseInfo.asp?DB_ID=120&Link=0) at the Apple Inc website.  R and  linux web scraping tools were used for this study. 

**Data collection date (from API);**
July 2017

**Dimension of the data set;**
7197 rows and 16 columns

**Content:**
------------

appleStore.csv
--------------

1. ""id"" : App ID

2. ""track_name"": App Name

3. ""size_bytes"": Size (in Bytes)

4. ""currency"": Currency Type

5. ""price"": Price amount

6. ""rating_count_tot"": User Rating counts (for all version)

7. ""rating_count_ver"": User Rating counts (for current version)

8. ""user_rating"" : Average User Rating value (for all version)

9. ""user_rating_ver"": Average User Rating value (for current version)

10. ""ver"" : Latest version code

11. ""cont_rating"": Content Rating

12. ""prime_genre"": Primary Genre 

13. ""sup_devices.num"": Number of supporting devices 

14. ""ipadSc_urls.num"": Number of screenshots showed for display

15. ""lang.num"": Number of supported languages

16. ""vpp_lic"": Vpp Device Based Licensing Enabled

appleStore_description.csv
--------------------------

1.   id : App ID
2.   track_name: Application name
3.  size_bytes: Memory size (in Bytes)
4.   app_desc: Application description

# Acknowledgements
The data was extracted from the [iTunes Search API](http://www.transtats.bhttps://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/iTuneSearchAPI/SearchExamples.html#//apple_ref/doc/uid/TP40017632-CH6-SW1ts.gov/DatabaseInfo.asp?DB_ID=120&Link=0) at the Apple Inc website.  R and  linux web scraping tools were used for this study. 

## Inspiration
1. *How does the App details contribute the user ratings?*
2. *Try to compare app statistics for different groups?*

**Reference: R package**
From github, with 
`devtools::install_github(""ramamet/applestoreR"")`

## Licence
Copyright (c) 2018 Ramanathan Perumal

",2018-06-10T07:04:28.357Z,ramamet4/app-store-apple-data-set-10k-apps,5.904947,https://www.kaggle.com/ramamet4/app-store-apple-data-set-10k-apps,3,24257,GPL 2,73,"Ready version: 7, 2018-06-10T07:04:28.357Z",0.8235294,currency
32,Kickstarter Project Statistics,4000 live projects plus 4000 most backed projects,"Crowdfunding has become one of the main sources of initial capital for small businesses and start-up companies that are looking to launch their first products. Websites like [Kickstarter](https://www.kickstarter.com) and [Indiegogo](https://www.indiegogo.com/) provide a platform for millions of creators to present their innovative ideas to the public. This is a win-win situation where creators could accumulate initial fund while the public get access to cutting-edge prototypical products that are not available in the market yet.

At any given point, Indiegogo has around 10,000 live campaigns while Kickstarter has 6,000. It has become increasingly difficult for projects to stand out of the crowd. Of course, advertisements via various channels are by far the most important factor to a successful campaign. However, for creators with a smaller budget, this leaves them wonder,

**""How do we increase the probability of success of our campaign starting from the very moment we create our project on these websites?""**

# Data Sources
All of my raw data are scraped from [Kickstarter.com](https://www.kickstarter.com).

1. First 4000 **live projects** that are currently campaigning on Kickstarter (live.csv)
    - *Last updated: 2016-10-29 5pm PDT*
    - amt.pledged: amount pledged (float)
    - blurb: project blurb (string)
    - by: project creator (string)
    - country: abbreviated country code (string of length 2)
    - currency: currency type of amt.pledged (string of length 3)
    - end.time: campaign end time (string ""YYYY-MM-DDThh:mm:ss-TZD"")
    - location: mostly city (string)
    - pecentage.funded: unit % (int)
    - state: mostly US states (string of length 2) and others (string)
    - title: project title (string)
    - type: type of location (string: County/Island/LocalAdmin/Suburb/Town/Zip)
    - url: project url after domain (string)

2. Top 4000 **most backed** projects ever on Kickstarter (most_backed.csv)
    - *Last updated: 2016-10-30 10pm PDT*
    - amt.pledged
    - blurb
    - by
    - category: project category (string)
    - currency
    - goal: original pledge goal (float)
    - location
    - num.backers: total number of backers (int)
    - num.backers.tier: number of backers corresponds to the pledge amount in pledge.tier (int[len(pledge.tier)])
    - pledge.tier: pledge tiers in USD (float[])
    - title 
    - url

See more at http://datapolymath.paperplane.io/",2016-11-01T05:37:42.683Z,socathie/kickstarter-project-statistics,1.308381,https://www.kaggle.com/socathie/kickstarter-project-statistics,1,4912,CC BY-NC-SA 4.0,102,"Ready version: 1, 2016-11-01T05:37:42.683Z",0.852941155,currency
33,Russian Financial Indicators,Dataset of Currency Rates ,"### Context

This data was extracted from the open database of quotations of currencies and precious metals located on the site of the Bank of Russia.
The link https://www.cbr.ru/Eng/statistics/?PrtId=finr is available for all internet users, the website is in Russian and in English.

### Content

It consists of 1128 observations of  23 variables.
Variables that indicating exchange rates are measured in rubles, the prices of precious metals are denoted in rubles per gram, foreign exchange.

The special variable `dual currency basket` is calculated according to the formula: 0.55 USD + 0.45 EUR.

The variables k_CNY, k_JPY are coefficients for the currencies values.

Foreign exchange reserves and monetary gold reserves consist of official data points for every month about the state reserves in Russia.

### Acknowledgements

From publicly available data the files in 'xlsx' and 'csv' formats have been generated and downloaded.
They are absolutely free for usage.

### Usage

A set of financial indicators is suitable for training in the field of data visualization and learning simple regression algorithms.",2017-11-06T00:14:12.11Z,olgabelitskaya/russian-financial-indicators,0.278978,https://www.kaggle.com/olgabelitskaya/russian-financial-indicators,0,97,Other (specified in description),1,"Ready version: 1, 2017-11-06T00:14:12.11Z",0.5882353,currency
34,Nepali Currency,,,2018-10-31T18:15:52.017Z,thevirusx3/nepali-currency,1073.942714,https://www.kaggle.com/thevirusx3/nepali-currency,0,12,Unknown,1,"Ready version: 4, 2018-10-31T18:15:52.017Z",0.125,currency
35,Kaggle Machine Learning & Data Science Survey 2017,A big picture view of the state of data science and machine learning.,"### Context

For the first time, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. The survey received over 16,000 responses and we learned a ton about who is working with data, whatâ€™s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field.

To share some of the initial insights from the survey, weâ€™ve worked with the folks from [The Pudding](https://pudding.cool/) to put together [this interactive report](https://kaggle.com/surveys/2017). Theyâ€™ve shared all of the kernels used in the report [here](https://www.kaggle.com/amberthomas/kaggle-2017-survey-results).

### Content

The data includes 5 files: 

  - `schema.csv`: a CSV file with survey schema. This schema includes the questions that correspond to each column name in both the `multipleChoiceResponses.csv` and `freeformResponses.csv`.
  - `multipleChoiceResponses.csv`: Respondents' answers to multiple choice and ranking questions. These are non-randomized and thus a single row does correspond to all of a single user's answers.
  -`freeformResponses.csv`: Respondents' freeform answers to Kaggle's survey questions. These responses are randomized within a column, so that reading across a single row does not give a single user's answers.
  - `conversionRates.csv`: Currency conversion rates (to USD) as accessed from the R package ""quantmod"" on September 14, 2017
  - `RespondentTypeREADME.txt`: This is a schema for decoding the responses in the ""Asked"" column of the `schema.csv` file.

### Kernel Awards in November
In the month of November, weâ€™re awarding $1000 a week for code and analyses shared on this dataset via [Kaggle Kernels](https://www.kaggle.com/kaggle/kaggle-survey-2017/kernels). Read more about this monthâ€™s [Kaggle Kernels Awards](https://www.kaggle.com/about/datasets-awards/kernels) and help us advance the state of machine learning and data science by exploring this one of a kind dataset.

### Methodology
  - This survey received 16,716 usable respondents from 171 countries and territories. If a country or territory received less than 50 respondents, we grouped them into a group named â€œOtherâ€ for anonymity.
  - We excluded respondents who were flagged by our survey system as â€œSpamâ€ or who did not answer the question regarding their employment status (this question was the first required question, so not answering it indicates that the respondent did not proceed past the 5th question in our survey).
  - Most of our respondents were found primarily through Kaggle channels, like our email list, discussion forums and social media channels.
  - The survey was live from August 7th to August 25th. The median response time for those who participated in the survey was 16.4 minutes. We allowed respondents to complete the survey at any time during that window.
  - We received salary data by first asking respondents for their day-to-day currency, and then asking them to write in either their total compensation.
     - Weâ€™ve provided a csv with an exchange rate to USD for you to calculate the salary in US dollars on your own.
     - The question was optional
  - Not every question was shown to every respondent. In an attempt to ask relevant questions to each respondent, we generally asked work related questions to employed data scientists and learning related questions to students. There is a column in the `schema.csv` file called ""Asked"" that describes who saw each question. You can learn more about the different segments we used in the `schema.csv` file and `RespondentTypeREADME.txt` in the data tab.
  - To protect the respondentsâ€™ identity, the answers to multiple choice questions have been separated into a separate data file from the open-ended responses. We do not provide a key to match up the multiple choice and free form responses. Further, the free form responses have been randomized column-wise such that the responses that appear on the same row did not necessarily come from the same survey-taker. 
",2017-10-27T22:03:03.417Z,kaggle/kaggle-survey-2017,3.692041,https://www.kaggle.com/kaggle/kaggle-survey-2017,3,16040,"Database: Open Database, Contents: Â© Original Authors",296,"Ready version: 4, 2017-10-27T22:03:03.417Z",0.8235294,currency
36,IndianNew Currency Notes,,,2018-10-11T14:00:34.55Z,trnpandey/indiannew-currency-notes,11.519162,https://www.kaggle.com/trnpandey/indiannew-currency-notes,0,48,Data files Â© Original Authors,1,"Ready version: 1, 2018-10-11T14:00:34.55Z",0.25,currency
37,Global Food Prices,743k Rows of Monthly Market Food Prices Across Developing Countries,"### Context: 
Global food price fluctuations can cause famine and large population shifts. Price changes are increasingly critical to policymakers as global warming threatens to destabilize the food supply.

### Content: 
Over 740k rows of prices obtained in developing world markets for various goods. Data includes information on country, market, price of good in local currency, quantity of good, and month recorded.

### Acknowledgements: 
Compiled by the [World Food Program](http://www1.wfp.org/) and distributed by [HDX](https://data.humdata.org/dataset/wfp-food-prices).

### Inspiration: 
This data would be particularly interesting to pair with currency fluctuations, weather patterns, and/or refugee movements--do any price changes in certain staples predict population upheaval? Do certain weather conditions influence market prices?

### License:
Released under [CC BY-IGO](https://creativecommons.org/licenses/by/3.0/igo/legalcode).",2017-08-03T20:52:44.033Z,jboysen/global-food-prices,4.62487,https://www.kaggle.com/jboysen/global-food-prices,2,3675,Other (specified in description),6,"Ready version: 1, 2017-08-03T20:52:44.033Z",0.852941155,currency
38,Demonetization in India,Withdrawal of  500 and 1000 bills in India,"Withdrawal of a particular form of currency (such a gold coins, currency notes) from circulation is known as demonetization .
------------------------------------------------------------------------

**Context:** 

On November 8th, Indiaâ€™s Prime Minister  announced that 86% of the countryâ€™s currency would be rendered null and void in 50 days and it will withdraw all 500 and 1,000 rupee notes â€” the countryâ€™s most popular currency denominations from circulation, while a new 2,000 rupee note added in.  It was posited as a move to crackdown on corruption and the countryâ€™s booming under-regulated and virtually untaxed grassroots economy.

**Content:** 

***The field names are following:***

ID	                                 
QUERY	                                  
TWEET_ID	                            
INSERTED DATE	                    
TRUNCATED	                             
LANGUAGE	                                           
possibly_sensitive	coordinates	                              
retweeted_status	                                   
created_at_text	                                                  
created_at	                                          
CONTENT	                                         
from_user_screen_name	                                   
from_user_id	from_user_followers_count	                                  
from_user_friends_count	                            
from_user_listed_count	                                      
from_user_statuses_count	                                 
from_user_description	                                  
from_user_location	                                        
from_user_created_at	                                       
retweet_count	                                         
entities_urls	                                              
entities_urls_counts	                                   
entities_hashtags	                                                        
entities_hashtags_counts	                                       
entities_mentions	                                          
entities_mentions_counts	                                       
in_reply_to_screen_name	                                          
in_reply_to_status_id	                                               
source	                                                   
entities_expanded_urls	                                           
json_output	                                                        
entities_media_count	                                               
media_expanded_url	                                          
media_url	                                          
media_type	                                             
video_link	                                                            
photo_link	                                      
twitpic                                            


**Acknowledgements:** 

Dataset is created by pulling tweets by hashtag from twitter.

**Inspiration:** 

Dataset can be used to understand trending tweets.
Dataset can be used for sentiment analysis and topic mining.
Dataset can be used for time series analysis of tweets.

**What questions would you like answered by the community ?** 

What is the general sentiment of tweets ?                                                                                   
Conclusion regarding tweet sentiments varying over time.

**What feedback would be helpful on the data itself ?**

An in depth analysis of data.",2017-01-17T14:08:53.173Z,shan4224/demonetization-in-india,5.093071,https://www.kaggle.com/shan4224/demonetization-in-india,2,2408,CC0: Public Domain,13,"Ready version: 3, 2017-01-17T14:08:53.173Z",0.8235294,currency
39,EURO-USD History Data (1 Min Interval 2002-2017),History EURO/USD currency data,,2018-05-10T04:01:10.467Z,veidak/eurousd-history-data-1-min-interval-20022017,48.082155,https://www.kaggle.com/veidak/eurousd-history-data-1-min-interval-20022017,2,86,CC0: Public Domain,0,"Ready version: 1, 2018-05-10T04:01:10.467Z",0.5,currency
40,Economic calendar Investing.com Forex (2011-2019),"Archive of important events, economic news, volatility in a convenient format","
Introduction
------------

   Explore the archive of relevant economic information: relevant news on all indicators with explanations, data on past publications on the economy of the United States, Britain, Japan and other developed countries, volatility assessments and much more. For the construction of their forecast models, the use of in-depth training is optimal, with a learning model built on the basis of EU and Forex data.
    The economic calendar is an indispensable assistant for the trader.

Data set
--------

   The data set is created in the form of an CSV, Excel spreadsheet (two files 2011-2013, 2014-2019), which can be found at boot time. You can see the source of the data on the site https://www.investing.com/economic-calendar/

![http://comparic.com/wp-content/uploads/2016/12/Economic_Calendar_-_Investing.com_-_2016-12-19_02.45.10.jpg][1]

1. column - Event date
2. column - Event time (time New York)
3. column - Country of the event
4. column - The degree of volatility (possible fluctuations in currency, indices, etc.) caused by this event
5. column - Description of the event
6. column - Evaluation of the event according to the actual data, which came out better than the forecast, worse or correspond to it
7. column - Data format (%, K x103, M x106, T x109)
8. column - Actual event data
9. column - Event forecast data
10. column - Previous data on this event (with comments if there were any interim changes).


Inspiration
-------------

 1. Use the historical EU in conjunction with the Forex data (exchange rates, indices, metals, oil, stocks) to forecast subsequent Forex data in order to minimize investment risks (combine fundamental market analysis and technical).
 2. Historical events of the EU used as a forecast of the subsequent (for example, the calculation of the probability of an increase in the rate of the Fed).
 3. Investigate the impact of combinations of EC events on the degree of market volatility at different time periods.
 4. To trace the main trends in the economies of the leading countries (for example, a decrease in the demand for unemployment benefits).
 5. Use the EU calendar together with the news background archive for this time interval for a more accurate forecast.
 


  [1]: http://comparic.com/wp-content/uploads/2016/12/Economic_Calendar_-_Investing.com_-_2016-12-19_02.45.10.jpg",2019-07-20T20:17:41.923Z,devorvant/economic-calendar,6.238669,https://www.kaggle.com/devorvant/economic-calendar,5,1490,"Database: Open Database, Contents: Database Contents",7,"Ready version: 1, 2019-07-20T20:17:41.923Z",0.9705882,forex
41,FOREX: EURUSD dataset,"3 years of winning trades in EURUSD 4H, 99 features for operation , make $$$","### Context
Forex is the largest market in the world, predicting the movement of prices is not a simple task, this dataset pretends to be the gateway for people who want to conduct trading using machine learning.


### Content
This dataset contains 4479 simulated winning transactions (real data, fictitious money)  (3 years 201408-201708) with buy transactions (2771 operations 50.7%) and sell (2208 transactions, 49.3%), to generate this data a script of metatrader was used, operations were performed in time frame 4Hour and fixed stop loss and take profits of 50 pips (4 digits) were used to determine if the operation is winning. Each operation contains a set of classic technical indicators like rsi, mom, bb, emas, etc. (last 24 hours)


### Acknowledgements
Thanks to Kaggle for giving me the opportunity to share my passion for machine learning.
My profile: https://www.linkedin.com/in/rsx2010/ 


### Inspiration
The problem of predicting price movement is reduced with this dataset to a classification problem:
<br>
""use the variables rsi1 to dayOfWeek to predict the type of correct operation to be performed (field=tipo)""
<br>
tipo = 0 ==> Operation buy
<br>
tipo= 1 ==> Operation = sell:

Good luck<br>
Rodrigo Salas Vallet-Cendre.<br>
rasvc@hotmail.com",2017-09-05T02:05:55.703Z,rsalaschile/forex-eurusd-dataset,0.400495,https://www.kaggle.com/rsalaschile/forex-eurusd-dataset,0,895,CC0: Public Domain,1,"Ready version: 1, 2017-09-05T02:05:55.703Z",0.7058824,forex
42,EURUSD - 15m - 2010-2016,"FOREX currency rates data for EURUSD, 15 minute candles, BID, years 2010-2016","# Context 

I've always wanted to have a proper sample Forex currency rates dataset for testing purposes, so I've created one.


# Content

The data contains Forex EURUSD currency rates in 15-minute slices (OHLC - Open High Low Close, and Volume). BID price only. Spread is *not provided*, so be careful. 

(Quick reminder: Bid price + Spread = Ask price)

The dates are in the yyyy-mm-dd hh:mm format, GMT. Volume is in Units.

# Acknowledgements

Dukascopy Bank SA
https://www.dukascopy.com/swiss/english/marketwatch/historical/

# Inspiration

Just would like to see if there is still an way to beat the current Forex market conditions, with the prop traders' advanced automatic algorithms running in the wild.",2017-02-22T14:42:13.003Z,meehau/EURUSD,3.494511,https://www.kaggle.com/meehau/EURUSD,2,1460,CC BY-NC-SA 4.0,12,"Ready version: 2, 2017-02-22T14:42:13.003Z",0.8235294,forex
43,EUR USD Forex Pair Historical Data (2002 - 2019),Historical Data from Oanda,"### Content

This dataset contains historical data saved from Oanda Brokerage. The columns represent the Bid and Ask price for every minute / hour.  There are also news downloaded form Investing.com. These can be used to forecast the trend of the Forex market with machine learning techniques.


### Acknowledgements

The data is downloaded from Quantconnect.com. If they have any concerns, I will remove it instantly.

### Inspiration

The reason I am sharing this dataset is because I struggled so much to find good quality data which is large enough to train trading algorithms. So if you want to try LSTMs, stochastics methods or anything else you are free to use this dataset.",2019-03-02T11:17:43.19Z,imetomi/eur-usd-forex-pair-historical-data-2002-2019,107.197636,https://www.kaggle.com/imetomi/eur-usd-forex-pair-historical-data-2002-2019,5,302,GNU Affero General Public License 3.0,3,"Ready version: 1, 2019-03-02T11:17:43.19Z",0.8235294,forex
44,EURUSD jan/2014 - oct/2018,"Forex with a ton of indicators, MQL5 retrieved from XM.COM","Forex with a ton of indicators, MQL5 retrieved from XM.COM


All info was retrieved using a Robot called XAPTUR
https://bitbucket.org/yurisa2/robos-da-mamae/src/master/Experts/Xaptur/Xaptur.mq5

Developed by me.... Citations welcome.",2018-10-04T01:37:53Z,yurisa2/eurusd-2014-2018,1017.43878,https://www.kaggle.com/yurisa2/eurusd-2014-2018,4,74,CC0: Public Domain,2,"Ready version: 3, 2018-10-04T01:37:53Z",0.647058845,forex
45,Forex Data Source,,,2018-04-29T15:42:10.233Z,mannir/forex-data-source,3.29447,https://www.kaggle.com/mannir/forex-data-source,0,67,GPL 2,0,"Ready version: 1, 2018-04-29T15:42:10.233Z",0.235294119,forex
46,Deep Learning A-Z - ANN dataset,"Kirill Eremenko ""Deep Learning A-Zâ„¢: Hands-On Artificial Neural Networks"" course","# Context 

This is the dataset used in the section ""ANN (Artificial Neural Networks)"" of the Udemy course from Kirill Eremenko (Data Scientist & Forex Systems Expert) and Hadelin de Ponteves (Data Scientist), called **Deep Learning A-Zâ„¢: Hands-On Artificial Neural Networks**. The dataset is **very useful for beginners** of Machine Learning, and a simple playground where to compare several techniques/skills.

It can be freely downloaded here: https://www.superdatascience.com/deep-learning/


----------


The story:
A bank is investigating a very high rate of customer leaving the bank. Here is a 10.000 records dataset to investigate and predict which of the customers are more likely to leave the bank soon.

The story of the story:
I'd like to compare several techniques (better if not alone, and with the experience of several Kaggle users) to improve my basic knowledge on Machine Learning.


# Content

I will write more later, but the columns names are very self-explaining.


# Acknowledgements

Udemy instructors Kirill Eremenko (Data Scientist & Forex Systems Expert) and Hadelin de Ponteves (Data Scientist), and their efforts to provide this dataset to their students.


# Inspiration

Which methods score best with this dataset? Which are fastest (or, executable in a decent time)? Which are the basic steps with such a simple dataset, very useful to beginners?",2017-05-16T12:20:30.84Z,filippoo/deep-learning-az-ann,0.2728,https://www.kaggle.com/filippoo/deep-learning-az-ann,1,1416,Unknown,28,"Ready version: 1, 2017-05-16T12:20:30.84Z",0.7058824,forex
47,Daily USDJPY (2000-2019) with Technical Indicators,For Time Series Prediction of Forex,"### Context

Possible time series prediction:

 - Colour of the next day candlestick 
 - Next day Close or Open price

### Content

Refer to attached Column Description file for details.


### Acknowledgements

Thanks to all who have helped in making a contribution to this dataset.

",2019-04-19T14:02:19.243Z,cfchan/daily-usdjpy-20002019-with-technical-indicators,0.594757,https://www.kaggle.com/cfchan/daily-usdjpy-20002019-with-technical-indicators,4,32,Unknown,1,"Ready version: 1, 2019-04-19T14:02:19.243Z",0.470588237,forex
48,Forex RSI and BBPP multiperiod (m1-h4) ,,,2018-11-11T11:52:30.603Z,yurisa2/forex-rsi-and-bbpp-multiperiod-m1h4,5565.310574,https://www.kaggle.com/yurisa2/forex-rsi-and-bbpp-multiperiod-m1h4,0,96,Unknown,2,"Ready version: 2, 2018-11-11T11:52:30.603Z",0.1764706,forex
49,forex_strategy_results_next,,,2019-05-11T19:14:43.487Z,sinusgamma/forex-strategy-results-next,0.125853,https://www.kaggle.com/sinusgamma/forex-strategy-results-next,0,3,CC0: Public Domain,2,"Ready version: 1, 2019-05-11T19:14:43.487Z",0.294117659,forex
50,forex_strategy_results_first,,,2019-05-07T08:28:25.523Z,sinusgamma/forex-strategy-results-first,0.07624,https://www.kaggle.com/sinusgamma/forex-strategy-results-first,0,1,CC0: Public Domain,2,"Ready version: 1, 2019-05-07T08:28:25.523Z",0.294117659,forex
51,Foreign Exchange (FX) Prediction - USD/JPY,Jan 2017 Martket Data(Lightweight CSV),"Context

Coming Soon

Content

Coming Soon

Acknowledgements

Special Thanks to http://www.histdata.com/download-free-forex-data/

Inspiration

å®Ÿéš›ã®å–å¼•ã«ã“ã®æƒ…å ±ã‚’ä½¿ã†ã¨ãã¯ååˆ†ã”æ³¨æ„ãã ã•ã„ã€‚å¼Šç¤¾ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ¡ãƒ³ãƒãƒ¼ã¯æå¤±ã®è²¬ä»»ã‚’å–ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚",2017-08-13T06:26:53.733Z,team-ai/foreign-exchange-fx-prediction-usdjpy,0.2926,https://www.kaggle.com/team-ai/foreign-exchange-fx-prediction-usdjpy,2,342,CC0: Public Domain,4,"Ready version: 1, 2017-08-13T06:26:53.733Z",0.8235294,forex
52,Forex & Crypto & Bonds & Indices & Commodities,,,2019-04-06T10:18:49.68Z,crtatu/forex-crypto-bonds-indices-commodities,391.51808,https://www.kaggle.com/crtatu/forex-crypto-bonds-indices-commodities,0,24,Unknown,1,"Ready version: 1, 2019-04-06T10:18:49.68Z",0.117647059,forex
53,Forex Oracle Offers,Customers Information,,2018-11-22T00:58:54.9Z,forexoracle/forex-oracle-offers,0.368301,https://www.kaggle.com/forexoracle/forex-oracle-offers,0,12,Other (specified in description),1,"Ready version: 1, 2018-11-22T00:58:54.9Z",0.1875,forex
54,eur_doll_forex,,,2018-12-14T12:11:00.983Z,mycorino/eur-doll-forex,47.059451,https://www.kaggle.com/mycorino/eur-doll-forex,0,36,Unknown,3,"Ready version: 1, 2018-12-14T12:11:00.983Z",0.1875,forex
55,Hourly GBPUSD w Technical Indicators (2000-2019),Time Series Forecasting for Forex,"### Context

Possible prediction of the next opening or closing price


### Content

See Column_Description_GBPUSD.csv


### Acknowledgements

Thanks to all who have made a contribution to this dataset
",2019-04-23T02:16:18.47Z,cfchan/hourly-gbpusd-w-technical-indicators-20002019,17.862885,https://www.kaggle.com/cfchan/hourly-gbpusd-w-technical-indicators-20002019,5,32,Unknown,1,"Ready version: 1, 2019-04-23T02:16:18.47Z",0.470588237,forex
56,FX USD/JPY Prediction,Jan 2017 Martket Data(Lightweight CSV),"### Context

Coming Soon

### Content

Coming Soon

### Acknowledgements

Special Thanks to http://www.histdata.com/download-free-forex-data/


### Inspiration
å®Ÿéš›ã®å–å¼•ã«ã“ã®æƒ…å ±ã‚’ä½¿ã†ã¨ãã¯ååˆ†ã”æ³¨æ„ãã ã•ã„ã€‚å¼Šç¤¾ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ¡ãƒ³ãƒãƒ¼ã¯æå¤±ã®è²¬ä»»ã‚’å–ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚",2017-08-09T10:59:10.39Z,daiearth22/fx-usdjpy-prediction,0.304973,https://www.kaggle.com/daiearth22/fx-usdjpy-prediction,0,67,CC0: Public Domain,0,"Ready version: 1, 2017-08-09T10:59:10.39Z",0.647058845,forex
57,GBP/USD Historical data (month),data to test accuracy and prediction,"### Context

data set for most dominant forex pair GB/USD


### Content
the first column containing date and second containing data
### Acknowledgements

We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.


### Inspiration

Your data will be in front of the world's largest data science community. What questions do you want to see answered?",2019-03-23T07:13:17.623Z,mohsinsajjad/dataset,0.001155,https://www.kaggle.com/mohsinsajjad/dataset,3,5,Unknown,1,"Ready version: 1, 2019-03-23T07:13:17.623Z",0.7058824,forex
58,eur/usd,03 08 2017 by minute,"# Context 

A day in the life or a forex trader.

# Content

One day of minute by minute statistics from my trading log.

# Acknowledgements
The Python community and stack overflow.


# Inspiration
Jessy Livermore",2017-03-11T10:46:02.16Z,ugnix911aalc/eurusd,0.01691,https://www.kaggle.com/ugnix911aalc/eurusd,0,76,CC0: Public Domain,1,"Ready version: 1, 2017-03-11T10:46:02.16Z",0.7058824,forex
59,binance criptos price from jun 2017 to may 2018,Binance candlestick  from jun 2017 to may 2018,"### Context
This dataset content is related to criptocurrency price in 11 months period for use in data analysis, finance and backtests. 

### Content

We have a complete candlestick with close, open prices, 24 volume... of some coins in binance exchange.
A note here, the close adj is equal close column because binance haven't a adjustment in close price as type forex.   
",2018-06-06T19:55:02.727Z,gabsgear/binance-criptos-price-from-jun-2017-to-may-2018,0.187679,https://www.kaggle.com/gabsgear/binance-criptos-price-from-jun-2017-to-may-2018,2,76,CC0: Public Domain,1,"Ready version: 1, 2018-06-06T19:55:02.727Z",0.647058845,forex
60,US Consumer Finance Complaints,US consumer complaints on financial products and company responses,"Each week [the CFPB](http://www.consumerfinance.gov/data-research/consumer-complaints/) sends thousands of consumersâ€™ complaints about financial products and services to companies for response. Those complaints are published here after the company responds or after 15 days, whichever comes first. By adding their voice, consumers help improve the financial marketplace.",2016-04-26T22:33:46.69Z,cfpb/us-consumer-finance-complaints,94.858347,https://www.kaggle.com/cfpb/us-consumer-finance-complaints,1,7057,Unknown,83,"Ready version: 1, 2016-04-26T22:33:46.69Z",0.7058824,finance
61,Campaign Finance versus Election Results,Can an election be predicted from the preceding campaign finance reports?,"# Context 

This dataset was assembled to investigate the possibility of predicting congressional election results by campaign finance reports from the period leading up to the election.


# Content

Each row represents a candidate, with information on their campaign including the state, district, office, total contributions, total expenditures, etc.  The content is specific to the year leading up to the 2016 election: (1/1/2015 through 10/19/2016).


# Acknowledgements

Campaign finance information came directly from FEC.gov.
Election results and vote totals for house races were taken from CNN's election results page.


# Inspiration

How much of an impact does campaign spending and fundraising have on an election?  Is the impact greater in certain areas?  Given this dataset, to what degree of accuracy could we have predicted the election results?",2016-12-07T21:14:32.993Z,danerbland/electionfinance,0.217837,https://www.kaggle.com/danerbland/electionfinance,2,944,CC0: Public Domain,7,"Ready version: 1, 2016-12-07T21:14:32.993Z",0.8235294,finance
62,2016 Presidential Campaign Finance,How did presidential candidates spend their campaign funds?,"# Context 

The Federal Election Commission (FEC) is an independent regulatory agency established in 1975 to administer and enforce the Federal Election Campaign Act (FECA), which requires public disclosure of campaign finance information. The FEC publishes campaign finance reports for presidential and legislative election campaign candidates on the [Campaign Finance Disclosure Portal][1].

# Content

The finance summary report contains one record for each financial report (Form 3P) filed by the presidential campaign committees during the 2016 primary and general election campaigns. Presidential committees file quarterly prior to the election year and monthly during the election year. The campaign expenditures file contains individual operating expenditures made by the campaign committee and reported on Form 3P Line 23 during the same period. Operating expenditures consist of the routine costs of campaigning for president, which include staffing, travel, advertising, voter outreach, and other activities.


[1]: http://www.fec.gov/disclosurep/pnational.do",2017-01-17T19:48:13.063Z,fec/presidential-campaign-finance,1.120759,https://www.kaggle.com/fec/presidential-campaign-finance,2,774,CC0: Public Domain,1,"Ready version: 1, 2017-01-17T19:48:13.063Z",0.8235294,finance
63,Finance â‚¹ - India,Statewise India's finance detail from 1980 to 2015.,"**Connect/Follow me on [LinkedIn](http://link.rajanand.org/linkedin) for more updates on interesting dataset like this. Thanks.**

### Content

This dataset contains the various finance detail of India.

1. Aggregate expenditure.
2. Capital expenditure.
3. Social sector expenditure.
3. Revenue expenditure.
5. Revenue deficit. 
6. Gross fiscal deficit.
7. Own tax revenues.
8. Nominal GSDP series. 

Granularity: Annual
Time period: 1980-81 to 2015-16.
Amount: in crore rupees (i.e, 1 crore = 10 million)

### Acknowledgements

[National Institution for Transforming India (NITI Aayog)](http://niti.gov.in/)/Planning commission, Govt of India has published this data on their [website](http://niti.gov.in/state-statistics).",2017-08-27T12:17:02.98Z,rajanand/finance-india,0.025746,https://www.kaggle.com/rajanand/finance-india,2,985,CC BY-SA 4.0,0,"Ready version: 1, 2017-08-27T12:17:02.98Z",0.7647059,finance
64,Mutual Funds and ETFs,25k+ Mutual Funds and 2k+ ETFs scraped from Yahoo Finance,"### Context

ETFs represent a cheap alternative to Mutual Funds and they are growing in fast in the last decade.
Is the 2017 hype around ETFs confirmed by good returns in 2018?


### Content

The file contains 25,265 Mutual Funds and 2,353 ETFs with general aspects (as Total Net Assets, management company and size), portfolio indicators (as cash, stocks, bonds, and sectors), returns (as year_to_date, 2018-10) and financial ratios (as price/earning, Treynor and Sharpe ratios, alpha, and beta).  


### Acknowledgements

Data has been scraped from the publicly available website https://finance.yahoo.com.


### Inspiration

Datasets allow for multiple comparisons regarding portfolio decisions from investment managers in Mutual Funds and portfolio restrictions to the indexes in ETFs.
The inspiration comes from the 2017 hype regarding ETFs, that convinced many investors to buy shares of Exchange Traded Funds rather than Mutual Funds.
Datasets will be updated every one or two semesters, hopefully with additional information scraped from Morningstar.com.",2019-05-04T02:00:37.827Z,stefanoleone992/mutual-funds-and-etfs,4.5474,https://www.kaggle.com/stefanoleone992/mutual-funds-and-etfs,5,598,CC0: Public Domain,2,"Ready version: 3, 2019-05-04T02:00:37.827Z",1.0,finance
65,GAFA stock prices,"GAFA (Google, Apple, Facebook, Amazon) stock prices from Yahoo Finance","GAFA (Google, Apple, Facebook, Amazon) stock prices until 20 April 2018.

Source: Yahoo Finance",2018-04-22T21:07:52.127Z,stexo92/gafa-stock-prices,0.407127,https://www.kaggle.com/stexo92/gafa-stock-prices,5,741,CC0: Public Domain,3,"Ready version: 1, 2018-04-22T21:07:52.127Z",0.5294118,finance
66,SF Campaign Finance Data,From San Francisco Open Data,"### Content  

More details about each file are in the individual file descriptions.  

### Context  

This is a dataset hosted by the city of San Francisco. The organization has an open data platform found [here](https://data.sfgov.org) and they update their information according the amount of data that is brought in. Explore San Francisco's Data using Kaggle and all of the data sources available through the San Francisco [organization page](https://www.kaggle.com/san-francisco)!  

* Update Frequency: This dataset is updated quarterly.

### Acknowledgements

This dataset is maintained using Socrata's API and Kaggle's API. [Socrata](https://socrata.com/) has assisted countless organizations with hosting their open data and has been an integral part of the process of bringing more data to the public.  

This dataset is distributed under the following licenses: Open Data Commons Public Domain Dedication and License",2019-01-02T22:32:32.257Z,san-francisco/sf-campaign-finance-data,56.501468,https://www.kaggle.com/san-francisco/sf-campaign-finance-data,3,155,Other (specified in description),1,"Ready version: 67, 2019-01-02T22:32:32.257Z",0.7941176,finance
67,finance study,,,2017-11-04T20:05:21.893Z,tusharpatil15/finance-study,3.118831,https://www.kaggle.com/tusharpatil15/finance-study,0,374,CC BY-NC-SA 4.0,0,"Ready version: 1, 2017-11-04T20:05:21.893Z",0.25,finance
68,Lending Club Loan Data,Analyze Lending Club's issued loans,"These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the ""present"" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file. k",2019-03-18T18:43:12.857Z,wendykan/lending-club-loan-data,736.483,https://www.kaggle.com/wendykan/lending-club-loan-data,1,53425,Unknown,582,"Ready version: 1, 2019-03-18T18:43:12.857Z",0.7352941,finance
69,S&P index historical Data,S&P Index Historical Data from Yahoo finance ,,2017-12-06T22:08:32.46Z,adityarajuladevi/sp-index-historical-data,0.09282,https://www.kaggle.com/adityarajuladevi/sp-index-historical-data,1,147,CC0: Public Domain,3,"Ready version: 1, 2017-12-06T22:08:32.46Z",0.5882353,finance
70,"ticks: bitcoin, ethereum,litecoin, ripple","Finance, Trading, Cryptocurrency",,2018-09-24T06:28:23.997Z,albala/ticks-bitcoin-ethereumlitecoin-ripple,37.397747,https://www.kaggle.com/albala/ticks-bitcoin-ethereumlitecoin-ripple,0,109,"Database: Open Database, Contents: Â© Original Authors",1,"Ready version: 1, 2018-09-24T06:28:23.997Z",0.3529412,finance
71,NYC Property Sales,A year's worth of properties sold on the NYC real estate market,"### Context

This dataset is a record of every building or building unit (apartment, etc.) sold in the New York City property market over a 12-month period.

### Content

This dataset contains the location, address, type, sale price, and sale date of building units sold. A reference on the trickier fields:

* `BOROUGH`: A digit code for the borough the property is located in; in order these are Manhattan (1), Bronx (2), Brooklyn (3), Queens (4), and Staten Island (5).
* `BLOCK`; `LOT`: The combination of borough, block, and lot forms a unique key for property in New York City. Commonly called a `BBL`.
* `BUILDING CLASS AT PRESENT` and `BUILDING CLASS AT TIME OF SALE`: The type of building at various points in time. See the glossary linked to below.

For further reference on individual fields see the [Glossary of Terms](http://www1.nyc.gov/assets/finance/downloads/pdf/07pdf/glossary_rsf071607.pdf). For the building classification codes see the [Building Classifications Glossary](http://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html).

Note that because this is a financial transaction dataset, there are some points that need to be kept in mind:

* Many sales occur with a nonsensically small dollar amount: $0 most commonly. These sales are actually transfers of deeds between parties: for example, parents transferring ownership to their home to a child after moving out for retirement.
* This dataset uses the financial definition of a building/building unit, for tax purposes. In case a single entity owns the building in question, a sale covers the value of the entire building. In case a building is owned piecemeal by its residents (a condominium), a sale refers to a single apartment (or group of apartments) owned by some individual.

### Acknowledgements

This dataset is a concatenated and slightly cleaned-up version of the New York City Department of Finance's [Rolling Sales dataset](http://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page).

### Inspiration

What can you discover about New York City real estate by looking at a year's worth of raw transaction records? Can you spot trends in the market, or build a model that predicts sale value in the future?",2017-09-22T19:43:30.99Z,new-york-city/nyc-property-sales,1.987717,https://www.kaggle.com/new-york-city/nyc-property-sales,2,5288,CC0: Public Domain,18,"Ready version: 1, 2017-09-22T19:43:30.99Z",0.8235294,finance
72,finance,,,2019-06-16T06:03:58.25Z,huskylovers/finance,733.757334,https://www.kaggle.com/huskylovers/finance,0,11,Unknown,5,"Ready version: 1, 2019-06-16T06:03:58.25Z",0.1875,finance
73,New York Stock Exchange,S&P 500 companies historical prices with fundamental data,"# Context 

This dataset is a playground for fundamental and  technical analysis. It is said that 30% of traffic on stocks is already generated by machines, can trading be fully automated? If not, there is still a lot to learn from historical data.    

# Content

Dataset consists of following files:

 - **prices.csv**: raw, as-is daily prices. Most of data spans from 2010 to the end 2016, for companies new on stock market date range is shorter. There have been approx. 140 stock splits in that time, this set doesn't account for that.
 - **prices-split-adjusted.csv**: same as prices, but there have been added adjustments for splits.
 - **securities.csv**: general description of each company with division on sectors
 - **fundamentals.csv**: metrics extracted from annual SEC 10K fillings (2012-2016), should be enough to derive most of popular fundamental indicators.

# Acknowledgements

Prices were fetched from Yahoo Finance, fundamentals are from Nasdaq Financials, extended by some fields from EDGAR SEC databases.

# Inspiration

Here is couple of things one could try out with this data:

 - One day ahead prediction: Rolling Linear Regression, ARIMA, Neural Networks, LSTM
 - Momentum/Mean-Reversion Strategies
 - Security clustering, portfolio construction/hedging

Which company has biggest chance of being bankrupt? Which one is undervalued (how prices behaved afterwards), what is Return on Investment?


",2017-02-22T10:18:25.517Z,dgawlik/nyse,34.402357,https://www.kaggle.com/dgawlik/nyse,1,29493,CC0: Public Domain,271,"Ready version: 3, 2017-02-22T10:18:25.517Z",0.852941155,finance
74,Consumer Complaint Database,Consumer Finance Complaints (Bureau of Consumer Financial Protection),"### Context

These are real world complaints received about financial products and services. Each complaint has been labeled with a specific product; therefore, this is a supervised text classification problem. With the aim to classify future complaints based on its content, we used different machine learning algorithms can make more accurate predictions (i.e., classify the complaint in one of the product categories)

### Content

The dataset contains different information of complaints that customers have made about a multiple products and services in the financial sector, such us Credit Reports, Student Loans, Money Transfer, etc.
The date of each complaint ranges from November 2011 to May 2019.


### Acknowledgements

This work is considered a U.S. Government Work. The dataset is public dataset and it was downloaded from 
https://catalog.data.gov/dataset/consumer-complaint-database
on 2019, May 13.


### Inspiration

This is a sort of tutorial for beginner ",2019-05-13T16:17:54.08Z,selener/consumer-complaint-database,179.716209,https://www.kaggle.com/selener/consumer-complaint-database,3,134,U.S. Government Works,2,"Ready version: 1, 2019-05-13T16:17:54.08Z",0.8235294,finance
75,Bitcoin Historical Data,"Bitcoin data at 1-min intervals from select exchanges, Jan 2012 to March 2019","### Context 
Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus ""chained"" together, serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow. Included here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. Happy (data) mining! 

### Content

coinbaseUSD_1-min_data_2014-12-01_to_2019-01-09.csv

bitstampUSD_1-min_data_2012-01-01_to_2019-03-13.csv 

CSV files for select bitcoin exchanges for the time period of Jan 2012 to March 2019, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price.  **Timestamps are in Unix time.  Timestamps without any trades or activity have their data fields filled with NaNs.** If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforseen technical error in data reporting or gathering. All effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk. 


### Acknowledgements and Inspiration

Bitcoin charts for the data. The various exchange APIs, for making it difficult or unintuitive enough to get OHLC and volume data at 1-min intervals that I set out on this data scraping project. Satoshi Nakamoto and the novel core concept of the blockchain, as well as its first execution via the bitcoin protocol. I'd also like to thank viewers like you! Can't wait to see what code or insights you all have to share. 
",2019-03-15T16:22:58.397Z,mczielinski/bitcoin-historical-data,123.326534,https://www.kaggle.com/mczielinski/bitcoin-historical-data,2,43353,CC BY-SA 4.0,128,"Ready version: 16, 2019-03-15T16:22:58.397Z",1.0,finance
76,NYC Parking Tickets,"42.3M Rows of Parking Ticket Data, Aug 2013-June 2017","### Context

The NYC Department of Finance collects data on every parking ticket issued in NYC (~10M per year!). This data is made publicly available to aid in ticket resolution and to guide policymakers.


### Content

There are four files, covering Aug 2013-June 2017. The files are roughly organized by fiscal year (July 1 - June 30) with the exception of the initial dataset. The initial dataset also lacks 8 columns that are included in the other three datasets (although be warned that these additional data columns are used sparingly). See the dataset descriptions for exact details. Columns include information about the vehicle ticketed, the ticket issued, location, and time.


### Acknowledgements

Data was produced by NYC Department of Finance. FY2018 data is found [here](https://data.cityofnewyork.us/City-Government/Parking-Violations-Issued-Fiscal-Year-2018/pvqr-7yc4) with updates every third week of the month.


### Inspiration

* When are tickets most likely to be issued? Any seasonality?
* Where are tickets most commonly issued?
* What are the most common years and types of cars to be ticketed?",2017-10-26T18:47:45.14Z,new-york-city/nyc-parking-tickets,2171.622562,https://www.kaggle.com/new-york-city/nyc-parking-tickets,4,8080,CC0: Public Domain,6,"Ready version: 2, 2017-10-26T18:47:45.14Z",0.8235294,finance
77,Credit Card Fraud Detection,Anonymized credit card transactions labeled as fraudulent or genuine,"Context
---------

It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.

Content
---------

The datasets contains transactions made by credit cards in September 2013 by european cardholders. 
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.

It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. 

Inspiration
---------

Identify fraudulent credit card transactions.

Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.

Acknowledgements
---------

The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (UniversitÃ© Libre de Bruxelles) on big data mining and fraud detection.
More details on current and past projects on related topics are available on [https://www.researchgate.net/project/Fraud-detection-5][1] and the page of the [DefeatFraud][2] project

Please cite the following works: 

Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca  Bontempi. [Calibrating Probability with Undersampling for Unbalanced  Classification.][3] In Symposium on Computational Intelligence and Data  Mining (CIDM), IEEE, 2015

Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca.  [Learned lessons in credit  card fraud detection from a practitioner perspective][4], Expert systems with applications,41,10,4915-4928,2014, Pergamon

Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi,  Cesare; Bontempi, Gianluca. [Credit card fraud detection: a realistic modeling and a novel learning strategy,][5]  IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE

Dal Pozzolo, Andrea [Adaptive Machine learning for credit card fraud detection][6] ULB MLG PhD thesis (supervised by G. Bontempi)

Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-AÃ«l; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. [Scarff: a scalable  framework for streaming credit card fraud detection with Spark][7], Information fusion,41, 182-194,2018,Elsevier

Carcillo, Fabrizio; Le Borgne, Yann-AÃ«l; Caelen, Olivier; Bontempi, Gianluca. [Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization,][8] International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing

Bertrand Lebichot, Yann-AÃ«l Le Borgne, Liyun He, Frederic OblÃ©, Gianluca Bontempi [Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection](https://www.researchgate.net/publication/332180999_Deep-Learning_Domain_Adaptation_Techniques_for_Credit_Cards_Fraud_Detection),  INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019

Fabrizio Carcillo, Yann-AÃ«l Le Borgne, Olivier Caelen, Frederic OblÃ©, Gianluca Bontempi [Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection ](https://www.researchgate.net/publication/333143698_Combining_Unsupervised_and_Supervised_Learning_in_Credit_Card_Fraud_Detection) Information Sciences, 2019



  [1]: https://www.researchgate.net/project/Fraud-detection-5
  [2]: https://mlg.ulb.ac.be/wordpress/portfolio_page/defeatfraud-assessment-and-validation-of-deep-feature-engineering-and-learning-solutions-for-fraud-detection/
  [3]: https://www.researchgate.net/publication/283349138_Calibrating_Probability_with_Undersampling_for_Unbalanced_Classification
  [4]: https://www.researchgate.net/publication/260837261_Learned_lessons_in_credit_card_fraud_detection_from_a_practitioner_perspective
  [5]: https://www.researchgate.net/publication/319867396_Credit_Card_Fraud_Detection_A_Realistic_Modeling_and_a_Novel_Learning_Strategy
  [6]: http://di.ulb.ac.be/map/adalpozz/pdf/Dalpozzolo2015PhD.pdf
  [7]: https://www.researchgate.net/publication/319616537_SCARFF_a_Scalable_Framework_for_Streaming_Credit_Card_Fraud_Detection_with_Spark
  
[8]: https://www.researchgate.net/publication/332180999_Deep-Learning_Domain_Adaptation_Techniques_for_Credit_Cards_Fraud_Detection",2018-03-23T01:17:27.913Z,mlg-ulb/creditcardfraud,69.155632,https://www.kaggle.com/mlg-ulb/creditcardfraud,3,136568,"Database: Open Database, Contents: Database Contents",2135,"Ready version: 3, 2018-03-23T01:17:27.913Z",0.852941155,finance
78,Daily News for Stock Market Prediction,Using 8 years daily news headlines to predict stock market movement,"Actually, I prepare this dataset for students on my Deep Learning and NLP course. 

But I am also very happy to see kagglers play around with it.

Have fun!

**Description:**

There are two channels of data provided in this dataset:

1. News data: I crawled historical news headlines from [Reddit WorldNews Channel][1] (/r/worldnews). They are ranked by reddit users' votes, and only the top 25 headlines are considered for a single date.
(Range: 2008-06-08 to 2016-07-01)

2. Stock data: Dow Jones Industrial Average (DJIA) is used to ""prove the concept"".
(Range: 2008-08-08 to 2016-07-01)

I provided three data files in *.csv* format:

1. **RedditNews.csv**: two columns
The first column is the ""date"", and second column is the ""news headlines"".
All news are ranked from top to bottom based on how *hot* they are.
Hence, there are 25 lines for each date.

2. **DJIA_table.csv**: 
Downloaded directly from [Yahoo Finance][2]: check out the web page for more info.

3. **Combined_News_DJIA.csv**:
To make things easier for my students, I provide this combined dataset with 27 columns.
The first column is ""Date"", the second is ""Label"", and the following ones are news headlines ranging from ""Top1"" to ""Top25"".

**=========================================**

*To my students:*

*I made this a binary classification task. Hence, there are only two labels:*

*""1"" when DJIA Adj Close value rose or stayed as the same;*

*""0"" when DJIA Adj Close value decreased.*

*For task evaluation, please use data from 2008-08-08 to 2014-12-31 as Training Set, and Test Set is then the following two years data (from 2015-01-02 to 2016-07-01). This is roughly a 80%/20% split.*

*And, of course, use AUC as the evaluation metric.*

**=========================================**

**+++++++++++++++++++++++++++++++++++++++++**

*To all kagglers:*

*Please upvote this dataset if you like this idea for market prediction.*

*If you think you coded an amazing trading algorithm,*

*friendly advice* 

*do play safe with your own money :)*

**+++++++++++++++++++++++++++++++++++++++++**

Feel free to contact me if there is any question~ 

And, remember me when you become a millionaire :P

**Note: If you'd like to cite this dataset in your publications, please use:**

`
Sun, J. (2016, August). Daily News for Stock Market Prediction, Version 1. Retrieved [Date You Retrieved This Data] from https://www.kaggle.com/aaron7sun/stocknews.
`

  [1]: https://www.reddit.com/r/worldnews?hl
  [2]: https://finance.yahoo.com/quote/%5EDJI/history?p=%5EDJI",2016-08-25T16:56:51.32Z,aaron7sun/stocknews,6.384909,https://www.kaggle.com/aaron7sun/stocknews,2,23371,CC BY-NC-SA 4.0,306,"Ready version: 1, 2016-08-25T16:56:51.32Z",0.882352948,finance
79,NYS Campaign Finance Filers and Filings,From New York State Open Data,"### Content  

More details about each file are in the individual file descriptions.  

### Context  

This is a dataset hosted by the State of New York. The state has an open data platform found [here](https://data.ny.gov/) and they update their information according the amount of data that is brought in. Explore New York State using Kaggle and all of the data sources available through the State of New York [organization page](https://www.kaggle.com/new-york-state)!  

* Update Frequency: This dataset is updated monthly.

### Acknowledgements

This dataset is maintained using Socrata's API and Kaggle's API. [Socrata](https://socrata.com/) has assisted countless organizations with hosting their open data and has been an integral part of the process of bringing more data to the public.  

This dataset is distributed under the following licenses: Public Domain",2019-07-03T09:42:54.753Z,new-york-state/nys-campaign-finance-filers-and-filings,361.934238,https://www.kaggle.com/new-york-state/nys-campaign-finance-filers-and-filings,2,26,CC0: Public Domain,0,"Ready version: 39, 2019-07-03T09:42:54.753Z",0.7941176,finance
80,Bitcoin Historical Data,"Bitcoin data at 1-min intervals from select exchanges, Jan 2012 to March 2019","### Context 
Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus ""chained"" together, serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow. Included here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. Happy (data) mining! 

### Content

coinbaseUSD_1-min_data_2014-12-01_to_2019-01-09.csv

bitstampUSD_1-min_data_2012-01-01_to_2019-03-13.csv 

CSV files for select bitcoin exchanges for the time period of Jan 2012 to March 2019, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price.  **Timestamps are in Unix time.  Timestamps without any trades or activity have their data fields filled with NaNs.** If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforseen technical error in data reporting or gathering. All effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk. 


### Acknowledgements and Inspiration

Bitcoin charts for the data. The various exchange APIs, for making it difficult or unintuitive enough to get OHLC and volume data at 1-min intervals that I set out on this data scraping project. Satoshi Nakamoto and the novel core concept of the blockchain, as well as its first execution via the bitcoin protocol. I'd also like to thank viewers like you! Can't wait to see what code or insights you all have to share. 
",2019-03-15T16:22:58.397Z,mczielinski/bitcoin-historical-data,123.326534,https://www.kaggle.com/mczielinski/bitcoin-historical-data,2,43353,CC BY-SA 4.0,128,"Ready version: 16, 2019-03-15T16:22:58.397Z",1.0,exchanges
81,Bitcoin markets,Exchanges daily data,10 different exchanges daily data from 01/01/2017 to 01/03/2017,2018-04-10T08:24:51.66Z,gorgia/bitcoin-markets,0.150843,https://www.kaggle.com/gorgia/bitcoin-markets,0,62,CC0: Public Domain,1,"Ready version: 2, 2018-04-10T08:24:51.66Z",0.4117647,exchanges
82,BTC orderbook history,BTC/JPY and BTC/USD orderbook history from some exchanges,,2018-08-19T08:30:52.583Z,vochicong/btc-historical-data,76.2327,https://www.kaggle.com/vochicong/btc-historical-data,1,88,Unknown,1,"Ready version: 1, 2018-08-19T08:30:52.583Z",0.470588237,exchanges
83,Cryptocurrency Historical Prices,"Prices of top cryptocurrencies including Bitcoin, Ethereum, Ripple, Bitcoin cash","### Context

Things like Block chain, Bitcoin, Bitcoin cash, Ethereum, Ripple etc are constantly coming in the news articles I read. So I wanted to understand more about it and [this post][1] helped me get started. Once the basics are done, the data scientist inside me started raising questions like:

1. How many cryptocurrencies are there and what are their prices and valuations?
2. Why is there a sudden surge in the interest in recent days? 

For getting answers to all these questions (and if possible to predict the future prices ;)), I started collecting data from [coinmarketcap][2] about the cryptocurrencies. 



So what next? 
Now that we have the price data, I wanted to dig a little more about the factors affecting the price of coins. I started of with Bitcoin and there are quite a few parameters which affect the price of Bitcoin. Thanks to [Blockchain Info][3], I was able to get quite a few parameters on once in two day basis.

This will help understand the other factors related to Bitcoin price and also help one make future predictions in a better way than just using the historical price.


### Content

The dataset has one csv file for each currency. Price history is available on a daily basis from April 28, 2013.  This dataset has the historical price information of some of the top crypto currencies by market capitalization. The currencies included are:

 - Bitcoin
 - Ethereum
 - Ripple
 - Bitcoin cash
 - Bitconnect
 - Dash
 - Ethereum Classic
 - Iota
 - Litecoin
 - Monero
 - Nem
 - Neo
 - Numeraire
 - Stratis
 - Waves



 - Date : date of observation 
 - Open : Opening price on the given day
 - High : Highest price on the given day
 - Low : Lowest price on the given day
 - Close : Closing price on the given day
 - Volume : Volume of transactions on the given day
 - Market Cap : Market capitalization in USD

**Bitcoin Dataset (bitcoin_dataset.csv) :**

This dataset has the following features.

 - Date : Date of observation
 - btc_market_price : Average USD market price across major bitcoin exchanges.
 - btc_total_bitcoins : The total number of bitcoins that have already been mined.
 - btc_market_cap : The total USD value of bitcoin supply in circulation.
 - btc_trade_volume : The total USD value of trading volume on major bitcoin exchanges.
 - btc_blocks_size : The total size of all block headers and transactions.
 - btc_avg_block_size : The average block size in MB.
 - btc_n_orphaned_blocks : The total number of blocks mined but ultimately not attached to the main Bitcoin blockchain.
 - btc_n_transactions_per_block : The average number of transactions per block.
 - btc_median_confirmation_time : The median time for a transaction to be accepted into a mined block.
 - btc_hash_rate : The estimated number of tera hashes per second the Bitcoin network is performing.
 - btc_difficulty : A relative measure of how difficult it is to find a new block.
 - btc_miners_revenue : Total value of coinbase block rewards and transaction fees paid to miners.
 - btc_transaction_fees : The total value of all transaction fees paid to miners.
 - btc_cost_per_transaction_percent : miners revenue as percentage of the transaction volume.
 - btc_cost_per_transaction : miners revenue divided by the number of transactions.
 - btc_n_unique_addresses : The total number of unique addresses used on the Bitcoin blockchain.
 - btc_n_transactions : The number of daily confirmed Bitcoin transactions.
 - btc_n_transactions_total : Total number of transactions.
 - btc_n_transactions_excluding_popular : The total number of Bitcoin transactions, excluding the 100 most popular addresses.
 - btc_n_transactions_excluding_chains_longer_than_100 : The total number of Bitcoin transactions per day excluding long transaction chains.
 - btc_output_volume : The total value of all transaction outputs per day.
 - btc_estimated_transaction_volume : The total estimated value of transactions on the Bitcoin blockchain.
 - btc_estimated_transaction_volume_usd : The estimated transaction value in USD value.

**Ethereum Dataset (ethereum_dataset.csv):**

This dataset has the following features

 - Date(UTC) : Date of transaction
 - UnixTimeStamp : unix timestamp
 - eth_etherprice : price of ethereum
 - eth_tx : number of transactions per day
 - eth_address : Cumulative address growth
 - eth_supply : Number of ethers in supply
 - eth_marketcap : Market cap in USD
 - eth_hashrate : hash rate in GH/s
 - eth_difficulty : Difficulty level in TH
 - eth_blocks : number of blocks per day
 - eth_uncles : number of uncles per day
 - eth_blocksize : average block size in bytes
 - eth_blocktime : average block time in seconds
 - eth_gasprice : Average gas price in Wei
 - eth_gaslimit : Gas limit per day
 - eth_gasused : total gas used per day
 - eth_ethersupply : new ether supply per day
 - eth_chaindatasize : chain data size in bytes
 - eth_ens_register : Ethereal Name Service (ENS) registrations per day



### Acknowledgements

This data is taken from [coinmarketcap][5] and it is [free][6] to use the data.

Bitcoin dataset is obtained from [Blockchain Info][7].

Ethereum dataset is obtained from [Etherscan][8].

Cover Image : Photo by Thomas Malama on Unsplash

### Inspiration

Some of the questions which could be inferred from this dataset are:

 1. How did the historical prices / market capitalizations of various currencies change over time?
 2. Predicting the future price of the currencies
 3. Which currencies are more volatile and which ones are more stable?
 4. How does the price fluctuations of currencies correlate with each other?
 5. Seasonal trend in the price fluctuations

Bitcoin / Ethereum dataset could be used to look at the following:

 1. Factors affecting the bitcoin / ether price.
 2. Directional prediction of bitcoin / ether price. (refer [this paper][9] for more inspiration)
 3. Actual bitcoin price prediction.
 


  [1]: https://www.linkedin.com/pulse/blockchain-absolute-beginners-mohit-mamoria
  [2]: https://coinmarketcap.com/
  [3]: https://blockchain.info/
  [4]: https://etherscan.io/charts
  [5]: https://coinmarketcap.com/
  [6]: https://coinmarketcap.com/faq/
  [7]: https://blockchain.info/
  [8]: https://etherscan.io/charts
  [9]: http://cs229.stanford.edu/proj2014/Isaac%20Madan,%20Shaurya%20Saluja,%20Aojia%20Zhao,Automated%20Bitcoin%20Trading%20via%20Machine%20Learning%20Algorithms.pdf",2018-02-21T12:36:47.22Z,sudalairajkumar/cryptocurrencypricehistory,0.715347,https://www.kaggle.com/sudalairajkumar/cryptocurrencypricehistory,2,19255,CC0: Public Domain,39,"Ready version: 13, 2018-02-21T12:36:47.22Z",0.7058824,exchanges
84,Cryptocurrencies,Historical price data for 1200 cryptocurrencies (excluding BTC),"### Context

Thousands of cryptocurrencies have sprung up in the past few years. Can you predict which one will be the next BTC?


### Content

The dataset contains daily opening, high, low, close, and trading volumes for over 1200 cryptocurrencies (excluding bitcoin).


### Acknowledgements

https://timescaledata.blob.core.windows.net/datasets/crypto_data.tar.gz

### Inspiration

Speculative forces are always at work on cryptocurrency exchanges - but do they contain any statistically significant features?",2018-07-11T14:15:12.387Z,akababa/cryptocurrencies,9.049797,https://www.kaggle.com/akababa/cryptocurrencies,2,1023,CC0: Public Domain,1,"Ready version: 4, 2018-07-11T14:15:12.387Z",0.882352948,exchanges
85,"AMEX, NYSE, NASDAQ stock histories","Daily historical data of over 8,000 stocks trading on AMEX, NYSE, and NASDAQ","# AMEX, NYSE, and NASDAQ stocks histories
#### Update every **Satur... Sun... I mean Friday... &gt;_&lt; sometime during the weekend. I lied, I've been too busy the past few months and haven't updated in forever until today (2019.2.18)**
###Full history of stock symbols:
- Unzip **fh_&lt; version_date &gt;.zip**
- Each stock symbol has a .csv file under **full_history/**
    - i.e. AMD.csv
- Columns in .csv
    - **date** - year-month-day, 2018-08-08
    - **volume** - int, volume of the day
    - **open** - float, opening price of the day
    - **close** - float, closing price of the day
    - **high** - float, highest price of the day
    - **low** - float, lowest price of the day
    - **adjclose** - float, adjusted closing price of the day

###Other files:
- all_symbols.txt - All the stock symbols with history
- excluded_symbols.txt - All the ones that I couldn't retrieve data for
- NASDAQ.txt - NASDAQ listing
- NYSE.txt - NYSE listing
- AMEX.txt - AMEX listing

###Disclaimer
This dataset contains **almost** all the stocks listed on these exchanges as of the date shown in the file name. Some of the symbols cannot be found on Yahoo Finance, which I plan on using CNN Money to scrape. There are other symbols that have different classes that require some modification before I can make them queryable... I have yet to decide on the best course of action. If you want to know what these excluded symbols are, see excluded_symbols.txt.

Note: there used to be some tickers missing because of poor connection, that's been solved now.

**I've also been asked why I don't put everything into one table, and here's my rationale (copy/pasted from my email):**

It is possible and I've debated this before, but I've decided to go with individual files for quite a number of reasons, and I highly recommend you consider these before combining them:
1) I don't need to load everything into memory or search for the right rows if I only want to work with particular sets,
2) easier and faster to manipulate (append, remove, or whatever) when all the data of a ticker is in the same place,
3) I don't need to repeat ticker names for each row just to know which row belongs to which ticker,
4) reduce risk, latency, and waits during parallel processing of different ticker data,
5) in case of any unforeseen bad writes or termination, this way reduces the chances of affecting the entire dataset and allows for restart anytime without the need to keep backup things up every 5 minutes.
I get all these benefits only at the cost of slightly larger compressed file and a few more lines of code. To me it's worth it, but I can understand if you are frustrated, but it is possible to concatenate everything.

###Github - for you to DIY:
**https://github.com/qks1lver/redtide**

###Data source
**Listing files (i.e. NYSE.txt) are from http://eoddata.com/symbols.aspx**

**Daily historical data compiled from Yahoo Finance**

###Need someone to talk to?
If you have questions, e-mail me: jiunyyen@gmail.com

Happy mining!
",2019-04-21T05:25:28.943Z,qks1lver/amex-nyse-nasdaq-stock-histories,526.877452,https://www.kaggle.com/qks1lver/amex-nyse-nasdaq-stock-histories,4,2590,CC BY-SA 4.0,5,"Ready version: 7, 2019-04-21T05:25:28.943Z",0.7647059,exchanges
86,Movie Dialog Corpus,A metadata-rich collection of fictional conversations from raw movie scripts,"### Context
This corpus contains a metadata-rich collection of fictional conversations extracted from raw movie scripts:

- 220,579 conversational exchanges between 10,292 pairs of movie characters
- involves 9,035 characters from 617 movies
- in total 304,713 utterances
- movie metadata included:
	- genres
	- release year
	- IMDB rating
	- number of IMDB votes
	- IMDB rating
- character metadata included:
	- gender (for 3,774 characters)
	- position on movie credits (3,321 characters)



### Content
In all files the original field separator was "" +++$+++ "" and have been converted to tabs (\t). Additionally, the original file encoding was ISO-8859-2. It's possible that the field separator conversion and decoding may have left some artifacts. 


- movie_titles_metadata.txt
	- contains information about each movie title
	- fields: 
		- movieID, 
		- movie title,
		- movie year, 
	   	- IMDB rating,
		- no. IMDB votes,
 		- genres in the format ['genre1','genre2',Ã‰,'genreN']

- movie_characters_metadata.txt
	- contains information about each movie character
	- fields:
		- characterID
		- character name
		- movieID
		- movie title
		- gender (""?"" for unlabeled cases)
		- position in credits (""?"" for unlabeled cases) 

- movie_lines.txt
	- contains the actual text of each utterance
	- fields:
		- lineID
		- characterID (who uttered this phrase)
		- movieID
		- character name
		- text of the utterance

- movie_conversations.txt
	- the structure of the conversations
	- fields
		- characterID of the first character involved in the conversation
		- characterID of the second character involved in the conversation
		- movieID of the movie in which the conversation occurred
		- list of the utterances that make the conversation, in chronological 
			order: ['lineID1','lineID2',Ã‰,'lineIDN']
			has to be matched with movie_lines.txt to reconstruct the actual content

- raw_script_urls.txt
	- the urls from which the raw sources were retrieved


### Acknowledgements
This corpus comes from the paper, ""Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs"" by Cristian Danescu-Niculescu-Mizil and Lillian Lee.  

The paper and up-to-date data can be found here: [http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html][1]

Please see the README for more information on the authors' collection procedures.

The file formats were converted to TSV and may contain a few errors

### Inspiration

 - What are all of these imaginary people talking about? Are they representative of how real people communicate?
 - Can you identify themes in movies from certain writers or directors? 
 - How does the dialog change between characters?

  [1]: http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html",2017-07-11T21:41:03.35Z,Cornell-University/movie-dialog-corpus,9.606952,https://www.kaggle.com/Cornell-University/movie-dialog-corpus,2,2537,Other (specified in description),4,"Ready version: 1, 2017-07-11T21:41:03.35Z",0.875,exchanges
87,Cornell Movie-Dialog Corpus,This corpus contains a large metadata-rich collection of fictional conversations,"This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:

220,579 conversational exchanges between 10,292 pairs of movie characters

involves 9,035 characters from 617 movies

in total 304,713 utterances

movie metadata included:

genres

release year

IMDB rating

number of IMDB votes

IMDB rating

character metadata included:

gender (for 3,774 characters)

position on movie credits (3,321 characters)",2018-03-28T12:42:03.66Z,rajathmc/cornell-moviedialog-corpus,10.04298,https://www.kaggle.com/rajathmc/cornell-moviedialog-corpus,1,215,CC0: Public Domain,3,"Ready version: 1, 2018-03-28T12:42:03.66Z",0.8125,exchanges
88,Enrico's Email Flows,"Anonymized Metadata (i.e. sender, receivers, date) from my 2005-2018 Archives","### Context

Email archives are a great source of information about the real-world social networks people are generally most involved in. Although sharing of full email exchanges is almost never a good idea, flow metadata (i.e. who sent a message to whom, and when) can be **anonymized** quite effectively and still carry a lot of information.

I'm sharing over 10 years of flow metadata from my work and personal email accounts to enable data scientists experiment with their favourite statistics and social network analysis tools. A getting-started notebook is available [here](https://www.kaggle.com/emarock/getting-started-with-email-flows).

For anyone willing to extract similar datasets from their own email accounts, the tool I put together for producing mine is available at [https://github.com/emarock/mailfix](https://github.com/emarock/mailfix) (currently supports extraction from Gmail accounts, IMAP accounts and Apple Mail on macOS).


### Content

This dataset contains two files:

 - `work.csv`: email flow metadata from my work account (~146,000 emails, from 2005 to 2018)
 - `personal.csv`: email flow metadata from my personal account (~41,000 emails, from 2006 to 2018)

As one should expect from any decade long archive, the data presents some partial corruptions and anomalies, that are however time-confined and should be easily identified and filtered out through basic statistical analysis. I will be happy to discuss and provide more information in the comments.


### Inspiration

Basic exploration:

 - Who am I?
 - Who's human and who's not? How different are attention-seekers from mailing list engines?
 - How did my communication patterns change over time? Did they change in the same way in and out of work?
 - Did my social network grow? Did it shrink?
 - Who's my boss? Who were my former ones? Who'll be the next one?

I will be also available to extend the dataset with additional data for training advanced classifiers (e.g. lists of actual humans, mailing lists, VIPs...). Feel free to ask in the comments.


### Anonymization and Privacy Note

The anonymization function (code [here](https://github.com/emarock/mailfix/blob/master/lib/anonymizer.js), tests [here](https://github.com/emarock/mailfix/blob/master/test/anonymizer.js)) is based on [djb2 string hashing](http://www.cse.yorku.ca/~oz/hash.html) and on a [Mersenne Twister pseudorandom generator](https://en.wikipedia.org/wiki/Mersenne_Twister), implemented in the [string-hash](https://www.npmjs.com/package/string-hash) and [casual](https://www.npmjs.com/package/casual) node.js modules. It should be practically irreversible, modulo implementation defects.

However, if you've ever been involved in email exchanges with me, you can work your way back to the anonymized address associated to your actual address by comparing the message timestamps. Similarly, with a little more guesswork, you can discover the anonymized addresses of those who were also involved in those exchanges. Since that is also true for them in respect to you, if that is of any concern just reach out and I'll censor the problematic entries in the dataset.",2018-03-21T10:10:40.373Z,emarock/enricos-email-flows,12.867137,https://www.kaggle.com/emarock/enricos-email-flows,3,84,CC0: Public Domain,1,"Ready version: 1, 2018-03-21T10:10:40.373Z",0.882352948,exchanges
89,Poloniex BTCETH OrderBook Stream Sample,Order Flow from a websocket,"### Context
Orderbook flow from exchanges is usually very expensive and hard to find luckily many exchanges offer websockets with full access to data. For a while I've been collecting this data, and this is a sample.

### Content

Data is acquired from [Poloniex][1] via websocket and stored in a database. Data consists of following fields:

1. typeOT - string - Order 'o' or Trade 't'

3. typeSB - numeric - Sell '0' or Buy '1' in term of trades, same goes for asks/bids in term of orders

4. rate - numeric - actual rate at which it was traded/bidded/asked

5. amount - numeric - actual amount

6. timeDateT - numeric - time in UNIX epoch

7.  seq - numeric - sequence number for sorting

8. timeDateOTe - date - time and date in a format, added by me

9. timeDateOTh - date - time and date in a format, added by me

### Acknowledgements

Thank You Poloniex and crypto community for sharing data with researches.

### Inspiration

- build a state of art database of features?
- build state of art database of images?
- visualize it in a new manner?
- process each tick/order very very fast?
- continuously build LOB and show some statistics?

  [1]: http://www.poloniex.com",2018-04-12T04:04:11.673Z,praeconium/poloniex-btceth-order-book-stream-sample,67.039769,https://www.kaggle.com/praeconium/poloniex-btceth-order-book-stream-sample,2,33,Unknown,1,"Ready version: 1, 2018-04-12T04:04:11.673Z",0.7058824,exchanges
90,S&P 500,SP500 data history from yahoo,"This data-set has data spanning from 2013 till 2018. The S&P 500 stock market index, maintained by S&P Dow Jones Indices, comprises 505 common stocks issued by 500 large-cap companies and traded on American stock exchanges, and covers about 80 percent of the American equity market by capitalization. The index is weighted by free-float market capitalization, so more valuable companies account for relatively more of the index. The index constituents and the constituent weights are updated regularly using rules published by S&P Dow Jones Indices. Although the index is called the S&P ""500"", the index contains 505 stocks because it includes two share classes of stock from 5 of its component companies.

The dataset comprises of all the S&P 500 components with the records created for each stock's open and closing rate spanning from last 5 years.

yahoo finance",2019-03-27T15:28:14.16Z,florentbaptist/sp-500,10.035049,https://www.kaggle.com/florentbaptist/sp-500,1,48,Unknown,1,"Ready version: 1, 2019-03-27T15:28:14.16Z",0.5882353,exchanges
91,NY Multi Agency Permits,From New York City Open Data,"### Content  

The Multi Agency Permits dataset contains the permits data from two different data sources/exchanges â€“ DOB Jobs Permits and DOHMH Permits  

### Context  

This is a dataset hosted by the City of New York. The city has an open data platform found [here](https://opendata.cityofnewyork.us/) and they update their information according the amount of data that is brought in. Explore New York City using Kaggle and all of the data sources available through the City of New York [organization page](https://www.kaggle.com/new-york-city)!  

* Update Frequency: This dataset is updated daily.

### Acknowledgements

This dataset is maintained using Socrata's API and Kaggle's API. [Socrata](https://socrata.com/) has assisted countless organizations with hosting their open data and has been an integral part of the process of bringing more data to the public.  

[Cover photo](https://unsplash.com/photos/E_8Zk_hfpcE) by [Adi Goldstein](https://unsplash.com/@adigold1) on [Unsplash](https://unsplash.com/)  
_Unsplash Images are distributed under a unique [Unsplash License](https://unsplash.com/license)._",2019-03-06T06:48:48.45Z,new-york-city/ny-multi-agency-permits,190.748122,https://www.kaggle.com/new-york-city/ny-multi-agency-permits,1,19,CC0: Public Domain,0,"Ready version: 229, 2019-03-06T06:48:48.45Z",0.7647059,exchanges
92,Sentiment140 dataset with 1.6 million tweets,Sentiment analysis with tweets,"### Context

This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .

### Content

It contains the following 6 fields:

1. **target**: the polarity of the tweet (*0* = negative, *2* = neutral, *4* = positive)

2. **ids**: The id of the tweet ( *2087*)

3. **date**: the date of the tweet (*Sat May 16 23:58:44 UTC 2009*)

4. **flag**: The query (*lyx*). If there is no query, then this value is NO_QUERY.

5. **user**: the user that tweeted (*robotickilldozr*)

6.  **text**: the text of the tweet (*Lyx is cool*)


### Acknowledgements

The official link regarding the dataset with resources about how it was generated is [here][1]
The official paper detailing the approach is  [here][2]

Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. *CS224N Project Report, Stanford, 1(2009), p.12*.


### Inspiration

To detect severity from tweets. You [may have a look at this][3].

[1]: http://%20http://help.sentiment140.com/for-students/
[2]: http://bhttp://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf
[3]: https://www.linkedin.com/pulse/social-machine-learning-h2o-twitter-python-marios-michailidis",2017-09-13T22:43:19.117Z,kazanova/sentiment140,88.031309,https://www.kaggle.com/kazanova/sentiment140,3,13986,Other (specified in description),37,"Ready version: 2, 2017-09-13T22:43:19.117Z",0.882352948,tweets
93,Russian Troll Tweets,"200,000 malicious-account tweets captured by NBC","### Context

As part of the House Intelligence Committee investigation into how Russia may have influenced the 2016 US Election, Twitter released the screen names of almost 3000 Twitter accounts believed to be connected to Russiaâ€™s Internet Research Agency, a company known for operating social media troll accounts. Twitter immediately suspended these accounts, deleting their data from Twitter.com and the Twitter API. A team at NBC News including Ben Popken and EJ Fox was able to reconstruct a dataset consisting of a subset of the deleted data for their investigation and were able to show how these troll accounts went on attack during key election moments. This dataset is the body of this open-sourced reconstruction.

For more background, read the NBC news article publicizing the release: [""Twitter deleted 200,000 Russian troll tweets. Read them here.""](https://www.nbcnews.com/tech/social-media/now-available-more-200-000-deleted-russian-troll-tweets-n844731)

### Content

This dataset contains two CSV files. `tweets.csv` includes details on individual tweets, while `users.csv` includes details on individual accounts.

To recreate a link to an individual tweet found in the dataset, replace `user_key` in `https://twitter.com/user_key/status/tweet_id` with the screen-name from the `user_key` field and `tweet_id` with the number in the `tweet_id` field.

Following the links will lead to a suspended page on Twitter. But some copies of the tweets as they originally appeared, including images, can be found by entering the links on web caches like `archive.org` and `archive.is`.

### Acknowledgements

If you publish using the data, please credit NBC News and include a link to this page. Send questions to `ben.popken@nbcuni.com`.

### Inspiration

What are the characteristics of the fake tweets? Are they distinguishable from real ones? ",2018-02-15T00:49:04.63Z,vikasg/russian-troll-tweets,21.99381,https://www.kaggle.com/vikasg/russian-troll-tweets,5,2291,CC0: Public Domain,8,"Ready version: 2, 2018-02-15T00:49:04.63Z",0.7352941,tweets
94,Financial Tweets,"Tweets from verified users concerning stocks traded on the NYSE, NASDAQ, & SNP","### Context
I have been interested in using public sentiment and journalism to gather sentiment profiles on publicly traded companies. I first developed a Python package (https://github.com/dwallach1/Stocker) that scrapes the web for articles written about companies, and then noticed the abundance of overlap with Twitter. I then developed a NodeJS project that I have been running on my RaspberryPi to monitor Twitter for all tweets coming from those mentioned in the *content* section. If one of them tweeted about a company in the stocks_cleaned.csv file, then it would write the tweet to the database. Currently, the file is only from earlier today, but after about a month or two, I plan to update the tweets.csv file (hopefully closer to 50,000 entries. 

I am not quite sure how this dataset will be relevant, but I hope to use these tweets and try to generate some sense of public sentiment score.  


### Content

This dataset has all the publicly traded companies (tickers and company names) that were used as input to fill the tweets.csv. The influencers whose tweets were monitored were: 
['MarketWatch', 'business', 'YahooFinance', 'TechCrunch', 'WSJ', 'Forbes', 'FT', 'TheEconomist', 'nytimes', 'Reuters', 'GerberKawasaki', 'jimcramer', 'TheStreet', 'TheStalwart', 'TruthGundlach', 'Carl_C_Icahn', 'ReformedBroker', 'benbernanke', 'bespokeinvest', 'BespokeCrypto', 'stlouisfed', 'federalreserve', 'GoldmanSachs', 'ianbremmer', 'MorganStanley', 'AswathDamodaran', 'mcuban', 'muddywatersre', 'StockTwits', 'SeanaNSmith'



### Acknowledgements

The data used here is gathered from a project I developed : https://github.com/dwallach1/StockerBot

### Inspiration

I hope to develop a financial sentiment text classifier that would be able to track Twitter's (and the entire public's) feelings about any publicly traded company (and cryptocurrency).",2018-08-09T17:13:55.89Z,davidwallach/financial-tweets,2.273078,https://www.kaggle.com/davidwallach/financial-tweets,2,2008,"Database: Open Database, Contents: Database Contents",4,"Ready version: 4, 2018-08-09T17:13:55.89Z",0.8235294,tweets
95,Australian Election 2019 Tweets,"May 18th 2019, 180k+ tweets","### Context

During the 2019 Australian election I noticed that almost everything I was seeing on Twitter was unusually left-wing. So I decided to scrape some data and investigate. Unfortunately my sentiment analysis has so far been too inaccurate to come to any useful conclusions. I decided to share the data so that others may be able to help with the sentiment or any other interesting analysis.


### Content

Over 180,000 tweets collected using Twitter API keyword search between 10.05.2019 and 20.05.2019.
Columns are as follows:

- **created_at**: Date and time of tweet creation
- **id**: Unique ID of the tweet
- **full_text**: Full tweet text
- **retweet_count**: Number of retweets
- **favorite_count**: Number of likes
- **user_id**: User ID of tweet creator
- **user_name**: Username of tweet creator
- **user_screen_name**: Screen name of tweet creator
- **user_description**: Description on tweet creator's profile
- **user_location**: Location given on tweet creator's profile
- **user_created_at**: Date the tweet creator joined Twitter  

The **latitude** and **longitude** of **user_location** is also available in location_geocode.csv. This information was retrieved using the Google Geocode API. 

### Acknowledgements

Thanks to Twitter for providing the free API.


### Inspiration

There are a lot of interesting things that could be investigated with this data. Primarily I was interested to do sentiment analysis, before and after the election results were known, to determine whether Twitter users are indeed a left-leaning bunch. Did the tweets become more negative as the results were known?

Other ideas for investigation include:

- Take into account retweets and favourites to weight overall sentiment analysis.

- Which parts of the world are interested (ie: tweet about) the Australian elections, apart from Australia?

- How do the users who tweet about this sort of thing tend to describe themselves?

- Is there a correlation between when the user joined Twitter and their political views (this assumes the sentiment analysis is already working well)?

- Predict gender from username/screen name and segment tweet count and sentiment by gender",2019-05-21T09:41:38.763Z,taniaj/australian-election-2019-tweets,29.972572,https://www.kaggle.com/taniaj/australian-election-2019-tweets,5,2370,CC0: Public Domain,8,"Ready version: 2, 2019-05-21T09:41:38.763Z",1.0,tweets
96,Tweets Targeting Isis,General tweets about Isis & related words,"Context
-------

The image at the top of the page is a frame from today's (7/26/2016) Isis #TweetMovie from twitter, a ""normal"" day when two Isis operatives murdered a priest saying mass in a French church. (You can see this in the center left).  A selection of data from this site is being made available here to Kaggle users.  

UPDATE: An excellent study by Audrey Alexander titled [Digital Decay?][1] is now available which traces the ""change over time among English-language Islamic State sympathizers on Twitter.

Intent
------

This data set is intended to be a counterpoise to the [How Isis Uses Twitter][2] data set.  That data set contains 17k tweets alleged to originate with ""100+ pro-ISIS fanboys"".  This new set contains 122k tweets collected on two separate days, 7/4/2016 and 7/11/2016, which contained any of the following terms, with no further editing or selection:

 - isis 
 - isil 
 - daesh 
 - islamicstate 
 - raqqa 
 - Mosul
 - ""islamic state""

This is not a perfect counterpoise as it almost surely contains a small number of pro-Isis fanboy tweets.  However, unless some entity, such as Kaggle, is willing to expend significant resources on a service something like an expert level Mechanical Turk or Zooniverse, a high quality counterpoise is out of reach.  

A counterpoise provides a balance or backdrop against which to measure a primary object, in this case the original pro-Isis data. So if anyone wants to discriminate between pro-Isis tweets and other tweets concerning Isis you will need to model the original pro-Isis data or **signal** against the counterpoise which is  **signal + noise**.  Further background and some analysis can be found in [this forum thread][3].

This data comes from postmodernnews.com/token-tv.aspx which daily collects about 25MB of Isis tweets for the purposes of graphical display. PLEASE NOTE: This server is not currently active.

Data Details
------------

There are several differences between the format of this data set and the pro-ISIS fanboy [dataset][4].
 1. All the twitter t.co tags have been expanded where possible
 2. There are no ""description, location, followers, numberstatuses"" data columns.   

I have also included my version of the original pro-ISIS fanboy set.  This version has all the t.co links expanded where possible. 


  [1]: https://extremism.gwu.edu/sites/extremism.gwu.edu/files/DigitalDecayFinal_0.pdf
  [2]: https://www.kaggle.com/kzaman/how-isis-uses-twitter
  [3]: https://www.kaggle.com/forums/f/1277/how-isis-uses-twitter/t/22165/isis-tweetmovie
  [4]: https://www.kaggle.com/kzaman/how-isis-uses-twitter",2016-07-29T23:59:27.183Z,activegalaxy/isis-related-tweets,11.258466,https://www.kaggle.com/activegalaxy/isis-related-tweets,2,1573,CC0: Public Domain,24,"Ready version: 3, 2016-07-29T23:59:27.183Z",0.875,tweets
97,Hillary Clinton and Donald Trump Tweets,Tweets from the major party candidates for the 2016 US Presidential Election,"Twitter has played an increasingly prominent role in the 2016 US Presidential Election. Debates have raged and candidates have risen and fallen based on tweets.

This dataset provides ~3000 recent tweets from [Hillary Clinton](https://twitter.com/HillaryClinton) and [Donald Trump](https://twitter.com/realDonaldTrump), the two major-party presidential nominees.

[![graph](https://www.kaggle.io/svf/377009/a6e6d9eeb0a7158b8f31498b1274c30b/clinton_vs_trump_retweets_and_favorites.png)](https://www.kaggle.com/benhamner/d/benhamner/clinton-trump-tweets/twitter-showdown-clinton-vs-trump)",2016-09-28T00:37:25.633Z,benhamner/clinton-trump-tweets,1.028293,https://www.kaggle.com/benhamner/clinton-trump-tweets,2,4958,Unknown,87,"Ready version: 1, 2016-09-28T00:37:25.633Z",0.7352941,tweets
98,Democrat Vs. Republican Tweets,200 tweets of Dems and Reps,"### Context

Twitter give the general public unfiltered direct access to the ideas and policies of politicians. This means that understanding the content and reach of these tweets can help us understand what connects with constituents. This dataset is meant to help with that exploration. By applying sentiment analysis (using an already trained system) we can apply sentiment context to these tweets. This will help us understand who responds to positive and negative content. Finally this analysis may help to indentify fake or hyperbole polarized Twitter users. 


### Content

The dataset contains two files both in .csv format.  The first is a list of the political party and the representative handles, and the second are the 200 latest tweets as of May 2018 from those twitter users.

### Acknowledgements

I would like to thank the following website and people who helped me get started

### Inspiration

I was first inspired by trying to find out if the average person would be able to distinguish between political tweets of no context was given. I made a small website that you can try this on. I will use real user data to cross check and see if ML methods are actually better than the average person. 

Other ace uses are the following:
Can we use this to detect Russian troll twitter accounts?
Do people respond to negative or positive political tweets?
",2018-05-27T17:20:55.79Z,kapastor/democratvsrepublicantweets,4.838286,https://www.kaggle.com/kapastor/democratvsrepublicantweets,3,913,CC0: Public Domain,8,"Ready version: 4, 2018-05-27T17:20:55.79Z",0.8235294,tweets
99,(Better) - Donald Trump Tweets!,A collection of all of Donald Trump tweets--better than its predecessors,"# Context 
Unlike [This][1] dataset, (which proved to be unusable). And [This one][2] which was filled with unnecessary columns; This Donald trump dataset has the cleanest usability and consists of over 7,000 tweets, no nonsense

**You may need to use a decoder other than UTF-8 if you want to see the emojis**
# Content

**Data consists of:**

 - 

 -Date

 -Time

 -Tweet_Text

 -Type

 -Media_Type

 -Hashtags

 -Tweet_Id

 -Tweet_Url

 -twt_favourites_IS_THIS_LIKE_QUESTION_MARK

 -Retweets

I scrapped this from someone on reddit
===
  [1]: https://www.kaggle.com/austinvernsonger/donaldtrumptweets
  [2]: https://www.kaggle.com/benhamner/clinton-trump-tweets",2017-04-16T04:24:29.33Z,kingburrito666/better-donald-trump-tweets,0.583499,https://www.kaggle.com/kingburrito666/better-donald-trump-tweets,2,2169,Unknown,30,"Ready version: 2, 2017-04-16T04:24:29.33Z",0.7058824,tweets
100,Election Day Tweets,"Tweets scraped from Twitter on November 8, 2016","Tweets scraped by [Chris Albon](https://github.com/chrisalbon) on the day of the 2016 United States elections.

Chris Albon's site only posted tweet IDs, rather than full tweets. We're in the process of scraping the full information, but due to API limiting this is taking a very long time. Version 1 of this dataset contains just under 400k tweets, about 6% of the 6.5 million originally posted.

This dataset will be updated as more tweets become available.

## Acknowledgements

[The original data](https://github.com/chrisalbon/election_day_2016_twitter) was scraped by [Chris Albon](https://github.com/chrisalbon), and tweet IDs were posted to his Github page.

## The Data

Since I (Ed King) used my own Twitter API key to scrape these tweets, this dataset contains a couple of fields with information on whether I have personally interacted with particular users or tweets. Since Kaggle encouraged me to not remove any data from a dataset, I'm leaving it in; feel free to build a classifier of the types of users I follow.

The dataset consists of the following fields:

- **text**: text of the tweet
- **created_at**: date and time of the tweet
- **geo**: a JSON object containing coordinates [latitude, longitude] and a `type'
- **lang**: Twitter's guess as to the language of the tweet
- **place**: a Place object from the Twitter API
- **coordinates**: a JSON object containing coordinates [longitude, latitude] and a `type'; **note** that coordinates are reversed from the **geo** field
- **user.favourites_count**: number of tweets the user has favorited
- **user.statuses_count**: number of statuses the user has posted
- **user.description**: the text of the user's profile description
- **user.location**: text of the user's profile location
- **user.id**: unique id for the user
- **user.created_at**: when the user created their account
- **user.verified**: bool; is user verified?
- **user.following**: bool; am I (Ed King) following this user?
- **user.url**: the URL that the user listed in their profile (not necessarily a link to their Twitter profile)
- **user.listed_count**: number of lists this user is on (?)
- **user.followers_count**: number of accounts that follow this user
- **user.default_profile_image**: bool; does the user use the default profile pic?
- **user.utc_offset**: positive or negative distance from UTC, in seconds
- **user.friends_count**: number of accounts this user follows
- **user.default_profile**: bool; does the user use the default profile?
- **user.name**: user's profile name
- **user.lang**: user's default language
- **user.screen_name**: user's account name
- **user.geo_enabled**: bool; does user have geo enabled?
- **user.profile_background_color**: user's profile background color, as hex in format ""RRGGBB"" (no '#')
- **user.profile_image_url**: a link to the user's profile pic
- **user.time_zone**: full name of the user's time zone
- **id**: unique tweet ID
- **favorite_count**: number of times the tweet has been favorited
- **retweeted**: is this a retweet?
- **source**: if a link, where is it from (e.g., ""Instagram"")
- **favorited**: have I (Ed King) favorited this tweet?
- **retweet_count**: number of times this tweet has been retweeted


I've also included a file called ```bad_tweets.csv``` , which includes all of the tweet IDs that could not be scraped, along with the error message I received while trying to scrape them. This typically happens because the tweet has been deleted, the user has deleted their account (or been banned), or the user has made their tweets private. The fields in this file are **id** and **exception.response**.",2016-11-26T23:01:06.707Z,kinguistics/election-day-tweets,88.00211,https://www.kaggle.com/kinguistics/election-day-tweets,2,1322,CC0: Public Domain,6,"Ready version: 1, 2016-11-26T23:01:06.707Z",0.875,tweets
101,Russian Troll Tweets,3 million tweets from accounts associated with the 'Internet Research Agency',"# 3 million Russian troll tweets

This data was used in the FiveThirtyEight story [Why Weâ€™re Sharing 3 Million Russian Troll Tweets](https://fivethirtyeight.com/features/why-were-sharing-3-million-russian-troll-tweets/).

This directory contains data on nearly 3 million tweets sent from Twitter handles connected to the Internet Research Agency, a Russian ""troll factory"" and a defendant in [an indictment](https://www.justice.gov/file/1035477/download) filed by the Justice Department in February 2018, as part of special counsel Robert Mueller's Russia investigation. The tweets in this database were sent between February 2012 and May 2018, with the vast majority posted from 2015 through 2017.

FiveThirtyEight obtained the data from Clemson University researchers [Darren Linvill](https://www.clemson.edu/cbshs/faculty-staff/profiles/darrenl), an associate professor of communication, and [Patrick Warren](http://pwarren.people.clemson.edu/), an associate professor of economics, on July 25, 2018. They gathered the data using custom searches on a tool called Social Studio, owned by Salesforce and contracted for use by Clemson's [Social Media Listening Center](https://www.clemson.edu/cbshs/centers-institutes/smlc/).

The basis for the Twitter handles included in this data are the [November 2017](https://democrats-intelligence.house.gov/uploadedfiles/exhibit_b.pdf) and [June 2018](https://democrats-intelligence.house.gov/uploadedfiles/ira_handles_june_2018.pdf) lists of Internet Research Agency-connected handles that Twitter [provided](https://democrats-intelligence.house.gov/news/documentsingle.aspx?DocumentID=396) to Congress. This data set contains every tweet sent from each of the 2,752 handles on the November 2017 list since May 10, 2015. For the 946 handles newly added on the June 2018 list, this data contains every tweet since June 19, 2015. (For certain handles, the data extends even earlier than these ranges. Some of the listed handles did not tweet during these ranges.) The researchers believe that this includes the overwhelming majority of these handlesâ€™ activity. The researchers also removed 19 handles that remained on the June 2018 list but that they deemed very unlikely to be IRA trolls.

In total, the nine CSV files include 2,973,371 tweets from 2,848 Twitter handles. Also, as always, caveat emptor -- in this case, tweet-reader beware: In addition to their own content, some of the tweets contain active links, which may lead to adult content or worse.

The Clemson researchers used this data in a working paper, [Troll Factories: The Internet Research Agency and State-Sponsored Agenda Building](http://pwarren.people.clemson.edu/Linvill_Warren_TrollFactory.pdf), which is currently under review at an academic journal. The authorsâ€™ analysis in this paper was done on the data file provided here, limiting the date window to June 19, 2015, to Dec. 31, 2017.

The files have the following columns:

    Header | Definition
    ---|---------
    `external_author_id` | An author account ID from Twitter 
    `author` | The handle sending the tweet
    `content` | The text of the tweet
    `region` | A region classification, as [determined by Social Studio](https://help.salesforce.com/articleView?   id=000199367&type=1)
    `language` | The language of the tweet
    `publish_date` | The date and time the tweet was sent
    `harvested_date` | The date and time the tweet was collected by Social Studio
    `following` | The number of accounts the handle was following at the time of the tweet
    `followers` | The number of followers the handle had at the time of the tweet
    `updates` | The number of â€œupdate actionsâ€ on the account that authored the tweet, including tweets, retweets and likes
    `post_type` | Indicates if the tweet was a retweet or a quote-tweet
    `account_type` | Specific account theme, as coded by Linvill and Warren
    `retweet` | A binary indicator of whether or not the tweet is a retweet
    `account_category` | General account theme, as coded by Linvill and Warren
    `new_june_2018` | A binary indicator of whether the handle was newly listed in June 2018

If you use this data and find anything interesting, please let us know. Send your projects to oliver.roeder@fivethirtyeight.com or [@ollie](https://twitter.com/ollie).

The Clemson researchers wish to acknowledge the assistance of the Clemson University Social Media Listening Center and Brandon Boatwright of the University of Tennessee, Knoxville.",2018-08-01T09:04:25.733Z,fivethirtyeight/russian-troll-tweets,183.447803,https://www.kaggle.com/fivethirtyeight/russian-troll-tweets,5,938,CC0: Public Domain,5,"Ready version: 2, 2018-08-01T09:04:25.733Z",0.7058824,tweets
102,Christmas Tweets,"50,000 scraped tweet metadata from this 2k16 Christmas","# Context 

This dataset contains the metadata of over 50,000 tweets from Christmas Eve and Christmas. We are hoping the data science and research community can use this to develop new and informative conclusions about this holiday season.


# Content

We acquired this data through a web crawler written in Java. The first field is the id of the tweet, and the second is the HTML metadata. We recommend using BeautifulSoup or another library to parse this data and extract information from each tweet.


# Inspiration

We would especially like to see research on the use of emojis in tweets, the type of sentiment there is on Christmas (Maybe determine how grateful each country is), or some kind of demographic on the age or nationality of active Twitter users during Christmas.",2016-12-25T18:58:34.293Z,dhruvm/christmastwitterdata,7.189259,https://www.kaggle.com/dhruvm/christmastwitterdata,2,531,"Database: Open Database, Contents: Database Contents",4,"Ready version: 1, 2016-12-25T18:58:34.293Z",0.8235294,tweets
103,Good Morning Tweets,Tweets captured over ~24 hours with the text 'good morning' in them,"# Context 

It's possible, using R (and no doubt Python), to 'listen' to Twitter and capture tweets that match a certain description. I decided to test this out by grabbing tweets with the text 'good morning' in them over a 24 hours period, to see if you could see the world waking up from the location information and time-stamp. The main R package used was [streamR][1]


# Content

The tweets have been tidied up quite a bit. First, I've removed re-tweets, second, I've removed duplicates (not sure why Twitter gave me them in the first place), third, I've made sure the tweet contained the words 'good morning' (some tweets were returned that didn't have the text in for some reason) and fourth, I've removed all the tweets that didn't have a longitude and latitude included. This latter step removed the vast majority. What's left are various aspects of just under 5000 tweets. The columns are,

- text	
- retweet_count	
- favorited	
- truncated	
- id_str	
- in_reply_to_screen_name	
- source	
- retweeted	
- created_at	
- in_reply_to_status_id_str	
- in_reply_to_user_id_str	
- lang	
- listed_count	
- verified	
- location	
- user_id_str	
- description	
- geo_enabled	
- user_created_at	
- statuses_count	
- followers_count
- favourites_count	
- protected	
- user_url	
- name	
- time_zone	
- user_lang	
- utc_offset	
- friends_count	
- screen_name	
- country_code	
- country	
- place_type	
- full_name	
- place_name	
- place_id	
- place_lat	
- place_lon	
- lat	
- lon	
- expanded_url	
- url


# Acknowledgements

I used a few blog posts to get the code up and running, including [this one][2]


# Code

The R code I used to get the tweets is as follows (note, I haven't includes the code to set up the connection to Twitter. See the streamR PFD and the link above for that. You need a Twitter account),


    i = 1
    
    while (i <= 280) {
    
    filterStream(""tw_gm.json"", timeout = 300, oauth = my_oauth, track = 'good morning', language = 'en')
    tweets_gm = parseTweets(""tw_gm.json"")
        
    ex = grepl('RT', tweets_gm$text, ignore.case = FALSE) #Remove the RTs
    tweets_gm = tweets_gm[!ex,]
    
    ex = grepl('good morning', tweets_gm$text, ignore.case = TRUE) #Remove anything without good morning in the main tweet text
    tweets_gm = tweets_gm[ex,]
    
    ex = is.na(tweets_gm$place_lat) #Remove any with missing place_latitude information
    tweets_gm = tweets_gm[!ex,]
    
    tweets.all = rbind(tweets.all, tweets_gm) #Add to the collection
    
    i=i+1
    
    Sys.sleep(5)
    
    }


  [1]: https://cran.r-project.org/web/packages/streamR/streamR.pdf
  [2]: http://politicaldatascience.blogspot.co.uk/2015/12/rtutorial-using-r-to-harvest-twitter.html",2016-12-09T16:24:24.507Z,tentotheminus9/good-morning-tweets,0.978617,https://www.kaggle.com/tentotheminus9/good-morning-tweets,3,590,Unknown,10,"Ready version: 1, 2016-12-09T16:24:24.507Z",0.7058824,tweets
104,Elon Musk's Tweets,Tweets by @elonmusk from 2012 to 2017,"### Context

[Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk) is an American business magnate. He was one of the founders of PayPal in the past, and the founder and/or cofounder and/or CEO of SpaceX, Tesla, SolarCity, OpenAI, Neuralink, and The Boring Company in the present. He is known as much for his extremely forward-thinking ideas and huge media presence as he is for his extremely business savvy.

Musk is famously active on Twitter. This dataset contains all tweets made by [@elonmusk](https://twitter.com/elonmusk), his official Twitter handle, between November 16, 2012 and September 29, 2017.

### Content

This dataset includes the body of the tweet and the time it was made, as well as who it was re-tweeted from (if it is a retweet).

### Inspiration

* Can you figure out Elon Musk's opinions on various things by studying his Twitter statements?
* How Elon Musk's post rate increased, decreased, or stayed about the same over time? ",2017-10-12T10:41:47.637Z,kulgen/elon-musks-tweets,0.171605,https://www.kaggle.com/kulgen/elon-musks-tweets,0,898,CC0: Public Domain,7,"Ready version: 1, 2017-10-12T10:41:47.637Z",0.5882353,tweets
105,"5,000 #JustDoIt! Tweets Dataset",People reacting to Nike's endorsement of Colin Kaepernick,"### Context
Nike just announced its partnership with Colin Kaepernick to be the face of the 30th anniversary of its **JustDoIt** campaign.   
They used the slogan ""Believe in something, even if it means sacrificing everything.""  
Kaepernick had made a controversial decision not to stand up during the national anthem, as a protest to police brutality, a while back.  
This has stirred a heated debate, and became a big national issue especially when [Donald Trump commented on it](https://www.youtube.com/watch?v=oY3hpZVZ7pk).


### Content

This dataset contains 5,000 tweets that contain the hashtag #JustDoIt.   
All tweets happened on September 7, 2018, which is days after Nike made its announcement to endorse Kaepernick.

#### Some of the top entities of those tweets: 
### #JustDoIt #Nike #ColinKaepernick #TakeaKnee
### ğŸ˜‚ ğŸ¤£ âœ” ğŸ”¥ â¤ ğŸˆ ğŸ’¯ ğŸ’™ ğŸ‡ºğŸ‡¸
### @Nike @Kaepernick7 @realDonaldTrump



### Acknowledgements
Python, Twitter, twython, pandas, matplotlib do the heavy lifting in generating the data and exploring it.
 
### Inspiration
I'm an online marketing person. Love words, love numbers. Can't help it!  
I think it's very interesting to see how these issues unfold, and how people respond to them. Maybe you can uncover some hidden insights or patterns.   
I'm also trying to show how you can [use the `extract_` functions from my `advertools` package](https://www.kaggle.com/eliasdabbas/extract-entities-from-social-media-posts). 
",2018-09-08T16:32:37.09Z,eliasdabbas/5000-justdoit-tweets-dataset,3.256985,https://www.kaggle.com/eliasdabbas/5000-justdoit-tweets-dataset,4,944,CC0: Public Domain,6,"Ready version: 3, 2018-09-08T16:32:37.09Z",0.9411765,tweets
106,FIFA World Cup 2018 Tweets,A collection of tweets during the 2018 FIFA World Cup,"**Context:**
------------
The FIFA World Cup (often simply called the World Cupâ€Š), â€Šbeing the most prestigious association football tournament, as well as the most widely viewed and followed sporting event in the world, was one of the Top Trending topics frequently on Twitter while ongoing.&nbsp;<br> <br>
This dataset contains a random collection of 530k tweets starting from the Round of 16 till the World Cup Final that took place on 15 July, 2018 & was won by France<br>
A preliminary analysis from the data (till the Round of 16) is available at: <br>
[https://medium.com/@ritu_rg/nlp-text-visualization-twitter-sentiment-analysis-in-r-5ac22c778448][1]


**Content:**
------------
**Data Collection:**  <br>
The dataset was created using the Tweepy API, by streaming tweets from world-wide football fans before, during or after the matches.  <br> 
Tweepy is a Python API for accessing the Twitter API, that provides an easy-to-use interface for streaming real-time data from Twitter. More information related to this API can be found at: http://tweepy.readthedocs.io/en/v3.5.0/ <br> <br>

**Data Pre-processing:**  <br>
The dataset includes English language tweets containing any references to FIFA or the World Cup. The collected tweets have been pre-processed to facilitate analysisâ€Š, while trying to ensure that any information from the original tweets is not lost.&nbsp; <br>
- The original tweet has been stored in the column ""Orig_tweet"".&nbsp; <br>
- As part of pre-processing, using the ""BeautifulSoup"" & ""regex"" libraries in Python, the tweets have been cleaned off any nuances as required for natural language processing, such as website names, hashtags, user mentions, special characters, RTs, tabs, heading/trailing/multiple spaces, among others. <br>
- Words containing extensions such as n't 'll 're 've have been replaced with their proper English language counterparts. Duplicate tweets have been removed from the dataset. <br>
- The original Hashtags & User Mentions extracted during the above step have also been stored in separate columns. <br> <br>

**Data Storage:**  <br>
The collected tweets have been consolidated into a single dataset & shared as a Comma Separated Values file ""FIFA.csv"". <br>
Each tweet is uniquely identifiable by its ID, & characterized by the following attributes, per availability: <br>
- ""Lang""â€Š-â€ŠLanguage of the tweet <br>
- ""Date""â€Š-â€ŠWhen it was tweeted <br>
- ""Source""â€Š-â€ŠThe device/medium where it was tweeted from <br>
- ""len""â€Š-â€ŠThe length of the tweet <br>
- ""Orig_Tweet""â€Š-â€ŠThe tweet in its original form <br>
- ""Tweet""â€Š-â€ŠThe updated tweet after pre-processing <br>
- ""Likes""â€Š-â€ŠThe number of likes received by the tweet (till the time the extraction was done) <br>
- ""RTs""â€Š-â€ŠThe number of times the tweet was shared <br>
- ""Hashtags""â€Š-â€ŠThe Hashtags found in the original tweet <br>
- ""UserMentionNames"" & ""UserMentionID""â€Š-â€Š Extracted from the original tweet <br> <br>

It also includes the following attributes about the person that the tweet is from: <br>
- ""Name"" & ""Place"" of the user <br>
- ""Followers""â€Š-â€ŠThe number of followers that the user account has <br>
- ""Friends""â€Š-â€ŠThe number of friends the user account has <br>


**Acknowledgements:** <br>
-----------------
The following resources have helped me through using the Tweepy API: <br>
[http://tweepy.readthedocs.io/en/v3.5.0/auth_tutorial.html][2] <br>
[https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets][3] <br>
[https://www.safaribooksonline.com/library/view/mining-the-social/9781449368180/ch01.html][4] <br>



**Inspiration:** <br>
------------
This project gave me a fascinating look into the conversations & sentiments of people from all over the world, who were following this prestigious football tournament, while also giving me the opportunity to explore some of the streaming, natural language processing & visualizations techniques in both R & Python <br> <br>


  [1]: https://medium.com/@ritu_rg/nlp-text-visualization-twitter-sentiment-analysis-in-r-5ac22c778448
  [2]: http://tweepy.readthedocs.io/en/v3.5.0/auth_tutorial.html
  [3]: https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets
  [4]: https://www.safaribooksonline.com/library/view/mining-the-social/9781449368180/ch01.html",2018-08-15T05:30:46.967Z,rgupta09/world-cup-2018-tweets,45.758955,https://www.kaggle.com/rgupta09/world-cup-2018-tweets,1,1516,Unknown,8,"Ready version: 4, 2018-08-15T05:30:46.967Z",0.647058845,tweets
107,Hurricane Harvey Tweets,Recent tweets on Hurricane Harvey,"### Context

Tweets containing Hurricane Harvey from the morning of 8/25/2017. I hope to keep this updated if computer problems do not persist. 

***8/30 Update**
This update includes the most recent tweets tagged ""Tropical Storm Harvey"", which spans from 8/20 to 8/30 as well as the properly merged version of dataset including Tweets from when Harvey before it was downgraded back to a tropical storm. 


### Inspiration

What are the popular tweets?

Can we find popular news stories from this?

Can we identify people likely staying or leaving, and is there a difference in sentiment between the two groups?

Is it possible to predict popularity with respect to retweets, likes, and shares?",2017-09-09T17:40:57.633Z,dan195/hurricaneharvey,23.022262,https://www.kaggle.com/dan195/hurricaneharvey,2,655,CC0: Public Domain,3,"Ready version: 6, 2017-09-09T17:40:57.633Z",0.8235294,tweets
108,"Elon Musk Tweets, 2010 to 2017",All Elon Musk Tweets from 2010 to 2017,"# Content

 - tweet id, contains tweet-stamp
 - date + time, date and time of day (24hr)
 - tweet text, text of tweet, remove 'b'

# usage

What's someone going to do with a bunch of tweets?

 - Maybe someone would want to generate text using this dataset
 - or do sentiment analysis
 - Or find out the most likely time of day Elon would tweet.
 - pie his tweets per month, ITS DATA!!

Either way its up to you!

# Inspiration:

![elon][1]


  [1]: http://iotblog.ir/wp-content/uploads/2017/01/eloninfograph.jpg",2017-04-23T09:08:35.637Z,kingburrito666/elon-musk-tweets,0.178148,https://www.kaggle.com/kingburrito666/elon-musk-tweets,3,369,Other (specified in description),11,"Ready version: 1, 2017-04-23T09:08:35.637Z",0.8235294,tweets
109,First GOP Debate Twitter Sentiment,Analyze tweets on the first 2016 GOP Presidential Debate,"*This data originally came from [Crowdflower's Data for Everyone library](http://www.crowdflower.com/data-for-everyone).*

As the original source says,

> We looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both sentiment analysis and data categorization. Contributors were asked if the tweet was relevant, which candidate was mentioned, what subject was mentioned, and then what the sentiment was for a given tweet. We've removed the non-relevant messages from the uploaded dataset.

The data we're providing on Kaggle is a slightly reformatted version of the original source. It includes both a CSV file and SQLite database. The code that does these transformations is [available on GitHub](https://github.com/benhamner/crowdflower-first-gop-debate-twitter-sentiment)",2016-10-06T03:19:29.417Z,crowdflower/first-gop-debate-twitter-sentiment,2.669318,https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment,2,12708,CC BY-NC-SA 4.0,122,"Ready version: 2, 2016-10-06T03:19:29.417Z",0.852941155,tweets
110,24 thousand tweets later ,2017 tweets from incubators and accelerators,"###Context

I collected this data from incubators and accelerators to find out what they have been talking about in 2017.

###Content

The data contains the twitter usernames of various organizations and tweets for the year 2017 collected on 28th Dec 2017.

###Acknowledgements

Much appreciation to @emmanuelkens for helping in thinking through this

###Inspiration

I am very curious to find out what the various organizations have been talking about in 2017. I would also like to find out the most popular organization by tweets and engagement. I am also curious to find out if there is any relationship between the number of retweets a tweet gets and the time of day it was posted!
",2018-01-07T15:10:04.06Z,derrickmwiti/24-thousand-tweets-later,3.697713,https://www.kaggle.com/derrickmwiti/24-thousand-tweets-later,2,503,GPL 2,4,"Ready version: 3, 2018-01-07T15:10:04.06Z",0.7647059,tweets
111,2017 #Oscars Tweets,"29,000+ tweets about the 2017 Academy Awards","Hi,
I have extracted the Tweets related to Oscar 2017.

 - The timeframe is from Feb 27th,2017 to March 2nd,2017.
 - The number ofTweets is 29498. 
 - The whole idea of extraction to know how people reacted in general about Oscars and also after the Best Picture mix up.",2017-03-17T19:50:40.257Z,madhurinani/oscars-2017-tweets,11.54496,https://www.kaggle.com/madhurinani/oscars-2017-tweets,3,308,Other (specified in description),7,"Ready version: 3, 2017-03-17T19:50:40.257Z",0.7647059,tweets
112,Hacker News,All posts from Y Combinator's social news website from 2006 to late 2017,"### Context

This dataset contains all stories and comments from Hacker News from its launch in 2006.  Each story contains a story id, the author that made the post, when it was written, and the number of points the story received. Hacker News is a social news website focusing on computer science and entrepreneurship. It is run by Paul Graham's investment fund and startup incubator, Y Combinator. In general, content that can be submitted is defined as ""anything that gratifies one's intellectual curiosity"".

### Content

Each story contains a story ID, the author that made the post, when it was written, and the number of points the story received.

Please note that the text field includes profanity. All texts are the authorâ€™s own, do not necessarily reflect the positions of Kaggle or Hacker News, and are presented without endorsement.

## Querying BigQuery tables

You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at `bigquery-public-data.hacker_news.[TABLENAME]`. **Fork [this kernel][1] to get started**.

### Acknowledgements 

This dataset was kindly made publicly available by [Hacker News][2] under [the MIT license][3].

### Inspiration

 - Recent studies have found that many forums tend to be dominated by a
   very small fraction of users. Is this true of Hacker News?

 - Hacker News has received complaints that the site is biased towards Y
   Combinator startups. Do the data support this? 

 - Is the amount of coverage by Hacker News predictive of a startupâ€™s
   success?


  [1]: https://www.kaggle.com/mrisdal/mentions-of-kaggle-on-hacker-news
  [2]: https://github.com/HackerNews/API
  [3]: https://github.com/HackerNews/API/blob/master/LICENSE",2019-02-12T00:34:51.853Z,hacker-news/hacker-news,15883.923392,https://www.kaggle.com/hacker-news/hacker-news,4,0,CC0: Public Domain,1504,"Ready version: 2, 2019-02-12T00:34:51.853Z",0.7058824,news
113,News Category Dataset,Identify the type of news based on headlines and short descriptions,"# Context
This dataset contains around 200k news headlines from the year 2012 to 2018 obtained from [HuffPost](https://www.huffingtonpost.com/). The model trained on this dataset could be used to identify tags for untracked news articles or to identify the type of language used in different news articles.

# Content
Each news headline has a corresponding category. Categories and corresponding article counts are as follows:

* ```POLITICS```: ```32739```

* ```WELLNESS```: ```17827```

* ```ENTERTAINMENT```: ```16058```

* ```TRAVEL```: ```9887```

* ```STYLE & BEAUTY```: ```9649```

* ```PARENTING```: ```8677```

* ```HEALTHY LIVING```: ```6694```

* ```QUEER VOICES```: ```6314```

* ```FOOD & DRINK```: ```6226```

* ```BUSINESS```: ```5937```

* ```COMEDY```: ```5175```

* ```SPORTS```: ```4884```

* ```BLACK VOICES```: ```4528```

* ```HOME & LIVING```: ```4195```

* ```PARENTS```: ```3955```

* ```THE WORLDPOST```: ```3664```

* ```WEDDINGS```: ```3651```

* ```WOMEN```: ```3490```

* ```IMPACT```: ```3459```

* ```DIVORCE```: ```3426```

* ```CRIME```: ```3405```

* ```MEDIA```: ```2815```

* ```WEIRD NEWS```: ```2670```

* ```GREEN```: ```2622```

* ```WORLDPOST```: ```2579```

* ```RELIGION```: ```2556```

* ```STYLE```: ```2254```

* ```SCIENCE```: ```2178```

* ```WORLD NEWS```: ```2177```

* ```TASTE```: ```2096```

* ```TECH```: ```2082```

* ```MONEY```: ```1707```

* ```ARTS```: ```1509```

* ```FIFTY```: ```1401```

* ```GOOD NEWS```: ```1398```

* ```ARTS & CULTURE```: ```1339```

* ```ENVIRONMENT```: ```1323```

* ```COLLEGE```: ```1144```

* ```LATINO VOICES```: ```1129```

* ```CULTURE & ARTS```: ```1030```

* ```EDUCATION```: ```1004```

# Acknowledgements

This dataset was collected from [HuffPost](https://www.huffingtonpost.com/). If this is against the TOS, please let me know and I will take it down.


# Inspiration

* Can you categorize news articles based on their headlines and short descriptions?  

* Do news articles from different categories have different writing styles?

* A classifier trained on this dataset could be used on a free text to identify the type of language being used.

# Citation

Please link to ""https://rishabhmisra.github.io/publications/"" in your report if you're using this dataset.

If you're using this dataset for research purposes, please use the following BibTex for citation:


@dataset{dataset,

author = {Misra, Rishabh},

year = {2018},

month = {06},

pages = {},

title = {News Category Dataset},

doi = {10.13140/RG.2.2.20331.18729}

}


Thanks!

### Other datasets
Please also checkout the following datasets collected by me:

* [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)

* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)

* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)",2018-12-02T04:09:45.777Z,rmisra/news-category-dataset,26.337702,https://www.kaggle.com/rmisra/news-category-dataset,4,5173,CC0: Public Domain,22,"Ready version: 2, 2018-12-02T04:09:45.777Z",1.0,news
114,Getting Real about Fake News,Text & metadata from fake & biased news sources around the web,"The latest hot topic in the news is fake news and many are wondering what data scientists can do to detect it and stymie its viral spread. This dataset is only a first step in understanding and tackling this problem. It contains text and metadata scraped from 244 websites tagged as ""bullshit"" by the [BS Detector][2] Chrome Extension by [Daniel Sieradski][3]. 

**Warning**: I did not modify the list of news sources from the BS Detector so as not to introduce my (useless) layer of bias; I'm not an authority on fake news. There may be sources whose inclusion you disagree with. It's up to you to decide how to work with the data and how you might contribute to ""improving it"". The labels of ""bs"" and ""junksci"", etc. do not constitute capital ""t"" Truth. If there are other sources you would like to include, start a discussion. If there are sources you believe should not be included, start a discussion or write a kernel analyzing the data. Or take the data and do something else productive with it. Kaggle's choice to host this dataset is not meant to express any particular political affiliation or intent.

## Contents

The dataset contains text and metadata from 244 websites and represents 12,999 posts in total from the past 30 days. The data was pulled using the [webhose.io][4] API; because it's coming from their crawler, not all websites identified by the BS Detector are present in this dataset. Each website was labeled according to the BS Detector as documented here. Data sources that were missing a label were simply assigned a label of ""bs"". There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.

## Fake news in the news

For inspiration, I've included some (presumably non-fake) recent stories covering fake news in the news. This is a sensitive, nuanced topic and if there are other resources you'd like to see included here, please leave a suggestion. From defining fake, biased, and misleading news in the first place to deciding how to take action (a blacklist is not a good answer), there's a lot of information to consider beyond what can be neatly arranged in a CSV file.

* [How Fake News Spreads (NYT)][6]

* [We Tracked Down A Fake-News Creator In The Suburbs. Here's What We Learned (NPR)][7]

* [Does Facebook Generate Over Half of its Revenue from Fake News? (Forbes)][8]

* [Fake News is Not the Only Problem (Points - Medium)][9]

* [Washington Post Disgracefully Promotes a McCarthyite Blacklist From a New, Hidden, and Very Shady Group (The Intercept)][10]

## Improvements

If you have suggestions for improvements or would like to contribute, please let me know. The most obvious extensions are to include data from ""real"" news sites and to address the bias in the current list. I'd be happy to include any contributions in future versions of the dataset.

## Acknowledgements

Thanks to [Anthony][11] for pointing me to [Daniel Sieradski's BS Detector][12]. Thank you to Daniel Nouri for encouraging me to add a disclaimer to the dataset's page.


  [2]: https://github.com/selfagency/bs-detector
  [3]: https://github.com/selfagency
  [4]: https://webhose.io/api
  [5]: https://github.com/selfagency/bs-detector/blob/master/chrome/data/data.json
  [6]: http://www.nytimes.com/2016/11/20/business/media/how-fake-news-spreads.html
  [7]: http://www.npr.org/sections/alltechconsidered/2016/11/23/503146770/npr-finds-the-head-of-a-covert-fake-news-operation-in-the-suburbs
  [8]: http://www.forbes.com/forbes/welcome/?toURL=http://www.forbes.com/sites/petercohan/2016/11/25/does-facebook-generate-over-half-its-revenue-from-fake-news
  [9]: https://points.datasociety.net/fake-news-is-not-the-problem-f00ec8cdfcb#.577yk6s8a
  [10]: https://theintercept.com/2016/11/26/washington-post-disgracefully-promotes-a-mccarthyite-blacklist-from-a-new-hidden-and-very-shady-group/
  [11]: https://www.kaggle.com/antgoldbloom
  [12]: https://github.com/selfagency/bs-detector",2016-11-25T22:29:09.737Z,mrisdal/fake-news,21.412001,https://www.kaggle.com/mrisdal/fake-news,3,12762,CC0: Public Domain,93,"Ready version: 1, 2016-11-25T22:29:09.737Z",0.852941155,news
115,All the news,"143,000 articles from 15 American publications","NOTE: A larger version of this dataset is now available at [Components][1].

### Context

I wanted to see how articles clustered together if the articles were rendered into document-term matrices---would there be greater affinity among political affiliations, or medium, subject matter, etc. The data was scraped using BeautifulSoup and stored in Sqlite, but I've chopped it up into three separate CSVs here, because the entire Sqlite database came out to about 1.2 gb, beyond Kaggle's max.

The publications include the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. Sampling wasn't quite scientific; I chose publications based on my familiarity of the domain and tried to get a range of political alignments, as well as a mix of print and digital publications. By count, the publications break down accordingly:

The data primarily falls between the years of 2016 and July 2017, although there is a not-insignificant number of articles from 2015, and a possibly insignificant number from before then.

### Content

**articles1.csv** - 50,000 news articles (Articles 1-50,000)

**articles2.csv** - 49,999 news articles (Articles 50,001-100,00)

**articles3.csv** - Articles 100,001+



### Acknowledgements

Thanks mostly go to the maesters of Stack Overflow. 

For each publication, I used archive.org to grab the past year-and-a-half of either home-page headlines or RSS feeds and ran those links through the scraper. That is, the articles are not the product of scraping an entire site, but rather their more prominently placed articles. For example, CNN's articles from 5/6/16 were what appeared on the homepage of CNN.com proper, not everything within the CNN.com domain. Vox's articles from 5/6/16 were everything that appeared in the Vox RSS reader. on 5/6/16, and so on. RSS readers are a breeze to scrape, and so I used them when possible, but not every publication uses them or makes them easy to find.


![enter image description here][2]

It's not entirely even---this was something of a collect-it-all approach, and some sites are more prolific than others, and some have data that maintains integrity after scraping more easily than others.

### Inspiration

Sentiment analysis and topic modeling.


  [1]: https://components.one/datasets/all-the-news-articles-dataset/
  [2]: http://i.imgur.com/QDPtuEv.png",2017-08-20T05:58:47.09Z,snapcrack/all-the-news,265.107114,https://www.kaggle.com/snapcrack/all-the-news,1,9769,Unknown,22,"Ready version: 4, 2017-08-20T05:58:47.09Z",0.7352941,news
116,A Million News Headlines,News headlines published over a period of 15 Years,"### Context

This contains data of news headlines published over a period of 15 years.

Sourced from the reputable Australian news source ABC (Australian Broadcasting Corp.)

Agency Site: http://www.abc.net.au/

### Content

Format: CSV ; Single File

 1. **publish_date**: Date of publishing for the article in yyyyMMdd format
 2. **headline_text**: Text of the headline in Ascii , English , lowercase

Start Date: 2003-02-19 End Date: 2017-12-31

Total Records: **1,103,663**

Citation for usage:

**Rohit Kulkarni** (2017), A Million News Headlines [CSV Data file], doi:10.7910/DVN/SYBGZL, Retrieved from: [this url]

### Inspiration

I look at this news dataset as a summarised historical record of noteworthy events in the globe from early-2003 to end-2017 with a more granular focus on Australia.

This includes the entire corpus of articles published by the ABC website in the given time range. 
With a volume of 200 articles per day and a good focus on international news, we can be fairly certain that every event of significance has been captured here.

Digging into the keywords, one can see all the important episodes shaping the last decade and how they evolved over time.
Ex: financial crisis, iraq war, multiple US elections, ecological disasters, terrorism, famous people, Australian crimes  etc.

### Similar Work
Your kernals can be reused with minimal changes across all these datasets

 - 3M Clickbait Headlines for 6 years: [Examine the Examiner][1]
 - 1.3M Global Headlines from 20K sources over 1 week: [Global News Week][2]
 - 2.9M News Headlines from India from 2001-2017: [Headlines of India][3]
 - 1.4M News Headlines from Ireland from 1996-2017: [Ireland Historical News][4]


  [1]: https://www.kaggle.com/therohk/examine-the-examiner
  [2]: https://www.kaggle.com/therohk/global-news-week
  [3]: https://www.kaggle.com/therohk/india-headlines-news-dataset
  [4]: https://www.kaggle.com/therohk/ireland-historical-news",2019-06-13T18:14:28.073Z,therohk/million-headlines,19.29658,https://www.kaggle.com/therohk/million-headlines,4,11484,CC0: Public Domain,49,"Ready version: 8, 2019-06-13T18:14:28.073Z",0.9411765,news
117,News Aggregator Dataset,Headlines and categories of 400k news stories from 2014,"This dataset contains headlines, URLs, and categories for 422,937 news stories collected by a web aggregator between March 10th, 2014 and August 10th, 2014.

News categories included in this dataset include business; science and technology; entertainment; and health. Different news articles that refer to the same news item (e.g., several articles about recently released employment statistics) are also categorized together.

## Content
The columns included in this dataset are:

- **ID** : the numeric ID of the article
- **TITLE** : the headline of the article
- **URL** : the URL of the article
- **PUBLISHER** : the publisher of the article
- **CATEGORY** : the category of the news item; one of:
-- *b* : business
-- *t* : science and technology
-- *e* : entertainment
-- *m* : health
- **STORY** : alphanumeric ID of the news story that the article discusses
- **HOSTNAME** : hostname where the article was posted
- **TIMESTAMP** : approximate timestamp of the article's publication, given in Unix time (seconds since midnight on Jan 1, 1970)

## Acknowledgments
This dataset comes from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Any publications that use this data should cite the repository as follows:

Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.

This specific dataset can be found in the UCI ML Repository at [this URL](http://archive.ics.uci.edu/ml/datasets/News+Aggregator)

## Inspiration
What kinds of questions can we explore using this dataset? Here are a few possibilities:

- can we predict the category (business, entertainment, etc.) of a news article given only its headline?
- can we predict the specific story that a news article refers to, given only its headline?",2016-10-31T22:22:55.29Z,uciml/news-aggregator-dataset,30.370802,https://www.kaggle.com/uciml/news-aggregator-dataset,2,6757,CC0: Public Domain,54,"Ready version: 1, 2016-10-31T22:22:55.29Z",0.875,news
118,Daily News for Stock Market Prediction,Using 8 years daily news headlines to predict stock market movement,"Actually, I prepare this dataset for students on my Deep Learning and NLP course. 

But I am also very happy to see kagglers play around with it.

Have fun!

**Description:**

There are two channels of data provided in this dataset:

1. News data: I crawled historical news headlines from [Reddit WorldNews Channel][1] (/r/worldnews). They are ranked by reddit users' votes, and only the top 25 headlines are considered for a single date.
(Range: 2008-06-08 to 2016-07-01)

2. Stock data: Dow Jones Industrial Average (DJIA) is used to ""prove the concept"".
(Range: 2008-08-08 to 2016-07-01)

I provided three data files in *.csv* format:

1. **RedditNews.csv**: two columns
The first column is the ""date"", and second column is the ""news headlines"".
All news are ranked from top to bottom based on how *hot* they are.
Hence, there are 25 lines for each date.

2. **DJIA_table.csv**: 
Downloaded directly from [Yahoo Finance][2]: check out the web page for more info.

3. **Combined_News_DJIA.csv**:
To make things easier for my students, I provide this combined dataset with 27 columns.
The first column is ""Date"", the second is ""Label"", and the following ones are news headlines ranging from ""Top1"" to ""Top25"".

**=========================================**

*To my students:*

*I made this a binary classification task. Hence, there are only two labels:*

*""1"" when DJIA Adj Close value rose or stayed as the same;*

*""0"" when DJIA Adj Close value decreased.*

*For task evaluation, please use data from 2008-08-08 to 2014-12-31 as Training Set, and Test Set is then the following two years data (from 2015-01-02 to 2016-07-01). This is roughly a 80%/20% split.*

*And, of course, use AUC as the evaluation metric.*

**=========================================**

**+++++++++++++++++++++++++++++++++++++++++**

*To all kagglers:*

*Please upvote this dataset if you like this idea for market prediction.*

*If you think you coded an amazing trading algorithm,*

*friendly advice* 

*do play safe with your own money :)*

**+++++++++++++++++++++++++++++++++++++++++**

Feel free to contact me if there is any question~ 

And, remember me when you become a millionaire :P

**Note: If you'd like to cite this dataset in your publications, please use:**

`
Sun, J. (2016, August). Daily News for Stock Market Prediction, Version 1. Retrieved [Date You Retrieved This Data] from https://www.kaggle.com/aaron7sun/stocknews.
`

  [1]: https://www.reddit.com/r/worldnews?hl
  [2]: https://finance.yahoo.com/quote/%5EDJI/history?p=%5EDJI",2016-08-25T16:56:51.32Z,aaron7sun/stocknews,6.384909,https://www.kaggle.com/aaron7sun/stocknews,2,23371,CC BY-NC-SA 4.0,306,"Ready version: 1, 2016-08-25T16:56:51.32Z",0.882352948,news
119,Hacker News Posts,Hacker News posts from the past 12 months (including # of votes and comments),"This data set is Hacker News posts from the last 12 months (up to September 26 2016). 

It includes the following columns:

 - title: title of the post (self explanatory)

 - url: the url of the item being linked to

 - num_points: the number of upvotes the post received

 - num_comments: the number of comments the post received

 - author: the name of the account that made the post

 - created_at: the date and time the post was made (the time zone is Eastern Time in the US)

One fun project suggestion is a model to predict the number of votes a post will attract.
 
The scraper is written, so I can keep this up-to-date and add more historical data. I can also scrape the comments. Just make the request in this dataset's forum.

The is a fork of minimaxir's HN scraper (thanks minimaxir):
[https://github.com/minimaxir/get-all-hacker-news-submissions-comments][1]


  [1]: https://github.com/minimaxir/get-all-hacker-news-submissions-comments",2016-09-27T03:14:41.153Z,hacker-news/hacker-news-posts,20.702971,https://www.kaggle.com/hacker-news/hacker-news-posts,2,2016,CC0: Public Domain,39,"Ready version: 1, 2016-09-27T03:14:41.153Z",0.882352948,news
120,News Headlines Dataset For Sarcasm Detection,High quality dataset for the task of Sarcasm Detection,"#Context

Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in these requires the availability of contextual tweets.

To overcome the limitations related to noise in Twitter datasets, this **News Headlines dataset for Sarcasm Detection** is collected from two news website. [*TheOnion*](https://www.theonion.com/) aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). We collect real (and non-sarcastic) news headlines from [*HuffPost*](https://www.huffingtonpost.com/).

This new dataset has following advantages over the existing Twitter datasets:

* Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.

* Furthermore, since the sole purpose of *TheOnion* is to publish sarcastic news, we get high-quality labels with much less noise as compared to Twitter datasets.

* Unlike tweets which are replies to other tweets, the news headlines we obtained are self-contained. This would help us in teasing apart the real sarcastic elements.

# Content
Each record consists of three attributes:

* ```is_sarcastic```: 1 if the record is sarcastic otherwise 0

* ```headline```: the headline of the news article

* ```article_link```: link to the original news article. Useful in collecting supplementary data

# Further Details
General statistics of data, instructions on how to read the data in python, and basic exploratory analysis could be found at [this GitHub repo](https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection). A hybrid NN architecture trained on this dataset can be found at [this GitHub repo](https://github.com/rishabhmisra/Sarcasm-Detection-using-NN).

# Inspiration

Can you identify sarcastic sentences? Can you distinguish between fake news and legitimate news?

# Reading the data
Following code snippet could be used to read the data:

import json

def parse_data(file):

    for l in open(file,'r'):

        yield json.loads(l)

    
data = list(parse_data('./Sarcasm_Headlines_Dataset.json'))

# Citation

Please link to ""https://rishabhmisra.github.io/publications/"" in your report if you're using this dataset.

If you're using this dataset for research purposes, please use the following BibTex for citation:


@dataset{dataset,

author = {Misra, Rishabh},

year = {2018},

month = {06},

pages = {},

title = {News Headlines Dataset For Sarcasm Detection},

doi = {10.13140/RG.2.2.16182.40004}

}

Thanks!

### Other datasets
Please also checkout the following datasets collected by me:

* [News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset)

* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)

* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)",2019-07-03T23:52:57.127Z,rmisra/news-headlines-dataset-for-sarcasm-detection,3.425749,https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection,4,5723,CC0: Public Domain,48,"Ready version: 2, 2019-07-03T23:52:57.127Z",1.0,news
121,News Headlines Of India,18 Years of headlines focusing on India,"### Context

This News Dataset is a persistent historical archive of noteable events in the Indian subcontinent from start-2001 to end-2018, recorded in real-time by the journalists of India. It contains approximately 2.9 million events published by Times of India.

A majority of the data is focusing on Indian local news including national, city level and entertainment.

Agency Website: https://timesofindia.indiatimes.com

The individual events can be explored in detail via the archives section.

### Content

CSV Rows: 2,969,922

1. **publish_date**: Date of the article being published online in yyyyMMdd format

2. **headline_category**: Category of the headline, ascii, dot delimited, lowercase values

3. **headline_text**: Text of the Headline in English, only ascii characters

Start Date: 2001-01-01 End Date: 2018-12-31

See This Kernal for [Overview of Trends and Categories][1] 

### Inspiration



Times Group as a news agency, reaches out a very wide audience across Asia and drawfs every other agency in the quantity of English Articles published per day. 
Due to the heavy daily volume (avg. 650 articles) over multiple years, this data offers a deep insight into Indian society, its priorities, events, issues and talking points and how they have unfolded over time. 

It is possible to chop this dataset into a smaller piece for a more focused analysis, based on one or more facets. 

 - Time Range: Records during 2014 election, 2006 Mumbai Bombings
 - One or more Categories: like Mumbai, Movie Releases, ICC updates, Magazine, Middle East
 - One or more Keywords: like crime or ecology related words; names of political parties, celebrities, corporations.

### Acknowledgements

The headlines are extracted from several GB of raw HTML files using Jsoup, Java and Bash. The entire process takes 11 minutes.

This logic also : chooses the best worded headline for each article (longest one is usually picked) ; clusters about 17k categories to 200 large groups ; removes records where the date is ambiguous (9k cases) ; finally cleans the selected headline via a string 'domestication' function (which I use for any wild text from the internet).

The final categories are as per the latest sitemap.  Around 1.5k rare categories remain and these records (~20k) can be filtered out easily during analysis. The category is unknown for ~200k records.

Similar news datasets exploring other attributes, countries and topics can be seen on my profile.

Citation for usage:

**Rohit Kulkarni** (2017), News Headlines of India 2001-2018 [CSV data file], doi:10.7910/DVN/J7BYRX, Retrieved from: [this url]

  [1]: https://www.kaggle.com/therohk/india-news-publishing-trends-and-cities",2019-04-12T02:46:04.197Z,therohk/india-headlines-news-dataset,71.73913,https://www.kaggle.com/therohk/india-headlines-news-dataset,3,2425,CC0: Public Domain,10,"Ready version: 5, 2019-04-12T02:46:04.197Z",0.7647059,news
122,NEWS SUMMARY,Generating short length descriptions of news articles.,"### Context

I am currently working on summarizing chat context where it helps an agent in understanding previous context quickly. It interests me to apply the deep learning models to existing datasets and how they perform on them. I believe news articles are rich in grammar and vocabulary which allows us to gain greater insights.


### Content

The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. I gathered the summarized news from Inshorts and only scraped the news articles from Hindu, Indian times and Guardian.  Time period ranges from febrauary to august 2017.

### Acknowledgements

I would like to thank the authors of Inshorts for their amazing work

### Inspiration

* Generating short length descriptions(headlines) from text(news articles).
* Summarizing large amount of information which can be represented in compressed space

###Purpose

When I was working on the summarization task I didn't find any open source data-sets to work on, I believe there are people just like me who are working on these tasks and I hope it helps them.

###Contributions

It will be really helpful if anyone found nice insights from this data and can share their work. Thankyou...!!!

For those who are interested here is the link for the github code which includes the scripts for scraping.
https://github.com/sunnysai12345/News_Summary",2019-02-11T09:03:28.783Z,sunnysai12345/news-summary,20.492757,https://www.kaggle.com/sunnysai12345/news-summary,3,2331,GPL 2,3,"Ready version: 2, 2019-02-11T09:03:28.783Z",0.7647059,news
123,News Articles,This dataset include articles from 2015 till date,"# Content

This Dataset is scraped from https://www.thenews.com.pk website. It has news articles from 2015 till date related to business and sports. It Contains the Heading of the particular Article, Its content and its date. The content also contains the place from where the statement or Article was published.

# Importance

This dataset can be used to detect main patterns between writing pattern of different types of articles. One more thing that can be extracted from it is that we could also detect the main locations from where the different types of articles originate.

# Improvements

Some Data Cleaning could still be done specially in the content area of the dataset. One more thing that could be done is that we could extract the locations from the content and make a separated table for it.


# Acknowledgements

I'd like to thanks developer of Selenium Library. That helped a lot in retrieving the data.",2017-04-30T11:02:29.487Z,asad1m9a9h6mood/news-articles,1.916427,https://www.kaggle.com/asad1m9a9h6mood/news-articles,3,1253,CC0: Public Domain,7,"Ready version: 1, 2017-04-30T11:02:29.487Z",0.8235294,news
124,20 Newsgroups,"A collection of ~18,000 newsgroup documents from 20 different newsgroups","### Context

This dataset is a collection newsgroup documents. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.


### Content

There is file (list.csv) that contains a reference to the document_id number and the newsgroup it is associated with.
There are also 20 files that contain all of the documents, one document per newsgroup.

In this dataset, duplicate messages have been removed and the original messages only contain ""From"" and ""Subject"" headers (18828 messages total).

Each new message in the bundled file begins with these four headers:

Newsgroup: alt.newsgroup

Document_id: xxxxxx

From:  Cat

Subject:  Meow Meow Meow

The Newsgroup and Document_id can be referenced against list.csv


Organization
- Each newsgroup file in the bundle represents a single newsgroup
- Each message in a file is the text of some newsgroup document that was posted to that newsgroup.

This is a list of the 20 newsgroups:

- comp.graphics
- comp.os.ms-windows.misc
- comp.sys.ibm.pc.hardware
- comp.sys.mac.hardware
- comp.windows.x	rec.autos
- rec.motorcycles
- rec.sport.baseball
- rec.sport.hockey	sci.crypt
- sci.electronics
- sci.med
- sci.space
- misc.forsale	talk.politics.misc
- talk.politics.guns
- talk.politics.mideast	talk.religion.misc
- alt.atheism
- soc.religion.christian


### Acknowledgements

Ken Lang is credited by the source for collecting this data. The source of the data files is here:  
http://qwone.com/~jason/20Newsgroups/

### Inspiration

- This dataset text can be used to classify text documents",2017-07-26T21:05:38.987Z,crawford/20-newsgroups,28.248814,https://www.kaggle.com/crawford/20-newsgroups,2,2280,Other (specified in description),9,"Ready version: 1, 2017-07-26T21:05:38.987Z",0.8235294,news
125,One Week of Global News Feeds,7 days of tracking 20k news feeds worldwide,"# Context

This dataset is a snapshot of most of the new news content published online over one week. It covers the 7 Day-period of August 24 through August 30 for the years 2017 and 2018. 

Year 2017: 1,398,431 ; Year 2018: 1,912,873

Prepared by **Rohit Kulkarni**

It includes approximately **3.3 million** articles, with **20,000 news sources** and **20+ languages**.

This dataset has just four fields (as per the [column metadata](https://www.kaggle.com/therohk/global-news-week/data)):

 - **publish_time** - earliest known time of the url appearing online in yyyyMMddHHmm format, IST timezone

 - **feed_code** - unique identifier for the publisher or domain

 - **source_url** - url of the article

 - **headline_text** - Headline of the article (UTF8, 20+ possible languages)

See the [""Basic Feed Exploration""](https://www.kaggle.com/therohk/basic-feed-code-exploration) notebook for a quick look at the dataset contents.

# Inspiration

The sources include news feeds, news websites, government agencies, tech journals, company websites, blogs and wikipedia updates. The data has been collected by polling RSS feeds and by crawling other large news aggregators. 

As of 2017, this 7 day slice was selected as there wasn't any downtime or outage during the interval. New news content is produced at this rate by publishers everyday, throughout the year. 

# Acknowledgements

This dataset is free to use with the following citation: 

Rohit Kulkarni (2018), One Week of Global Feeds [News CSV Dataset], doi:10.7910/DVN/ILAT5B, Retrieved from: [this url]

Remodelling from raw IJS news feed: http://newsfeed.ijs.si/

Original paper by M Trampus, B Novak: Internals of An Aggregated Web News Feed

Hosted By: Josef Stefan Institute, Slovenia : http://ailab.ijs.si/people/

Lieve News: http://eventregistry.org/",2019-04-10T15:05:18.423Z,therohk/global-news-week,282.624224,https://www.kaggle.com/therohk/global-news-week,4,1047,CC0: Public Domain,8,"Ready version: 4, 2019-04-10T15:05:18.423Z",0.882352948,news
126,Hacker News Corpus,A subset of all Hacker News articles,"### Context

This dataset contains a randomized sample of roughly one quarter of all stories and comments from Hacker News from its launch in 2006. Hacker News is a social news website focusing on computer science and entrepreneurship. It is run by Paul Graham's investment fund and startup incubator, Y Combinator. In general, content that can be submitted is defined as ""anything that gratifies one's intellectual curiosity"".

### Content

Each story contains a story ID, the author that made the post, when it was written, and the number of points the story received.

Please note that the text field includes profanity. All texts are the authorâ€™s own, do not necessarily reflect the positions of Kaggle or Hacker News, and are presented without endorsement.

### Acknowledgements

This dataset was kindly made publicly available by [Hacker News][1] under [the MIT license][2].

### Inspiration

 - Recent studies have found that many forums tend to be dominated by a
   very small fraction of users. Is this true of Hacker News?

 - Hacker News has received complaints that the site is biased towards Y
   Combinator startups. Do the data support this? 

 - Is the amount of coverage by Hacker News predictive of a startupâ€™s
   success?

### Use this dataset with BigQuery

You can use Kernels to analyze, share, and discuss this data on Kaggle, but if youâ€™re looking for real-time updates and bigger data, check out the data in BigQuery, too: https://cloud.google.com/bigquery/public-data/hacker-news

The BigQuery version of this dataset has roughly four times as many articles.



  [1]: https://github.com/HackerNews/API
  [2]: https://github.com/HackerNews/API/blob/master/LICENSE",2017-06-29T20:16:20.2Z,hacker-news/hacker-news-corpus,667.291612,https://www.kaggle.com/hacker-news/hacker-news-corpus,2,676,Other (specified in description),4,"Ready version: 2, 2017-06-29T20:16:20.2Z",0.8235294,news
127,News of the Brazilian Newspaper,167.053 news of the site Folha de SÃ£o Paulo (Brazilian Newspaper),"### Content

The dataset consists of 167.053 examples and contains Headlines, Url of Article, Complete Article and Category. I gathered the summarized news from Inshorts and only scraped the news articles from Folha de SÃ£o Paulo - http://www.folha.uol.com.br/ (Brazilian Newspaper). Time period ranges is between January 2015 and September 2017.",2019-06-05T03:33:51.58Z,marlesson/news-of-the-site-folhauol,193.086895,https://www.kaggle.com/marlesson/news-of-the-site-folhauol,3,947,CC0: Public Domain,7,"Ready version: 2, 2019-06-05T03:33:51.58Z",0.7058824,news
128,The Examiner - SpamClickBait News Dataset,SiX Years of Crowd Sourced Journalism,"### Context

Presenting a compendium of crowdsourced journalism from the psuedo-news site **The Examiner**. 

This dataset contains the headlines of **3.09 million articles** written by **~21000 authors** over **6 years**. 

While The Examiner was never praised for its quality, it consistently churned out 1000s of articles per day over several years. 

At their height in 2011, The Examiner was ranked highly in google search and had enormous shares on social media.
At one point it was the 10th largest site on mobile and was attracting 20 million unique visitors a month.

As a platform driven towards advert revenue, most of their content was rushed, unsourced and factually sparse. 
It still manages to paint a colourful picture about the trending topics over a long period of time.

Prepared by Rohit Kulkarni

### Content

Format: CSV Rows: 3,089,781

 - **publish_date**: Date when the article was published on the site in yyyyMMdd format
 - **headline_text**: Text of the headline in English in Ascii

Start Date: 2010-01-01   End Date: 2015-21-31

Another copy of the file with headlines tokenised to lowercase ascii only is included. Note that both files were derived using alternate methodologies and might not be in sync.

Similar news datasets exploring other attributes, countries and topics can be accessed via my profile.

### Inspiration

The Examiner had emerged as an early winner in the digital content landscape of the 2000's using catchy headlines. 

It changed many roles over the years, from leftist citizen news to a multiuser blogging platform to a content farm.

With falling views its operations were absorbed by AXS in 2014 and the website was finally shut down in June 2016.

The original portal and content no longer exists: http://www.examiner.com

This is potentially, the last surviving record of its existence.",2019-06-22T12:03:21.843Z,therohk/examine-the-examiner,148.217226,https://www.kaggle.com/therohk/examine-the-examiner,3,823,CC0: Public Domain,4,"Ready version: 6, 2019-06-22T12:03:21.843Z",0.8235294,news
129,BBC News Summary,Extractive Summarization of BBC News Articles ,"### Context

Text summarization is a way to condense the large amount of information into a concise form by the process of selection of important information and discarding unimportant and redundant information. With the amount of textual information present in the world wide web the area of text summarization is becoming very important. The extractive summarization is the one where the exact sentences present in the document are used as summaries. The extractive summarization is simpler and is the general practice among the automatic text summarization researchers at the present time. Extractive summarization process involves giving scores to sentences using some method and then using the sentences that achieve highest scores as summaries. As the exact sentence present in the document is used the semantic factor can be ignored which results in generation of less calculation intensive summarization procedure. This kind of summary is generally completely unsupervised and language independent too. Although this kind of summary does its job in conveying the essential information it may not be necessarily smooth or fluent. Sometimes there can be almost no connection between adjacent sentences in the summary resulting in the text lacking in readability. 


### Content

This dataset for extractive text summarization has four hundred and seventeen political news articles of BBC from 2004 to 2005 in the News Articles folder. For each articles, five summaries are provided in the Summaries folder. The first clause of the text of articles is the respective title. 


### Acknowledgements

This dataset was created using a dataset used for data categorization that onsists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005 used in the paper of D. Greene and P. Cunningham. ""Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering"", Proc. ICML 2006; whose all rights, including copyright, in the content of the original articles are owned by the BBC. More at http://mlg.ucd.ie/datasets/bbc.html
",2018-05-06T11:08:19.42Z,pariza/bbc-news-summary,3.928416,https://www.kaggle.com/pariza/bbc-news-summary,2,1481,CC0: Public Domain,2,"Ready version: 2, 2018-05-06T11:08:19.42Z",0.75,news
130,Yet Another Chinese News Dataset,"With Article Titles, Descriptions, Cover Images, and Links.","A collections of news articles in Traditional and Simplified Chinese. It includes some Internet news outlets that are NOT Chinese state media (they deserve a separate dataset). 

Complete coverage is not guaranteed. Therefore this dataset is not suitable for analyzing event coverage. It is meant for using as a corpus for NLP algorithms.

## Data Collection Process

1. The links to the news articles were collected from the RSS feeds or the Twitter accounts of the news outlets. 
2. Download and parse the web pages. Then the meta tags were used to extract the title, description/summary, and cover image of each article. (These are the stuffs that are used in the Twitter and Facebook summary cards.)

Note: Only minimal text cleaning has been performed on the meta tags.

### Data Fields

1. title: Article title from `og:title` or `twitter:title` meta tag.
2. desc: Article summary from `twitter:description` or `og:description` meta tag.
3. image: URL to the cover image from `twitter:image` or `og:image` meta tag.
4. url: URL of the article.
5. source: The code of the news outlet.
6. date: The publish date of the article on Twitter or in RSS feeds. Format: YYYYMMDD

This dataset does not provide full texts of the article. You'll need to scrape it yourself using the links provided.
",2019-07-11T17:01:17.377Z,ceshine/yet-another-chinese-news-dataset,24.983959,https://www.kaggle.com/ceshine/yet-another-chinese-news-dataset,3,137,CC BY-SA 4.0,4,"Ready version: 7, 2019-07-11T17:01:17.377Z",1.0,news
131,Old Newspapers,A cleaned subset of HC Corpora newspapers,"### Context

The [HC Corpora](https://web.archive.org/web/20161021044006/http://corpora.heliohost.org/) was a great resource that contains natural language text from various newspapers, social media posts and blog pages in multiple languages. This is a cleaned version of the raw data from newspaper subset of the HC corpus. 

Originally, this subset was created for a [language identification task for similar languages](http://corporavm.uni-koeln.de/vardial/sharedtask.html)

### Content

The columns of each row in the `.tsv` file are:

 - **Langauge**: Language of the text.
 - **Source**: Newspaper from which the text is from.
 - **Date**: Date of the article that contains the text.
 - **Text**: Sentence/paragraph from the newspaper

The corpus contains 16,806,041 sentences/paragraphs in 67 languages:

 - Afrikaans
 - Albanian
 - Amharic
 - Arabic
 - Armenian
 - Azerbaijan
 - Bengali
 - Bosnian
 - Catalan
 - Chinese (Simplified)
 - Chinese (Traditional)
 - Croatian
 - Welsh
 - Czech
 - German
 - Danish
 - Danish
 - English
 - Spanish
 - Spanish (South America)
 - Finnish
 - French
 - Georgian
 - Galician
 - Greek
 - Hebrew
 - Hindi
 - Hungarian
 - Icelandic
 - Indonesian
 - Italian
 - Japanese
 - Khmer
 - Kannada
 - Korean
 - Kazakh
 - Lithuanian
 - Latvian
 - Macedonian
 - Malayalam
 - Mongolian
 - Malay
 - Nepali
 - Dutch
 - Norwegian (Bokmal)
 - Punjabi
 - Farsi
 - Polish
 - Portuguese (Brazil)
 - Portuguese (EU)
 - Romanian
 - Russian
 - Serbian
 - Sinhalese
 - Slovak
 - Slovenian
 - Swahili
 - Swedish
 - Tamil
 - Telugu
 - Tagalog
 - Thai
 - Turkish
 - Ukranian
 - Urdu
 - Uzbek
 - Vietnamese

Languages in HC Corpora but not in this (yet):

 - Estonian
 - Greenlandic
 - Gujarati 

### Acknowledge

All credits goes to Hans Christensen, the creator of HC Corpora.

Dataset image is from [Philip Strong](https://unsplash.com/search/photos/newspaper?photo=gZaj16Ztu2Y).


### Inspire

Use this dataset to:

 - create a language identifier / detector
 - exploratory corpus linguistics (Itâ€™s one capstone project from [Courseraâ€™s data science specialization](https://www.coursera.org/learn/data-science-project) )",2017-11-16T04:53:55.98Z,alvations/old-newspapers,2196.786581,https://www.kaggle.com/alvations/old-newspapers,4,847,CC0: Public Domain,3,"Ready version: 6, 2017-11-16T04:53:55.98Z",0.75,news
132,Getting Real about Fake News,Text & metadata from fake & biased news sources around the web,"The latest hot topic in the news is fake news and many are wondering what data scientists can do to detect it and stymie its viral spread. This dataset is only a first step in understanding and tackling this problem. It contains text and metadata scraped from 244 websites tagged as ""bullshit"" by the [BS Detector][2] Chrome Extension by [Daniel Sieradski][3]. 

**Warning**: I did not modify the list of news sources from the BS Detector so as not to introduce my (useless) layer of bias; I'm not an authority on fake news. There may be sources whose inclusion you disagree with. It's up to you to decide how to work with the data and how you might contribute to ""improving it"". The labels of ""bs"" and ""junksci"", etc. do not constitute capital ""t"" Truth. If there are other sources you would like to include, start a discussion. If there are sources you believe should not be included, start a discussion or write a kernel analyzing the data. Or take the data and do something else productive with it. Kaggle's choice to host this dataset is not meant to express any particular political affiliation or intent.

## Contents

The dataset contains text and metadata from 244 websites and represents 12,999 posts in total from the past 30 days. The data was pulled using the [webhose.io][4] API; because it's coming from their crawler, not all websites identified by the BS Detector are present in this dataset. Each website was labeled according to the BS Detector as documented here. Data sources that were missing a label were simply assigned a label of ""bs"". There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.

## Fake news in the news

For inspiration, I've included some (presumably non-fake) recent stories covering fake news in the news. This is a sensitive, nuanced topic and if there are other resources you'd like to see included here, please leave a suggestion. From defining fake, biased, and misleading news in the first place to deciding how to take action (a blacklist is not a good answer), there's a lot of information to consider beyond what can be neatly arranged in a CSV file.

* [How Fake News Spreads (NYT)][6]

* [We Tracked Down A Fake-News Creator In The Suburbs. Here's What We Learned (NPR)][7]

* [Does Facebook Generate Over Half of its Revenue from Fake News? (Forbes)][8]

* [Fake News is Not the Only Problem (Points - Medium)][9]

* [Washington Post Disgracefully Promotes a McCarthyite Blacklist From a New, Hidden, and Very Shady Group (The Intercept)][10]

## Improvements

If you have suggestions for improvements or would like to contribute, please let me know. The most obvious extensions are to include data from ""real"" news sites and to address the bias in the current list. I'd be happy to include any contributions in future versions of the dataset.

## Acknowledgements

Thanks to [Anthony][11] for pointing me to [Daniel Sieradski's BS Detector][12]. Thank you to Daniel Nouri for encouraging me to add a disclaimer to the dataset's page.


  [2]: https://github.com/selfagency/bs-detector
  [3]: https://github.com/selfagency
  [4]: https://webhose.io/api
  [5]: https://github.com/selfagency/bs-detector/blob/master/chrome/data/data.json
  [6]: http://www.nytimes.com/2016/11/20/business/media/how-fake-news-spreads.html
  [7]: http://www.npr.org/sections/alltechconsidered/2016/11/23/503146770/npr-finds-the-head-of-a-covert-fake-news-operation-in-the-suburbs
  [8]: http://www.forbes.com/forbes/welcome/?toURL=http://www.forbes.com/sites/petercohan/2016/11/25/does-facebook-generate-over-half-its-revenue-from-fake-news
  [9]: https://points.datasociety.net/fake-news-is-not-the-problem-f00ec8cdfcb#.577yk6s8a
  [10]: https://theintercept.com/2016/11/26/washington-post-disgracefully-promotes-a-mccarthyite-blacklist-from-a-new-hidden-and-very-shady-group/
  [11]: https://www.kaggle.com/antgoldbloom
  [12]: https://github.com/selfagency/bs-detector",2016-11-25T22:29:09.737Z,mrisdal/fake-news,21.412001,https://www.kaggle.com/mrisdal/fake-news,3,12762,CC0: Public Domain,93,"Ready version: 1, 2016-11-25T22:29:09.737Z",0.852941155,fake news
133,Fake News detection,,,2017-12-07T20:39:58Z,jruvika/fake-news-detection,5.123582,https://www.kaggle.com/jruvika/fake-news-detection,0,1460,"Database: Open Database, Contents: Â© Original Authors",6,"Ready version: 1, 2017-12-07T20:39:58Z",0.294117659,fake news
134,FakeNewsNet,"Fake News, MisInformation, Data Mining","##FakeNewsNet
This is a repository for an ongoing data collection project for fake news research at ASU. We describe and compare FakeNewsNet with other existing datasets in [Fake News Detection on Social Media: A Data Mining Perspective][1]. We also perform a detail analysis of FakeNewsNet dataset, and build a fake news detection model on this dataset in [Exploiting Tri-Relationship for Fake News Detection][2]

JSON version of this dataset is available in github [here][3].
The new version of this dataset described in [FakeNewNet][4] will be published soon or you can email authors for more info.

## News Content
It includes all the fake news articles, with the news content attributes as follows:

1. _source_: It indicates the author or publisher of the news article
2. _headline_: It refers to the short text that aims to catch the attention of readers and relates well to the major of the news topic.
3. _body_text_: It elaborates the details of news story. Usually there is a major claim which shaped the angle of the publisher and is specifically highlighted and elaborated upon.
4. _image_video_: It is an important part of body content of news article, which provides visual cues to frame the story.

## Social Context
It includes the social engagements of fake news articles from Twitter. We extract profiles, posts and social network information for all relevant users. 

1. _user_profile_: It includes a set of profile fields that describe the users' basic information
2. _user_content_: It collects the users' recent posts on Twitter
3. _user_followers_: It includes the follower list of the relevant users
4. _user_followees_: It includes list of users that are followed by relevant users


###References
If you use this dataset, please cite the following papers:

`@article{shu2017fake,
  title={Fake News Detection on Social Media: A Data Mining Perspective},
  author={Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={19},
  number={1},
  pages={22--36},
  year={2017},
  publisher={ACM}
}`

`@article{shu2017exploiting,
  title={Exploiting Tri-Relationship for Fake News Detection},
  author={Shu, Kai and Wang, Suhang and Liu, Huan},
  journal={arXiv preprint arXiv:1712.07709},
  year={2017}
}`

`@article{shu2018fakenewsnet,
  title={FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic Information for Studying Fake News on Social Media},
  author={Shu, Kai and  Mahudeswaran, Deepak and Wang, Suhang and Lee, Dongwon and Liu, Huan},
  journal={arXiv preprint arXiv:1809.01286},
  year={2018}
}`


  [1]: https://arxiv.org/abs/1708.01967
  [2]: http://arxiv.org/abs/1712.07709
  [3]: https://github.com/KaiDMML/FakeNewsNet
  [4]: https://arxiv.org/abs/1809.01286",2018-11-02T19:08:58.527Z,mdepak/fakenewsnet,17.009927,https://www.kaggle.com/mdepak/fakenewsnet,5,553,CC BY-NC-SA 4.0,4,"Ready version: 1, 2018-11-02T19:08:58.527Z",0.7647059,fake news
135,Fake-News-Dataset,Two fake news datasets covering seven different news domains. ,"## Introduction
This describes two fake news datasets covering seven different news domains. One of the datasets is collected by combining manual and crowdsourced annotation approaches (FakeNewsAMT), while the second is collected directly from the web (Celebrity).

## Data collection
The FakeNewsDatabase dataset contains news in six different domains: technology, education, business, sports, politics, and entertainment. The legitimate news included in the dataset were collected from a variety of mainstream news websites predominantly in the US such as the ABCNews, CNN, USAToday, NewYorkTimes, FoxNews, Bloomberg, and CNET among others. The fake news included in this dataset consist of fake versions of the legitimate news in the dataset, written using Mechanical Turk. More details on the data collection are provided in section 3 of the paper. 

The Celebrity dataset contain news about celebrities (actors, singers, socialites, and politicians). The legitimate news in the dataset were obtained from entertainment, fashion and style news sections in mainstream news websites and from entertainment magazines websites. The fake news were obtained from gossip websites such as Entertainment Weekly, People Magazine, RadarOnline, and other tabloid and entertainment-oriented publications. The news articles were collected in pairs, with one article being legitimate and the other fake (rumors and false reports). The articles were manually verified using gossip-checking sites such as ""GossipCop.com"", and also cross-referenced with information from other entertainment news sources on the web.

The data directory contains two fake news datasets:

- Celebrity
The fake and legitimate news are provided in two separate folders. The fake and legitimate labels are also provided as part of the filename.

- FakeNewsAMT
The fake and legitimate news are provided in two separate folders. Each folder contains 40 news from six different domains: technology, education, business, sports, politics, and entertainment. The file names indicate the news domain: business (biz), education (edu), entertainment (entmt), politics (polit), sports (sports) and technology (tech). The fake and legitimate labels are also provided as part of the filename.

## Dataset citation :
@article{Perez-Rosas18Automatic,
author = {Ver\â€™{o}nica P\'{e}rez-Rosas, Bennett Kleinberg, Alexandra Lefevre, Rada Mihalcea},
title = {Automatic Detection of Fake News},
journal = {International Conference on Computational Linguistics (COLING)},
year = {2018}
}",2019-04-19T08:39:12.863Z,sumanthvrao/fakenewsdataset,2.084538,https://www.kaggle.com/sumanthvrao/fakenewsdataset,1,165,Unknown,2,"Ready version: 3, 2019-04-19T08:39:12.863Z",0.5625,fake news
136,News Headlines Dataset For Sarcasm Detection,High quality dataset for the task of Sarcasm Detection,"#Context

Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in these requires the availability of contextual tweets.

To overcome the limitations related to noise in Twitter datasets, this **News Headlines dataset for Sarcasm Detection** is collected from two news website. [*TheOnion*](https://www.theonion.com/) aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). We collect real (and non-sarcastic) news headlines from [*HuffPost*](https://www.huffingtonpost.com/).

This new dataset has following advantages over the existing Twitter datasets:

* Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.

* Furthermore, since the sole purpose of *TheOnion* is to publish sarcastic news, we get high-quality labels with much less noise as compared to Twitter datasets.

* Unlike tweets which are replies to other tweets, the news headlines we obtained are self-contained. This would help us in teasing apart the real sarcastic elements.

# Content
Each record consists of three attributes:

* ```is_sarcastic```: 1 if the record is sarcastic otherwise 0

* ```headline```: the headline of the news article

* ```article_link```: link to the original news article. Useful in collecting supplementary data

# Further Details
General statistics of data, instructions on how to read the data in python, and basic exploratory analysis could be found at [this GitHub repo](https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection). A hybrid NN architecture trained on this dataset can be found at [this GitHub repo](https://github.com/rishabhmisra/Sarcasm-Detection-using-NN).

# Inspiration

Can you identify sarcastic sentences? Can you distinguish between fake news and legitimate news?

# Reading the data
Following code snippet could be used to read the data:

import json

def parse_data(file):

    for l in open(file,'r'):

        yield json.loads(l)

    
data = list(parse_data('./Sarcasm_Headlines_Dataset.json'))

# Citation

Please link to ""https://rishabhmisra.github.io/publications/"" in your report if you're using this dataset.

If you're using this dataset for research purposes, please use the following BibTex for citation:


@dataset{dataset,

author = {Misra, Rishabh},

year = {2018},

month = {06},

pages = {},

title = {News Headlines Dataset For Sarcasm Detection},

doi = {10.13140/RG.2.2.16182.40004}

}

Thanks!

### Other datasets
Please also checkout the following datasets collected by me:

* [News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset)

* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)

* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)",2019-07-03T23:52:57.127Z,rmisra/news-headlines-dataset-for-sarcasm-detection,3.425749,https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection,4,5723,CC0: Public Domain,48,"Ready version: 2, 2019-07-03T23:52:57.127Z",1.0,fake news
137,Fake News Sample,,,2018-09-24T20:12:03.817Z,pontes/fake-news-sample,519.121643,https://www.kaggle.com/pontes/fake-news-sample,0,231,Unknown,1,"Ready version: 1, 2018-09-24T20:12:03.817Z",0.117647059,fake news
138,WSDM - Fake News Classification,Identify the fake news.,"# Background

WSDM (pronounced ""wisdom"") is one of the premier conferences on web-inspired research involving search and data mining. [The 12th ACM International WSDM Conference][1] will take place in Melbourne, Australia during Feb. 11-15, 2019. 

This task is organized by ByteDance, the Platinum Level Sponsor of the conference. ByteDance is a global Internet technology company started from China. Our goal is to build a global content platform that enable people to enjoy various content in various forms. We inform, entertain, and inspire people across language, culture and geography.

One of the challenges which we are facing is to combat different types of fake news. Fake news here refers to all forms of false, inaccurate or misleading information, which now poses a big threat to human civilization.

At Bytedance, we have created a large-scale database to store existing fake news articles. Any new article must go through a test on the truthfulness of content before being published. We conduct matching between the new article and the articles in the database. Articles identified as containing fake news will be withdrawn after human verification. The accuracy and efficiency of the process, therefore, becomes crucial for us to make the platform safe, reliable, and healthy.

# About This Dataset

This dataset is released as the competition dataset of [Task: Fake News Classification][1] with the following task:

Given the title of a fake news article A and the title of a coming news article B,  participants are asked to classify B into one of the three categories.

- agreed: B talks about the same fake news as A
- disagreed: B refutes the fake news in A
- unrelated: B is unrelated to A

## File 
- **train.csv** - training data contains 320,767 news pairs in both Chinese and English. This file provides the only data you can use to finish the task. Using external data is not allowed.
- **test.csv** - testing data contains 80,126  news pairs in both Chinese and English. The approximately 25% of the testing data is set to be public and is used to calculate your accuracy shown on the leading board. The remaining 75% private data is used to calculate your final result of the competition.
- **sample_submission.csv** - sample answer to the testing data.


## Data fields
- **id** - the id of each news pair.
- **tid1** - the id of fake news title 1.
- **tid2** - the id of news title 2.
- **title1_zh** - the fake news title 1 in Chinese.
- **title2_zh** - the news title 2 in Chinese.
- **title1_en** - the fake news title 1 in English.
- **title2_en** - the news title 2 in English.
- **label** - indicates the relation between the news pair: agreed/disagreed/unrelated.

The English titles are machine translated from the related Chinese titles. This may help participants from all background to get better understanding of the datasets. Participants are highly recommended to use the Chinese version titles to finish the task.  

# Evaluation Metrics

We use **Weighted Categorization Accuracy** to evaluate your performance. Weighted categorization accuracy can be generally defined as:

$$ WeightedAccuracy(y, \hat{y}, \omega) = \frac{1}{n} \displaystyle{\sum_{i=1}^{n}} \frac{\omega_i(y_i=\hat{y}_i)}{\sum \omega_i} $$

where \\(y\\) are ground truths, \\(\hat{y}\\) are the predicted results, and \\(\omega_i\\) is the weight associated with the \\(i\\)th item in the dataset.

In our test set, we assign each testing item a weight according to its category. The weights of the three categories, agreed, disagreed and unrelated are \\(\frac{1}{15}\\), \\(\frac{1}{5}\\), \\(\frac{1}{16}\\), respectively. We set the weights in consideration of the imbalance of the data distribution to minimize the bias to your performance caused by the majority class (unrelated pairs accounts for approximately 70% of the dataset). 

  [1]: https://www.kaggle.com/c/fake-news-pair-classification-challenge",2019-04-02T06:13:39.013Z,wsdmcup/wsdm-fake-news-classification,36.152251,https://www.kaggle.com/wsdmcup/wsdm-fake-news-classification,1,63,Unknown,1,"Ready version: 1, 2019-04-02T06:13:39.013Z",0.7058824,fake news
139,Fake News Data,A collection of fake news (headlines) datasets,"## Datasets

Data stored in this dataset comes from the following sources:

* [Fake News Net](https://github.com/KaiDMML/FakeNewsNet): A dataset containing fake and real news headlines (alongside urls from which the article content can be found). The two files stored here are `fnn_politics_fake` and `fnn_politices_real`. [1], [2], [3].

* [Fake News Dataset](http://web.eecs.umich.edu/~mihalcea/downloads.html#FakeNews): A dataset containing fake and real article headlines and blurbs across multiple domains (like business and education). The data is stored in `fnd_news_fake` and `fnd_news_real`. [4]

* [A Million Headlines](https://www.kaggle.com/therohk/million-headlines): A Kaggle dataset of a million headlines, as scraped from ABC News (Australian). [5]

* [All the News](https://www.kaggle.com/snapcrack/all-the-news): A Kaggle dataset containing news articles (with headlines) as scraped from 15 American news organizations. [6]

* [Reuters 2017](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/XDB74W/CVLKJH&version=2.0): A dataset containing articles from Reuters. [7]

[1] Kai Shu, Suhang Wang, and Huan Liu. ""Exploiting Tri-Relationship for
Fake News Detection"". In: arXiv preprint arXiv:1712.07709 (2017).

[2] Kai Shu et al. ""Fake News Detection on Social Media: A Data Mining
Perspective"". In: ACM SIGKDD Explorations Newsletter 19.1 (2017),
pp. 22-36.

[3] Kai Shu et al. ""FakeNewsNet: A Data Repository with News Content,
Social Context and Dynamic Information for Studying Fake News on
Social Media"". In: arXiv preprint arXiv:1809.01286 (2018).

[4] Veronica Perez-Rosas et al. ""Automatic Detection of Fake News"". In:
CoRR abs/1708.07104 (2017). arXiv: 1708.07104. url: http://arxiv.org/abs/1708.07104.

[5] Rohit Kulkarni (2017), A Million News Headlines, doi:10.7910/DVN/SYBGZL

[6] Andrew Thompson, ""All the News - Components"", https://components.one/datasets/all-the-news-articles-dataset/

[7] Rohit Kulkarni. The Historical Reuters News-Wire. Version DRAFT VERSION. 2018. doi: 10.7910/DVN/XDB74W. url: https://doi.org/10.7910/DVN/XDB74W.",2019-07-16T22:34:49.547Z,antmarakis/fake-news-data,38.477999,https://www.kaggle.com/antmarakis/fake-news-data,2,33,Unknown,6,"Ready version: 4, 2019-07-16T22:34:49.547Z",0.7647059,fake news
140,Fake News Detection Dataset,Detection of Fake News,,2019-01-21T09:47:48.09Z,ksaivenketpatro/fake-news-detection-dataset,0.435844,https://www.kaggle.com/ksaivenketpatro/fake-news-detection-dataset,0,178,Unknown,1,"Ready version: 1, 2019-01-21T09:47:48.09Z",0.1764706,fake news
141,Fake news,For Fake news detection using Machine Learning,,2019-05-20T05:02:15.39Z,mohit28rawat/fake-news,48.165856,https://www.kaggle.com/mohit28rawat/fake-news,0,27,Unknown,2,"Ready version: 1, 2019-05-20T05:02:15.39Z",0.235294119,fake news
142,Russian Troll Tweets,"200,000 malicious-account tweets captured by NBC","### Context

As part of the House Intelligence Committee investigation into how Russia may have influenced the 2016 US Election, Twitter released the screen names of almost 3000 Twitter accounts believed to be connected to Russiaâ€™s Internet Research Agency, a company known for operating social media troll accounts. Twitter immediately suspended these accounts, deleting their data from Twitter.com and the Twitter API. A team at NBC News including Ben Popken and EJ Fox was able to reconstruct a dataset consisting of a subset of the deleted data for their investigation and were able to show how these troll accounts went on attack during key election moments. This dataset is the body of this open-sourced reconstruction.

For more background, read the NBC news article publicizing the release: [""Twitter deleted 200,000 Russian troll tweets. Read them here.""](https://www.nbcnews.com/tech/social-media/now-available-more-200-000-deleted-russian-troll-tweets-n844731)

### Content

This dataset contains two CSV files. `tweets.csv` includes details on individual tweets, while `users.csv` includes details on individual accounts.

To recreate a link to an individual tweet found in the dataset, replace `user_key` in `https://twitter.com/user_key/status/tweet_id` with the screen-name from the `user_key` field and `tweet_id` with the number in the `tweet_id` field.

Following the links will lead to a suspended page on Twitter. But some copies of the tweets as they originally appeared, including images, can be found by entering the links on web caches like `archive.org` and `archive.is`.

### Acknowledgements

If you publish using the data, please credit NBC News and include a link to this page. Send questions to `ben.popken@nbcuni.com`.

### Inspiration

What are the characteristics of the fake tweets? Are they distinguishable from real ones? ",2018-02-15T00:49:04.63Z,vikasg/russian-troll-tweets,21.99381,https://www.kaggle.com/vikasg/russian-troll-tweets,5,2291,CC0: Public Domain,8,"Ready version: 2, 2018-02-15T00:49:04.63Z",0.7352941,fake news
143,Snopes_fake_legit_news,Articles taken by Snopes.com,"### Context

I did this in order to share the project i'm working on.


### Content

The dataset are created in 2 parts: 1 fold of articles of fake news and 1 fold of articles of legit news; each one was scraped from Snopes.  


### Acknowledgements

BeautifulSoup <3

### Inspiration

How can i clean the data, especially the one from the fake news collection?",2017-10-24T13:38:46.13Z,ciotolaaaa/snopes-fake-legit-news,2.065057,https://www.kaggle.com/ciotolaaaa/snopes-fake-legit-news,0,135,CC0: Public Domain,0,"Ready version: 1, 2017-10-24T13:38:46.13Z",0.5,fake news
144,Not Fake News,,,2017-09-05T20:31:14.877Z,mrisdal/not-fake-news,0.001236,https://www.kaggle.com/mrisdal/not-fake-news,0,58,CC0: Public Domain,0,"Ready version: 1, 2017-09-05T20:31:14.877Z",0.235294119,fake news
145,Who starts and who debunks rumors,Webpages cited by rumor trackers,"# Context 

[Emergent.info](http://www.emergent.info/) was a major rumor tracker, created by veteran journalist [Craig Silverman](https://twitter.com/CraigSilverman). It has been defunct for a while, but its well-structured format and well-documented content provides an opportunity for analyzing rumors on the web. 

[Snopes.com](http://www.snopes.com/) is one of the oldest rumors trackers on the web. Originally launched by Barbara and David Mikkelson, it is now run by a team of editors who investigate urban legends, myths, viral rumors and fake news. The investigators try to provide a detailed explanation for why they have chosen to confirm or debunk a rumor, often citing several web pages and other external sources. 

[Politifact.com](http://www.politifact.com/) is a fact-checker that is focused on statements made by politicians and claims circulated by political campaigns, blogs and similar websites. Politifact's labels range from ""true,"" to ""pants on fire!""  

---

# Content

This dataset consists of three files. One file is a collection of all webpages cited in Emergent.info, and the second is a collection of webpages cited in Snopes.com, and the third is a similar collection from Politifact.com. The webpages were often cited because they had started a rumor, shared a rumor, or debunked a rumor. 

###Emergent.info
Emergent.info often provides a clean timeline of the rumor's propagation on the web, and identifies which page was for the rumor, which page was against it, and which page was simply observing it. Please refer to the image below to learn more about the fields in this dataset.

![The image displays a sample post from Emergent.info and highlights the corresponding fields in emergent.csv.][1]

###Snopes.com
The structure of posts on **Snopes.com** is not as well-defined. Please refer to the image below to learn more about the fields in the Snopes dataset.

![This image displays a sample post from Snopes.com and highlights the corresponding fields in snopes.csv.][2]

###Politifact.com
Similar to Emergent.info, Politifact.com follows a well-structured format in reporting and documenting rumors. There is a sidebar on the right side of each page that lists all of the sources cited within the page. The top link is the likeliest to be the original source of the rumor. For this link, page_is_first_citation is set to true. 

![This image displays a sample post from Politifact.com and highlights the corresponding fields in politifact.csv.][3]

---

# Inspiration

I created this dataset in order to study domains that frequently start, propagate, or debunk rumors. By studying these domains and people who follow them, I hope to gain some insight into the dynamics of rumor propagation on the web, as well as social media. 

---

# Notes/Disclaimer

When using the Snopes dataset, please keep the following in mind: 

* In addition to debunking rumors, Snopes.com occasionally reports news and other types of content. This collection only includes data from ""[Fact Check](http://www.snopes.com/category/facts)"" posts on Snopes.
 
* Snopes.com was launched years ago. Some of the older posts on the website do not follow the current format of the site, therefore some of the fields might be missing.

* Snopes.com used to use a service named ""[DoNotLink.com](https://twitter.com/donotlink?lang=en)"" for citation purposes. That service is no longer active and as a result some of the links are missing from older posts on Snopes. 

* In addition, some of the shortened links would time-out prior to resolution, in which case they would not be added to the dataset. 

* Occasionally, a website that has been cited has not maliciously started a rumor. For instance, Andy Borowitz is a humorist who writes for *The New Yorker*. His satirical column is sometimes mistaken for real news; as a result, *The New Yorker* may be cited as a source of fake news on [Snopes.com](http://www.snopes.com/trump-blasts-media-for-reporting-things-he-says/). This does not mean that *The New Yorker* is a fake news website.

When using the Politifact dataset, please keep the following in mind:

* The data included in this dataset are collected from the ""[truth-o-meter](http://www.politifact.com/punditfact/statements/)"" page of Politifact.com.

* Politifact often fact-checks statements made by politicians. Since this dataset is focused on websites, I have ignored all the posts in which the rumor was attributed to a person, a political party, a campaign, or an organization. Instead, I have only included rumors attributed explicitly to websites or blogs. 

---

# Useful Tips for Using the Snopes collection

As opposed to the Emergent collection where each page is flagged with whether it was for or against a rumor, no such information is available for the Snopes dataset. To avoid manually labeling the data, you may use the following heuristics to identify which page started a rumor:

* Webpages that are cited in the ""Examples"" section of a post are often ""observing"" the rumor, i.e. they have not started it, but they are repeating it. In the snopes.csv file, these webpages have been flagged as ""page_is_example.""

* Webpages that are cited in the ""Featured Image"" section of a post are often not related to the rumor. The editors on Snopes have simply extracted an image from those pages to embed in their posts. In the snopes.csv file, these webpages have been flagged as ""page_is_image_credit.""

* Webpages that are cited through a secondary service (such as [archive.is](http://archive.is/)) are likelier to be rumor-propagators. Editors do not link to them directly so that a record of their page is available, even if it is later deleted.

* If neither of these hints help, very often (but not always) the first link cited on the page (for which ""page_is_example"" and ""page_is_image_credit"" are false) is the link to a page that started the rumor. This link is identified by the ""page_is_first_citation"" field. Pages for which both ""page_is_first_citation"" and ""page_is_archived"" are true are very likely to be rumor propagators. 

* To identify satirical websites that are mistaken for real news, it's useful to inspect the way they are cited on Snopes. To demonstrate that a website contains satire or humor, Snopes writers often cite the ""about us"" page of the site. Therefore it's useful to  see which domains often contain a URI to their ""about"" page (e.g. ""http://politicops.com/about-us/"").

  [1]: http://imgur.com/JZPExar.png
  [2]: http://i.imgur.com/jFT6Vdb.png
  [3]: http://i.imgur.com/Z83JP7c.png",2017-03-27T15:02:32.653Z,arminehn/rumor-citation,1.455017,https://www.kaggle.com/arminehn/rumor-citation,2,858,CC0: Public Domain,10,"Ready version: 3, 2017-03-27T15:02:32.653Z",0.882352948,fake news
146,WSDM - Fake News Classification,,,2018-11-24T02:31:59.66Z,xuyinjie/wsdm-fake-news-classification,36.090816,https://www.kaggle.com/xuyinjie/wsdm-fake-news-classification,0,66,Unknown,1,"Ready version: 1, 2018-11-24T02:31:59.66Z",0.125,fake news
147,Fake News Detection Dataset,,,2019-03-23T19:27:27.28Z,munagazzai/fake-news-detection-dataset,0.040272,https://www.kaggle.com/munagazzai/fake-news-detection-dataset,1,23,Unknown,1,"Ready version: 1, 2019-03-23T19:27:27.28Z",0.375,fake news
148,Fact-Checking Facebook Politics Pages,Hyperpartisan Facebook pages and misleading information during the 2016 election,"### Context

During the 2016 US presidential election, the phrase â€œfake newsâ€ found its way to the forefront in news articles, tweets, and fiery online debates the world over after misleading and untrue stories proliferated rapidly. [BuzzFeed News][1] analyzed over 1,000 stories from hyperpartisan political Facebook pages selected from the right, left, and mainstream media to determine the nature and popularity of false or misleading information they shared.
 
### Content

This dataset supports the original story [â€œHyperpartisan Facebook Pages Are Publishing False And Misleading Information At An Alarming Rateâ€][2] published October 20th, 2016. Here are more details on the methodology used for collecting and labeling the dataset (reproduced from the story):
  
**More on Our Methodology and Data Limitations**

â€œEach of our raters was given a rotating selection of pages from each category on different days. In some cases, we found that pages would repost the same link or video within 24 hours, which caused Facebook to assign it the same URL. When this occurred, we did not log or rate the repeat post and instead kept the original date and rating. Each rater was given the same guide for how to review posts:
 
* â€œ*Mostly True*: The post and any related link or image are based on factual information and portray it accurately. This lets them interpret the event/info in their own way, so long as they do not misrepresent events, numbers, quotes, reactions, etc., or make information up. This rating does not allow for unsupported speculation or claims.
 
* â€œ*Mixture of True and False*: Some elements of the information are factually accurate, but some elements or claims are not. This rating should be used when speculation or unfounded claims are mixed with real events, numbers, quotes, etc., or when the headline of the link being shared makes a false claim but the text of the story is largely accurate. It should also only be used when the unsupported or false information is roughly equal to the accurate information in the post or link. Finally, use this rating for news articles that are based on unconfirmed information.    

* â€œ*Mostly False*: Most or all of the information in the post or in the link being shared is inaccurate. This should also be used when the central claim being made is false.    
 
* â€œ*No Factual Content*: This rating is used for posts that are pure opinion, comics, satire, or any other posts that do not make a factual claim. This is also the category to use for posts that are of the â€œLike this if you think...â€ variety.
 
â€œIn gathering the Facebook engagement data, the API did not return results for some posts. It did not return reaction count data for two posts, and two posts also did not return comment count data. There were 70 posts for which the API did not return share count data. We also used CrowdTangle's API to check that we had entered all posts from all nine pages on the assigned days. In some cases, the API returned URLs that were no longer active. We were unable to rate these posts and are unsure if they were subsequently removed by the pages or if the URLs were returned in error.â€
 
### Acknowledgements
 
This dataset was originally published on GitHub by BuzzFeed News here: https://github.com/BuzzFeedNews/2016-10-facebook-fact-check 
 
### Inspiration
 
Here are some ideas for exploring the hyperpartisan echo chambers on Facebook:
 
* How do left, mainstream, and right categories of Facebook pages differ in the stories they share?

* Which types of stories receive the most engagement from their Facebook followers? Are videos or links more effective for engagement?

* Can you replicate BuzzFeedâ€™s findings that â€œthe least accurate pages generated some of the highest numbers of shares, reactions, and comments on Facebookâ€?


#[Start a new kernel][3]


  [1]: https://www.kaggle.com/buzzfeed
  [2]: https://www.buzzfeed.com/craigsilverman/partisan-fb-pages-analysis?utm_term=.kq9kqJDZ2#.ia1QB2KJl.
  [3]: https://www.kaggle.com/buzzfeed/fact-checking-facebook-politics-pages/kernels?modal=true",2017-06-05T19:09:40.407Z,mrisdal/fact-checking-facebook-politics-pages,0.04687,https://www.kaggle.com/mrisdal/fact-checking-facebook-politics-pages,4,633,Unknown,10,"Ready version: 1, 2017-06-05T19:09:40.407Z",0.7352941,fake news
149,AOSSIE: Fake News Detection datasets,,,2019-06-21T12:54:16.217Z,ad6398/aossie-fake-news-detection-datasets,210.153734,https://www.kaggle.com/ad6398/aossie-fake-news-detection-datasets,0,6,Unknown,2,"Ready version: 1, 2019-06-21T12:54:16.217Z",0.1764706,fake news
150,Boatos de WhatsApp e outros do BoatosOrg (pt + es),1900 boatos (pt) + 130 rumores (es) desmentidos por boatos.org,"### Contexto

Boatos sÃ£o compartilhados em milhares de grupos todos os dias no WhatsApp, enganando muitos brasileiros. Para tentar combater esses boatos, eu imaginei que possa ter algum padrÃ£o nesses textos, para tentar identificÃ¡-los automaticamente e combater esse problema usando Machine Learning.

Para isso, eu precisava de datos, entÃ£o criei esse dataset fazendo scrapping de todos os boatos falsos dos sites [boatos.org](http://www.boatos.org/) e [hablillas.org](http://hablillas.org/), que estÃ£o nesses CSVs para que vocÃª possa explorar Ã  vontade.

Se quiser saber mais sobre o que fiz com esses dados, dÃª uma olhada no projeto [Fake News Detector](https://fakenewsdetector.org/).

### ConteÃºdo

O dataset contÃ©m o texto do boato em si, o link desmentindo o boato e a data que ele foi dementido pelo boatos.org, ou pelo hablillas.org para os rumores em espanhol.

O cÃ³digo usado para fazer scrapping desses dados estÃ¡ no [repositÃ³rio do github](https://github.com/fake-news-detector/scrappers).

### Ideias para explorar

- Existe algum padrÃ£o de escrita nos boatos?
- Boatos costumam ter mais emojis que conversas comuns?
- Quem sÃ£o as pessoas que mais aparecem nos boatos? Dilma? Temer? Pabllo Vittar?
- Olhando as datas, estÃ£o surgindo boatos cada vez mais rÃ¡pido?
- Podemos identificar e bloquear um boato no whatsapp antes que ele se espalhe?

### Agradecimentos

Muito obrigado Ã  equipe do boatos.org que me permitiu publicar este dataset para experimentos fututos",2018-10-24T22:31:23.51Z,rogeriochaves/boatos-de-whatsapp-boatosorg,0.444433,https://www.kaggle.com/rogeriochaves/boatos-de-whatsapp-boatosorg,1,66,CC0: Public Domain,1,"Ready version: 4, 2018-10-24T22:31:23.51Z",0.647058845,fake news
151,Predict Pakistan Elections 2018,Help Us Predict the Next Winner,"### Context

Here comes the July 25th 2018 and Pakistan will see the 13th election (1954, 1962, 1970, 1977, 1985, 1988, 1990, 1993, 1997, 2002, 2008 and 2013) since independence. Itâ€™s middle of the week (Wednesday) with an expected temperature of 27-33 degree Celsius with almost no chances of rain anywhere in the country. 

We predict the historic votersâ€™ turn out in this election of 57-61%. Historically the average turn out is 45% since 1977 (lowest 35% in 1997, highest 55% in 1977 and 53% in last elections). Pakistan ranked 164th out of 169 nations in votersâ€™ turn out; Australia being the first with 94.5% turn out. 

Votersâ€™ participation in the country is very diverse, historically Musakhel and Kohlu yield less than 25% whereas Layyah and Khanewal yield more than 60% and everything else is in between. Punjab has the highest and Balochistan has the lowest votersâ€™ turnout.

The contest will bring 3,675 candidates for 272 national assembly seats, that is 13 candidates on average per seat. PTI has unleashed 244 candidates ([highest in number][1] by any political party). Islamabad will see [76 candidates][2] just for 3 seats fighting to rule the capital that guarantees the psychological edge. 

There a quite few interesting facts about these elections, for example we will see the highest number of Lotas (candidates who often change their party affiliation) ever. PTI believes to win the election no matter what may come while the survey pundits predicts the PML(N) [lead of at least 13%][3] over PTI. 

The history of elections and the charges of corruption, votersâ€™ fraud, ghost votes, interferences by deep state or violence go hand by hand. There is (almost) no country in the world without the fear or accusations of such incidents in their elections. 
We are releasing the complete National Assembly Electionsâ€™ Results dataset for 2002, 2008 and 2013 elections in CSV files for public and calling all data scientists, international observers and journalists out there to help us achieve our inspirations. 

### Content

Three CSV files for complete election results for the national assembly of Pakistan for 2002, 2008 and 2013.
The file contains Seat, Constituency, Candidates Name, Party Affiliation, Votes, TotalValidVotes, TotalRejectedVotes, TotalVotes, TotalRegisteredVoters and Turnout variables for each seat.


### Acknowledgements

The dataset should be referenced as â€œZeeshan-ul-hassan Usmani, Sana Rasheed, Muhammad Usman, Muhammad Ilyas and Qazi Humayun, Pakistan Elections Complete Dataset (2002, 2008, 2013), Kaggle, July 7, 2018.â€

### Inspiration
Here is the list of ideas we are working on and like you to help with. Please post your kernels and analysis 

1.	Map each NA constituency to a District. Get the list of Districts in Pakistan. So we will know how many constituencies we have in each district and which ones? Please update the dataset version on this page.

2.	Find and Convert the current 2018 candidates list to Excel sheet and upload here

3.	Find out total no of candidates in 2018 elections, from each party, from each province, total no of parties and Avg. no of candidates per seat

4.	Calculate the voterâ€™s turn out in each NA. Highest, lowest etc. Make a historical timeframe so we would know how many people voted in each NA in 2002, 2008 and 2013

5.	Do analysis on invalid votes in each NA in all elections. Do we see any patterns here?

6.	Can we predict the effect of rain on voterâ€™s turn out in a given constituency? 

7.	Find out how many NEW candidates we have this time who have never contested any elections before? How many in each party?

8.	Can we make District Profiles with good visuals and heat-maps of which party would be leading in which district?

9.	Can we color the map of Pakistan (as we do in the US with Red and Blue) for each district? We can have a color or PML(N), PTI, PPP and MMA (only four major parties to start with)

10.	Can we find out Swing Districts and the Confirmed Districts for major parties?

11.	Are there any external datasets that we can join with this dataset to do some analysis? Please post the links or update the datasets here

12.	Make the Candidatesâ€™ profile so we know his party position in each election and whether he lost or won the last election(s). You can  whatever values and information as you like

13.	Get the ***â€œLotaâ€*** Score for each candidate. So anyone with more than 2 would be a ***â€œCertified Lotaâ€.*** These candidates are the ones who have changed their parties by x no of times, from independent to PPP, from PTI to PML etc.

14.	Get the â€œConfirmed Constituenciesâ€ where historically we have only one sided results. For example, PPP would always win from NA-XYZ or Zardari have never lost an election doesnâ€™t matter where he ran from. Which party would definitely win which seats? 

15.	Get the list of â€œSwing Constituenciesâ€ which historically are as random as anybodyâ€™s guess. For example, NA-XYZ voted for PTI in 2002, then went to PPP in 2008, then to PMLN in 2013 and so on. Once we have this list we can go further down and talk in detail the margins of win/loss in previous elections, who are the candidates (their profiles, district profiles, voter turnout etc.) and even results of bi-elections. But it is very important to get this list in first place. This is where can apply some models to do predict which way it will sway

16.	Make the â€œParty Potentialâ€ list. For Example, PML(N) with all its candidates, profiles etc. has the potential  to win 86 seats, PTI 65, PPP 43 etc. Here we can predict which party would form the government in which province?

17.	Find out how many people voted so far in Pakistan in last 3 elections. Max, Min, Avg. Per Seat, Per Province?  Can we hypothesize that that avg. no of voters in Punjab per seat (who go out and vote) is double than the avg. no of voters in KPK? Or voter turnout in Bunner is less than 25% while in Chakwal It is more than 65%?

18.	Popular Vote winner. Even if PML(N) lose, can we say that it will fetch max no of votes from the country by vote count only? Or is it true for PPP or PTI?

19.	Find â€œFake Candidatesâ€ the people who are running but have no chance to win. Like no past elections or political history. These are the one who will withdraw 24 hours before the elections

20.	Find the â€œIndependentsâ€ who will go to the highest bidder after winning

21.	Find anything interesting you can on candidates. Like is it true if candidatesâ€™ name start from M or A, he has twice the chances of winning than the candidates whose names start with other letters?

22.	Surprise Me!


  [1]: https://gulfnews.com/news/asia/pakistan/pti-fields-highest-number-of-candidates-for-2018-elections-1.2244562
  [2]: https://www.geo.tv/latest/201359-general-election-76-candidates-to-contest-for-three-na-seats-in-islamabad
  [3]: https://en.wikipedia.org/wiki/Pakistani_general_election,_2018",2018-07-30T07:32:41.157Z,zusmani/predict-pakistan-elections-2018,48.731676,https://www.kaggle.com/zusmani/predict-pakistan-elections-2018,5,1201,Data files Â© Original Authors,17,"Ready version: 15, 2018-07-30T07:32:41.157Z",0.7647059,fake news
