{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping and API consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oriented to forest fires in Guadalajara\n",
    "###### *This code is really specific for the used webpages\n",
    "##### (By Daniel Hernández Mota)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain the first links \n",
    "\n",
    "def web_scrap_init (url, string):\n",
    "    # Use the URL of the general web-page to obtain the information\n",
    "    html =  requests.get(url).content\n",
    "    soup = BeautifulSoup(html,'html5lib')\n",
    "    # Use findall to determine the best datasets with the given the desired conditions\n",
    "    items = soup.find_all(string)\n",
    "    item_index = [i for i in range(len(items)) if items[i].text.startswith('Incendios forestales en Bosque La Primavera')]\n",
    "\n",
    "    # Add each element into a list\n",
    "    urls = [items[index].attrs['href'] for index in item_index if 'resource' not in items[index].attrs['href']]\n",
    "    return urls\n",
    "\n",
    "url = 'https://datos.jalisco.gob.mx/search/type/dataset?query=incendio&sort_by=changed'\n",
    "find = 'a'\n",
    "value1 = web_scrap_init(url,find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the latter data to obtain new webpages to scrap.\n",
    "def web_scrap_two (lst):\n",
    "    URL = \"https://datos.jalisco.gob.mx\"\n",
    "    soups = [BeautifulSoup(requests.get(URL+page).content, 'html5lib') for page in lst]\n",
    "\n",
    "    # Optain the links for the csv that have the data in a list. \n",
    "    csv_url = [soup.find_all('a', {'class':'btn btn-primary data-link'})[0].attrs['href'] for soup in soups]\n",
    "    return csv_url\n",
    "\n",
    "value2 = web_scrap_two(value1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all the data in a list and merge it in a dataframe\n",
    "# It is possible to obtain trough a csv!\n",
    "def web_scrap_data_df(csv_url):\n",
    "    crude_data = [requests.get(csv).content for csv in csv_url]\n",
    "    texts = [BeautifulSoup(crude_data[i],'html5lib').decode('ascii').split('<html>\\n <head>\\n </head>\\n <body>\\n')[1].split('</body>\\n</html>')[0] \n",
    "             for i in range(len(crude_data))]\n",
    "    #Add each element into a list. \n",
    "    final_df = [[row.split(',') for row in texts[element].split('\\n')] for element in range(len(texts))]\n",
    "    # There was a single value that made noise to all the data, it had 20 columns instead of 18, it weas fixed manually with\n",
    "    # the next code:\n",
    "\n",
    "    for i in range(len(final_df[2])):\n",
    "        try:\n",
    "            if len(final_df[2][i])==20:\n",
    "                final_df[2].pop(i)\n",
    "        except:\n",
    "            pass\n",
    "    #Generate the final dataframe with all the raw data from the three links:\n",
    "\n",
    "    dataframes = [pd.DataFrame(data[1:-1], columns = data[0]) for data in final_df]\n",
    "    fire_df=pd.DataFrame()\n",
    "    for dataframe in dataframes:\n",
    "        fire_df = fire_df.append(dataframe)\n",
    "    # Write the file to a dataframe\n",
    "    if True:\n",
    "        pass\n",
    "    else:\n",
    "        fire_df.to_csv('fire_data.csv')\n",
    "    return fire_df\n",
    "\n",
    "fire_df = web_scrap_data_df(value2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns\n",
    "newcols_dict = {colname : colname.lower().replace('  ','').replace(' ','_').replace('número', 'num').replace('geográficas', 'geo')\n",
    "                for colname in list(fire_df.columns)}\n",
    "fire_df = fire_df.rename(columns = newcols_dict)\n",
    "\n",
    "#Drop unessesary columns\n",
    "try:\n",
    "    fire_df = fire_df.drop(['level_0', 'index'], 1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Check for null values\n",
    "fire_df.isnull().sum()\n",
    "\n",
    "# Obtain a descrpition of the data\n",
    "fire_df.describe().T\n",
    "\n",
    "# There is only one type of data in \"fuente\", therefore that column can be descarted since it doesnt yield any information\n",
    "try:\n",
    "    fire_df = fire_df.drop( ['fuente'], axis = 1)\n",
    "except:\n",
    "    pass\n",
    "fire_df = fire_df.reset_index(drop=True)\n",
    "\n",
    "# Most information is not classified on the estrato_afectado_.+, therefore we are replazing the data of '' to 0\n",
    "fire_df = fire_df.replace('', float(0.00))\n",
    "\n",
    "# Change the type of the numerical data (which somehow is string) to float\n",
    "columns_fire = list(fire_df.columns)\n",
    "columns_fire_num = columns_fire[8:] \n",
    "columns_fire_num\n",
    "for column in columns_fire_num:\n",
    "    fire_df = fire_df.astype({column : 'float'})\n",
    "\n",
    "# Modify the dates to an actual date-format\n",
    "fire_df.fecha_de_registro = pd.to_datetime(fire_df['fecha_de_registro'])\n",
    "\n",
    "#Order the dataframe by the dates\n",
    "try:\n",
    "    fire_df = fire_df.sort_values(['fecha_de_registro']).reset_index(drop = True).drop('num_de_incendio', axis = 1)\n",
    "except:\n",
    "    pass\n",
    "# Change North coordinates in GMS to GD\n",
    "nort = list(fire_df['coordenadas_geo_norte'])\n",
    "#Obtain purely the GMS coordinates\n",
    "NGMS = [coordinate.replace('N','').replace('\"','').replace(' ','').replace('°','|').replace('´´','').replace('´','|').replace(\"''\",\"\").replace(\"'\",\"|\").split('|') for coordinate in nort]\n",
    "NGD = [float(coor[0])+float(coor[1])/60+float(coor[2])/3600 for coor in NGMS]\n",
    "\n",
    "# Change West coordinates in GMS to GD\n",
    "west = list(fire_df['coordenadas_geo_oeste'])\n",
    "# There was a single value that made noise\n",
    "fire_df.loc[(205-34),'coordenadas_geo_oeste'] = \"103°  33' 27.1''\"\n",
    "#Obtain purely the GMS coordinates\n",
    "WGMS = [coordinate.replace('W','').replace('O','0').replace('\"','').replace(' ','').replace('°','|').replace('´´','').replace('´','|').replace(\"''\",\"\").replace(\"'\",\"|\").split('|') for coordinate in west]\n",
    "WGD = [float(coor[0])+float(coor[1])/60+float(coor[2])/3600 for coor in WGMS]\n",
    "fire_df.coordenadas_geo_oeste = pd.Series(WGD)\n",
    "fire_df.coordenadas_geo_norte = pd.Series(NGD)\n",
    "\n",
    "# Obtain some insights from the numerical data\n",
    "fire_df.describe().T\n",
    "fire_df.head(2)\n",
    "\n",
    "if True:\n",
    "    pass\n",
    "else:\n",
    "    fire_df.to_csv('fire_data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the needed data for the API\n",
    "needed_data = fire_df[['coordenadas_geo_norte','coordenadas_geo_oeste','fecha_de_registro']]\n",
    "latitude = list(needed_data['coordenadas_geo_norte'])\n",
    "longitude = list(needed_data['coordenadas_geo_oeste'])\n",
    "date = list(needed_data['fecha_de_registro'])\n",
    "date_vals = [d.value//10**9 for d in date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_consumption_darksky(url, key, latitude, longitude,date_vals):\n",
    "    json_list = [requests.get('{}{}/{},-{},{}'.format(url,key,str(latitude[i]),str(longitude[i]),str(date_vals[i]))).json() for i in range(len(date_vals))]\n",
    "    \n",
    "    # Analysis of the information\n",
    "    # Lenght of the daily data as set:\n",
    "    daily_len = {len(json_list[i]['daily']['data'][0]) for i in range(len(json_list))}\n",
    "    # Lenght of the current data as set:\n",
    "    currently_len = {len(json_list[i]['currently']) for i in range(len(json_list))}\n",
    "    \n",
    "    # Each of the data frames has different lenght so an analysis has to be done for each one\n",
    "    pd_met_info = [pd.DataFrame(json_list[i]['daily']['data']) for i in range(len(json_list))]\n",
    "     \n",
    "    # Each data frame is assigned to its corresponding category\n",
    "    pd_distinct = [pd.DataFrame(),pd.DataFrame(),pd.DataFrame(),pd.DataFrame(),pd.DataFrame(),pd.DataFrame()]\n",
    "\n",
    "    for i in range(len(pd_met_info)):\n",
    "        if (len(list(pd_met_info[i].columns)) == list(daily_len)[0]):\n",
    "               pd_distinct[0] = pd_distinct[0].append(pd_met_info[i])\n",
    "        elif (len(list(pd_met_info[i].columns)) == list(daily_len)[1]):\n",
    "               pd_distinct[1] = pd_distinct[1].append(pd_met_info[i])\n",
    "        elif (len(list(pd_met_info[i].columns)) == list(daily_len)[2]):\n",
    "               pd_distinct[2] = pd_distinct[2].append(pd_met_info[i])\n",
    "        elif (len(list(pd_met_info[i].columns)) == list(daily_len)[3]):\n",
    "               pd_distinct[3] = pd_distinct[3].append(pd_met_info[i])\n",
    "        elif (len(list(pd_met_info[i].columns)) == list(daily_len)[4]):\n",
    "               pd_distinct[4] = pd_distinct[4].append(pd_met_info[i])\n",
    "        elif (len(list(pd_met_info[i].columns)) == list(daily_len)[5]):\n",
    "               pd_distinct[5] = pd_distinct[5].append(pd_met_info[i])\n",
    "\n",
    "    # To make easier the data manipulation, all the sets are compared to see which columns are the same, those will stick together\n",
    "    lenghts_distinct_df = [len(data) for data in pd_distinct]\n",
    "    new_columns = list(set(pd_distinct[0].columns) & set(pd_distinct[1].columns) & set(pd_distinct[2].columns) & set(pd_distinct[3].columns) & set(pd_distinct[4].columns) & set(pd_distinct[        \n",
    "    meteorological_pd = pd.DataFrame()\n",
    "\n",
    "    # Now it is possible to join all the dataframes \n",
    "    for df in pd_distinct:\n",
    "        meteorological_pd = meteorological_pd.append(df[new_columns])\n",
    "    if True:\n",
    "        pass\n",
    "    else:\n",
    "        meteorological_pd.to_csv('meteorological.csv')\n",
    "    return meteorological_pd\n",
    "\n",
    "key = '3674abb3a8982b4a3359284ea8d0986c'\n",
    "url = 'https://api.darksky.net/forecast/'    \n",
    "meteorological_pd = api_consumption_darksky(url, key, latitude, longitude,date_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null data\n",
    "meteorological_pd.isnull().sum()\n",
    "meteorological_pd[meteorological_pd[\"cloudCover\"].isnull()==True][['temperatureMin', 'time', 'uvIndex','cloudCover']]\n",
    "meteorological_pd = meteorological_pd.fillna(0)\n",
    "\n",
    "# Sort and refresh values\n",
    "meteorological_pd = meteorological_pd.sort_values('time', axis=0).reset_index(drop=True)\n",
    "meteorological_pd.head(4)\n",
    "\n",
    "# Stay only with relevant information\n",
    "\n",
    "meteorological_pd = meteorological_pd[['temperatureMin','temperatureMax','apparentTemperatureMin','apparentTemperatureMax', 'temperatureLow','temperatureHigh',\n",
    "                  'time','dewPoint', 'uvIndex','uvIndexTime', 'windBearing','cloudCover','icon','windSpeed', 'humidity','summary'\n",
    "                  ]]\n",
    "if True:\n",
    "    pass\n",
    "else:\n",
    "    meteorological_pd.to_csv('meteorological_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unite both dataframes\n",
    "forest_fire = pd.concat([fire_df,meteorological_pd], axis=1)\n",
    "\n",
    "# Obtain insights\n",
    "forest_fire.describe().T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
